<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>NIST AI Risk Management Framework (AI RMF 1.0)</title>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <style>
      :root {
        --bg: #f5f5f5;
        --card: #bbc7c9;
        --card-text: #303030;
        --border: #26c6da;
        --link: #3c506b;
        --entity-bg: #fff;
        --entity-border: #96b9bd;
      }
      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }
      body {
        font-family:
          "Inter",
          -apple-system,
          BlinkMacSystemFont,
          "Segoe UI",
          Roboto,
          sans-serif;
        background: var(--bg);
        color: #333;
        line-height: 1.6;
        max-width: 820px;
        margin: 0 auto;
        padding: 2rem 1rem;
      }
      h1 {
        font-size: 1.8rem;
        margin-bottom: 1.5rem;
      }
      h2 {
        font-size: 1.3rem;
        margin: 2rem 0 1rem;
        border-bottom: 2px solid #ccc;
        padding-bottom: 0.3rem;
      }
      h3 {
        font-size: 1.1rem;
        margin-bottom: 0.3rem;
      }
      .card {
        background: var(--card);
        color: var(--card-text);
        border-radius: 8px;
        padding: 1rem 1.2rem;
        margin-bottom: 0.8rem;
      }
      .card .tags {
        font-weight: 900;
        margin-bottom: 0.4rem;
      }
      .card ul {
        margin: 0.4rem 0 0 1.2rem;
      }
      .card li {
        margin-bottom: 0.2rem;
      }
      .card a {
        color: var(--link);
        text-decoration: none;
      }
      .card a:hover {
        color: white;
        background: var(--link);
        padding: 0.2rem;
        border-radius: 2px;
      }
      .toggle {
        text-align: right;
        font-size: 0.85rem;
        color: #555;
        cursor: pointer;
        user-select: none;
        margin-bottom: 0.2rem;
      }
      .toggle:hover {
        color: var(--link);
      }
      .chunk-original {
        display: none;
        background: #e0e0e0;
        border-radius: 8px;
        padding: 1rem 1.2rem;
        margin-bottom: 0.8rem;
        white-space: pre-wrap;
        font-size: 0.9rem;
      }
      a.entity-link {
        color: var(--link);
        text-decoration: none;
        border-bottom: 1px dashed var(--link);
      }
      a.entity-link:hover {
        color: white;
        border-bottom-style: solid;
      }
      .entity-card {
        background: var(--entity-bg);
        border: 1px solid var(--entity-border);
        border-radius: 8px;
        padding: 1rem 1.2rem;
        margin-bottom: 0.8rem;
      }
      .entity-card h3 {
        color: #00796b;
      }
      .entity-card .desc {
        color: #555;
        margin: 0.3rem 0 0.6rem;
      }
      .entity-card ul {
        margin: 0 0 0.6rem 1.2rem;
      }
      .entity-card li {
        margin-bottom: 0.2rem;
      }
      .chunk-links {
        font-size: 0.85rem;
        color: #777;
      }
      .chunk-links a {
        color: var(--link);
        text-decoration: none;
      }
      .chunk-links a:hover {
        text-decoration: underline;
      }
      .footer {
        font-size: 0.8rem;
        color: #696969;
        margin-top: 2rem;
        border-top: 1px solid #ddd;
        padding-top: 1rem;
        text-align: center;
        text-decoration: none;
      }
      .footer a {
        color: var(--link);
        text-decoration: none;
      }
      .footer a:hover {
        text-decoration: underline;
      }
      .detail {
        font-size: 0.9rem;
        color: #555;
        margin-top: 1rem;
      }
      .disclaimer {
        font-size: 0.6rem;
        color: #696969;
        margin-top: 1rem;
        border-top: 1px solid #ddd;
        padding-top: 1rem;
        text-align: left;
        border-bottom: 1px solid #ddd;
        padding-bottom: 1rem;
      }
      #graph {
        background: #373737;
        border: 1px solid #cfd8dc;
        border-radius: 8px;
        min-height: 500px;
      }
      svg {
        width: 100%;
        height: 100%;
        display: block;
        cursor: grab;
        min-height: 500px;
      }

      svg.panning {
        cursor: grabbing;
      }

      /* Guide circles */
      .ring {
        fill: rgb(145, 145, 145);
        /* filter: drop-shadow(0px 0px 14px #fff);    */
        stroke: #d0d0d0;
        stroke-width: 4;
        stroke-dasharray: 6 4;
        opacity: 0.05;
      }

      /* Edges */
      .edge {
        stroke: #aab;
        stroke-width: 1.4;
        fill: none;
        transition:
          opacity 0.15s,
          stroke 0.15s,
          stroke-width 0.15s;
      }

      .edge.same-layer {
        stroke: #aab;
        stroke-width: 1.6;
      }

      .edge.highlighted {
        stroke: #e06030;
        stroke-width: 2.4;
      }

      /* Nodes */
      .node-circle {
        stroke: #444;
        stroke-width: 1.5;
        cursor: pointer;
        transition:
          r 0.15s,
          stroke-width 0.15s;
      }

      .node-circle:hover {
        stroke-width: 2.5;
      }

      .node-circle.faded {
        opacity: 0.25;
      }

      .node-label {
        font-size: 11px;
        fill: #fff;
        opacity: 0.8;
        text-anchor: middle;
        dominant-baseline: central;
        pointer-events: none;
        font-weight: 500;
      }

      .node-label.faded {
        opacity: 0.2;
      }

      /* Tooltip */
      #tooltip {
        position: fixed;
        background: #5a5a5a;
        border: 1px solid #ccc;
        border-radius: 6px;
        padding: 8px 12px;
        font-size: 13px;
        color: #fff;
        pointer-events: none;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);
        display: none;
        z-index: 100;
        line-height: 1.5;
        max-width: 400px;
        word-wrap: wrap;
      }

      #tooltip .tt-label {
        font-weight: 600;
        font-size: 14px;
        opacity: 0.8;
      }

      #tooltip .tt-detail {
        color: #fff;
        opacity: 0.8;
        font-size: 10px;
        margin-top: 10px;
      }

      /* Graph container for fullscreen */
      .graph-container {
        position: relative;
      }
      .graph-container.fullscreen {
        position: fixed;
        top: 0;
        left: 0;
        width: 100vw;
        height: 100vh;
        z-index: 9999;
        background: var(--bg);
        margin: 0;
        padding: 0;
      }
      .graph-container.fullscreen svg {
        width: 100vw;
        height: 100vh;
      }
      .fullscreen-toggle {
        position: absolute;
        top: 8px;
        right: 8px;
        width: 32px;
        height: 32px;
        background: rgba(90, 90, 90, 0.75);
        border: none;
        border-radius: 6px;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        z-index: 10000;
        transition: background 0.15s;
      }
      .fullscreen-toggle:hover {
        background: rgba(60, 60, 60, 0.9);
      }
      .fullscreen-toggle svg {
        width: 18px;
        height: 18px;
        display: block;
      }
    </style>
  </head>
  <body>
    <h1>NIST AI Risk Management Framework (AI RMF 1.0)</h1>

    
    <div class="card">
      
      <p>The NIST AI 100-1 document outlines the Artificial Intelligence Risk Management Framework (AI RMF 1.0), which aims to provide guidelines for managing risks associated with AI systems. It emphasizes the importance of understanding and addressing various risks, impacts, and challenges in AI risk management, including risk measurement and prioritization. The framework is designed to be a living document, with plans for regular updates and community input, and includes a core structure for governance, mapping, measuring, and managing AI risks. Additionally, it highlights the need for trustworthy AI systems that are valid, safe, secure, accountable, and fair, while also addressing harmful biases.</p>
      
      <div class="detail">Extracted 247 entities, 310 relations and 32 topics on Feb 15, 2026</div>
      
      <div class="detail">
        Source:
        <a href="https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf" target="_blank" rel="noopener noreferrer"
          >https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf</a
        >
      </div>
      
    </div>

    
    <div class="card">
      ‣<a href="#chunks">Text Chunks</a> ‣<a href="#topics"
        >Topics</a
      >
      ‣<a href="#entities">Entities</a>
    </div>

    
    <h2 id="graph-title">Network</h2>
    <div class="graph-container" id="graph-container">
      <button
        class="fullscreen-toggle"
        id="fullscreen-toggle"
        title="Toggle fullscreen"
      >
        <svg
          id="fs-icon-expand"
          viewBox="0 0 24 24"
          fill="none"
          stroke="#fff"
          stroke-width="2"
          stroke-linecap="round"
          stroke-linejoin="round"
        >
          <polyline points="15 3 21 3 21 9"></polyline>
          <polyline points="9 21 3 21 3 15"></polyline>
          <line x1="21" y1="3" x2="14" y2="10"></line>
          <line x1="3" y1="21" x2="10" y2="14"></line>
        </svg>
        <svg
          id="fs-icon-collapse"
          viewBox="0 0 24 24"
          fill="none"
          stroke="#fff"
          stroke-width="2"
          stroke-linecap="round"
          stroke-linejoin="round"
          style="display: none"
        >
          <polyline points="4 14 10 14 10 20"></polyline>
          <polyline points="20 10 14 10 14 4"></polyline>
          <line x1="14" y1="10" x2="21" y2="3"></line>
          <line x1="10" y1="14" x2="3" y2="21"></line>
        </svg>
      </button>
      <div id="graph">
        <svg id="graph-svg">
          <g id="viewport">
            <g id="rings-layer"></g>
            <g id="edges-layer"></g>
            <g id="nodes-layer"></g>
          </g>
        </svg>
      </div>
    </div>

    <div id="tooltip"></div>
    <div id="node-info"></div>

    
    <h2 id="topics">Topics</h2>
    
    <div class="card">
      <div class="tags">Subdivision, Framework, Person</div>
      <ul>
        
        <li>Around Subdivision, Framework, Person.</li>
         
        <li><a href="#entity-ai-rmf-1" class="entity-link">AI RMF 1</a>, <a href="#entity-ai-rmf-profiles" class="entity-link">AI RMF Profiles</a>, <a href="#entity-ai-rmf-temporal-profiles" class="entity-link">AI RMF temporal profiles</a>, <a href="#entity-artificial-intelligence-risk-management-framework-(ai-rmf-1.0)" class="entity-link">Artificial Intelligence Risk Management Framework (AI RMF 1.0)</a>, <a href="#entity-gina-m.-raimondo" class="entity-link">Gina M. Raimondo</a>, <a href="#entity-laurie-e.-locascio" class="entity-link">Laurie E. Locascio</a>, <a href="#entity-manage-3" class="entity-link">MANAGE 3</a>, <a href="#entity-manage-3.1" class="entity-link">MANAGE 3.1</a>, <a href="#entity-manage-3.2" class="entity-link">MANAGE 3.2</a>, <a href="#entity-manage-4" class="entity-link">MANAGE 4</a>, <a href="#entity-manage-4.1" class="entity-link">MANAGE 4.1</a>, <a href="#entity-manage-4.2" class="entity-link">MANAGE 4.2</a>, <a href="#entity-manage-4.3" class="entity-link">MANAGE 4.3</a>, <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>, <a href="#entity-nist-special-publication-1270" class="entity-link">NIST Special Publication 1270</a>, <a href="#entity-national-institute-of-standards-and-technology" class="entity-link">National Institute of Standards and Technology</a>, <a href="#entity-u.s.-department-of-commerce" class="entity-link">U.S. Department of Commerce</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Function, Category, Person</div>
      <ul>
        
        <li>Around Function, Category, Person.</li>
         
        <li><a href="#entity-ai-actor-tasks" class="entity-link">AI Actor Tasks</a>, <a href="#entity-ai-deployment" class="entity-link">AI Deployment</a>, <a href="#entity-ai-development" class="entity-link">AI Development</a>, <a href="#entity-ai-impact-assessment" class="entity-link">AI Impact Assessment</a>, <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>, <a href="#entity-ai-actors" class="entity-link">AI actors</a>, <a href="#entity-ai-lifecycle-activities" class="entity-link">AI lifecycle activities</a>, <a href="#entity-ai-lifecycle-stages" class="entity-link">AI lifecycle stages</a>, <a href="#entity-domain-expert" class="entity-link">Domain Expert</a>, <a href="#entity-govern-5" class="entity-link">GOVERN 5</a>, <a href="#entity-govern" class="entity-link">Govern</a>, <a href="#entity-governance-and-oversight-tasks" class="entity-link">Governance and Oversight tasks</a>, <a href="#entity-human-factors-professionals" class="entity-link">Human factors professionals</a>, <a href="#entity-manage" class="entity-link">Manage</a>, <a href="#entity-map" class="entity-link">Map</a>, <a href="#entity-measure" class="entity-link">Measure</a>, <a href="#entity-oecd" class="entity-link">OECD</a>, <a href="#entity-operation-and-monitoring" class="entity-link">Operation and Monitoring</a>, <a href="#entity-people-&-planet-dimension" class="entity-link">People &amp; Planet dimension</a>, <a href="#entity-procurement-tasks" class="entity-link">Procurement tasks</a>, <a href="#entity-diverse-team" class="entity-link">diverse team</a>, <a href="#entity-downstream-risks" class="entity-link">downstream risks</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Concept, Framework, Technology</div>
      <ul>
        
        <li>Around Concept, Framework, Technology.</li>
         
        <li><a href="#entity-ai-lifecycle" class="entity-link">AI Lifecycle</a>, <a href="#entity-ai-technologies" class="entity-link">AI technologies</a>, <a href="#entity-accountability" class="entity-link">Accountability</a>, <a href="#entity-explainable-ai" class="entity-link">Explainable AI</a>, <a href="#entity-govern" class="entity-link">GOVERN</a>, <a href="#entity-general-public" class="entity-link">General public</a>, <a href="#entity-interpretability" class="entity-link">Interpretability</a>, <a href="#entity-manage" class="entity-link">MANAGE</a>, <a href="#entity-map" class="entity-link">MAP</a>, <a href="#entity-measure" class="entity-link">MEASURE</a>, <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>, <a href="#entity-national-ai-initiative-act-of-2020" class="entity-link">National AI Initiative Act of 2020</a>, <a href="#entity-national-security-commission-on-artificial-intelligence" class="entity-link">National Security Commission on Artificial Intelligence</a>, <a href="#entity-privacy" class="entity-link">Privacy</a>, <a href="#entity-transparency" class="entity-link">Transparency</a>, <a href="#entity-society" class="entity-link">society</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Outcome, Methodology</div>
      <ul>
        
        <li>Around Outcome, Methodology.</li>
         
        <li><a href="#entity-impacts-on-a-population" class="entity-link">impacts on a population</a>, <a href="#entity-methodologies" class="entity-link">methodologies</a>, <a href="#entity-organization-developing-the-ai-system" class="entity-link">organization developing the AI system</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Person, Category, System</div>
      <ul>
        
        <li>Around Person, Category, System.</li>
         
        <li><a href="#entity-ai-deployer" class="entity-link">AI deployer</a>, <a href="#entity-ai-designer" class="entity-link">AI designer</a>, <a href="#entity-ai-developer" class="entity-link">AI developer</a>, <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>, <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Category, Framework, Risk</div>
      <ul>
        
        <li>Around Category, Framework, Risk.</li>
         
        <li><a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>, <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>, <a href="#entity-ai-risk-management-framework-roadmap" class="entity-link">AI Risk Management Framework Roadmap</a>, <a href="#entity-ai-risks" class="entity-link">AI Risks</a>, <a href="#entity-ai-risks-and-trustworthiness" class="entity-link">AI Risks and Trustworthiness</a>, <a href="#entity-ai-community" class="entity-link">AI community</a>, <a href="#entity-bias" class="entity-link">Bias</a>, <a href="#entity-core-and-profiles" class="entity-link">Core and Profiles</a>, <a href="#entity-fairness-in-ai" class="entity-link">Fairness in AI</a>, <a href="#entity-foundational-information" class="entity-link">Foundational Information</a>, <a href="#entity-iso-guide-73" class="entity-link">ISO GUIDE 73</a>, <a href="#entity-nist" class="entity-link">NIST</a>, <a href="#entity-oecd-framework" class="entity-link">OECD framework</a>, <a href="#entity-risk-tolerance" class="entity-link">Risk Tolerance</a>, <a href="#entity-tevv-practices" class="entity-link">TEVV practices</a>, <a href="#entity-version-control-table" class="entity-link">Version Control Table</a>, <a href="#entity-computational-and-statistical-bias" class="entity-link">computational and statistical bias</a>, <a href="#entity-human-cognitive-bias" class="entity-link">human-cognitive bias</a>, <a href="#entity-policies-and-norms" class="entity-link">policies and norms</a>, <a href="#entity-residual-risk" class="entity-link">residual risk</a>, <a href="#entity-risk-management-culture" class="entity-link">risk management culture</a>, <a href="#entity-risk-management-framework" class="entity-link">risk management framework</a>, <a href="#entity-systemic-bias" class="entity-link">systemic bias</a>, <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">System, Function</div>
      <ul>
        
        <li>Around System, Function.</li>
         
        <li><a href="#entity-ai-system-lifecycle" class="entity-link">AI system lifecycle</a>, <a href="#entity-plan-and-design-function" class="entity-link">Plan and Design function</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Function, Technology, Resource</div>
      <ul>
        
        <li>Around Function, Technology, Resource.</li>
         
        <li><a href="#entity-ai-models-and-algorithms" class="entity-link">AI models and algorithms</a>, <a href="#entity-ai-trustworthiness-characteristics" class="entity-link">AI trustworthiness characteristics</a>, <a href="#entity-datasets" class="entity-link">datasets</a>, <a href="#entity-human-judgment" class="entity-link">human judgment</a>, <a href="#entity-social-and-organizational-behavior" class="entity-link">social and organizational behavior</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Resource</div>
      <ul>
        
        <li>Around Resource.</li>
         
        <li><a href="#entity-tevv-findings" class="entity-link">TEVV findings</a>, <a href="#entity-subject-matter-experts" class="entity-link">subject matter experts</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Category, Framework, Risk</div>
      <ul>
        
        <li>Around Category, Framework, Risk.</li>
         
        <li><a href="#entity-ai-systems" class="entity-link">AI systems</a>, <a href="#entity-affected-individuals/communities" class="entity-link">Affected individuals/communities</a>, <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>, <a href="#entity-privacy-and-cybersecurity-risk-management" class="entity-link">Privacy and cybersecurity risk management</a>, <a href="#entity-test,-evaluation,-verification,-and-validation-(tevv)" class="entity-link">Test, Evaluation, Verification, and Validation (TEVV)</a>, <a href="#entity-accuracy" class="entity-link">accuracy</a>, <a href="#entity-cognitive-biases" class="entity-link">cognitive biases</a>, <a href="#entity-compliance-experts" class="entity-link">compliance experts</a>, <a href="#entity-data-sparsity" class="entity-link">data sparsity</a>, <a href="#entity-enterprise-risk-management" class="entity-link">enterprise risk management</a>, <a href="#entity-explainability" class="entity-link">explainability</a>, <a href="#entity-fairness" class="entity-link">fairness</a>, <a href="#entity-generative-ai" class="entity-link">generative AI</a>, <a href="#entity-harmful-bias" class="entity-link">harmful bias</a>, <a href="#entity-human-decision-maker" class="entity-link">human decision maker</a>, <a href="#entity-human-roles-and-responsibilities" class="entity-link">human roles and responsibilities</a>, <a href="#entity-impacts-to-individuals,-groups,-communities,-organizations,-and-society" class="entity-link">impacts to individuals, groups, communities, organizations, and society</a>, <a href="#entity-inequitable-outcomes" class="entity-link">inequitable outcomes</a>, <a href="#entity-interpretability" class="entity-link">interpretability</a>, <a href="#entity-machine-learning-attacks" class="entity-link">machine learning attacks</a>, <a href="#entity-negative-residual-risks" class="entity-link">negative residual risks</a>, <a href="#entity-organizational-management" class="entity-link">organizational management</a>, <a href="#entity-pre-trained-models" class="entity-link">pre-trained models</a>, <a href="#entity-predictive-accuracy" class="entity-link">predictive accuracy</a>, <a href="#entity-privacy" class="entity-link">privacy</a>, <a href="#entity-reliability" class="entity-link">reliability</a>, <a href="#entity-risk-measurement-challenges" class="entity-link">risk measurement challenges</a>, <a href="#entity-robustness" class="entity-link">robustness</a>, <a href="#entity-societal-dynamics" class="entity-link">societal dynamics</a>, <a href="#entity-third-party-ai-technologies" class="entity-link">third-party AI technologies</a>, <a href="#entity-third-party-software,-hardware,-and-data" class="entity-link">third-party software, hardware, and data</a>, <a href="#entity-training-data" class="entity-link">training data</a>, <a href="#entity-transfer-learning" class="entity-link">transfer learning</a>, <a href="#entity-transparency" class="entity-link">transparency</a>, <a href="#entity-trustworthy-characteristics" class="entity-link">trustworthy characteristics</a>, <a href="#entity-video-compression-models" class="entity-link">video compression models</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Organization, Risk, Standard</div>
      <ul>
        
        <li>Around Organization, Risk, Standard.</li>
         
        <li><a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>, <a href="#entity-ai-system" class="entity-link">AI system</a>, <a href="#entity-iso-26000:2010" class="entity-link">ISO 26000:2010</a>, <a href="#entity-iso/iec-tr-24368:2022" class="entity-link">ISO/IEC TR 24368:2022</a>, <a href="#entity-responsible-ai" class="entity-link">Responsible AI</a>, <a href="#entity-civil-society-organizations" class="entity-link">civil society organizations</a>, <a href="#entity-end-users" class="entity-link">end users</a>, <a href="#entity-environmental-groups" class="entity-link">environmental groups</a>, <a href="#entity-environmental-impact" class="entity-link">environmental impact</a>, <a href="#entity-external-collaborators" class="entity-link">external collaborators</a>, <a href="#entity-fairness-and-bias" class="entity-link">fairness and bias</a>, <a href="#entity-internal-team" class="entity-link">internal team</a>, <a href="#entity-potentially-impacted-communities" class="entity-link">potentially impacted communities</a>, <a href="#entity-privacy-risk" class="entity-link">privacy risk</a>, <a href="#entity-risk-metrics" class="entity-link">risk metrics</a>, <a href="#entity-safety-risks" class="entity-link">safety risks</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Framework, Category, Standard</div>
      <ul>
        
        <li>Around Framework, Category, Standard.</li>
         
        <li><a href="#entity-ai-rmf" class="entity-link">AI RMF</a>, <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>, <a href="#entity-ai-technology" class="entity-link">AI technology</a>, <a href="#entity-ai-trustworthiness" class="entity-link">AI trustworthiness</a>, <a href="#entity-iso/iec-22989:2022" class="entity-link">ISO/IEC 22989:2022</a>, <a href="#entity-map,-measure,-and-manage-functions" class="entity-link">MAP, MEASURE, and MANAGE functions</a>, <a href="#entity-nist-cybersecurity-framework" class="entity-link">NIST Cybersecurity Framework</a>, <a href="#entity-nist-privacy-framework" class="entity-link">NIST Privacy Framework</a>, <a href="#entity-nist-risk-management-framework" class="entity-link">NIST Risk Management Framework</a>, <a href="#entity-nist.ai.100-1" class="entity-link">NIST.AI.100-1</a>, <a href="#entity-national-artificial-intelligence-initiative-act-of-2020" class="entity-link">National Artificial Intelligence Initiative Act of 2020</a>, <a href="#entity-oecd-framework-for-the-classification-of-ai-systems" class="entity-link">OECD Framework for the Classification of AI systems</a>, <a href="#entity-oecd-recommendation-on-ai:2019" class="entity-link">OECD Recommendation on AI:2019</a>, <a href="#entity-privacy-enhancing-technologies-(pets)" class="entity-link">Privacy-enhancing technologies (PETs)</a>, <a href="#entity-secure-software-development-framework" class="entity-link">Secure Software Development Framework</a>, <a href="#entity-tevv" class="entity-link">TEVV</a>, <a href="#entity-negative-ai-risks" class="entity-link">negative AI risks</a>, <a href="#entity-organizations" class="entity-link">organizations</a>, <a href="#entity-security-and-resilience" class="entity-link">security and resilience</a>, <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, <a href="#entity-standards,-guidelines,-best-practices,-methodologies,-and-tools" class="entity-link">standards, guidelines, best practices, methodologies, and tools</a>, <a href="#entity-third-party-software-and-data" class="entity-link">third-party software and data</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Concept</div>
      <ul>
        
        <li>Around Concept.</li>
         
        <li><a href="#entity-resilience" class="entity-link">Resilience</a>, <a href="#entity-security" class="entity-link">Security</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Category, Standard, Organization</div>
      <ul>
        
        <li>Around Category, Standard, Organization.</li>
         
        <li><a href="#entity-iso-31000:2018" class="entity-link">ISO 31000:2018</a>, <a href="#entity-accountability" class="entity-link">accountability</a>, <a href="#entity-governing-authorities" class="entity-link">governing authorities</a>, <a href="#entity-human-baseline" class="entity-link">human baseline</a>, <a href="#entity-individuals,-communities,-and-society" class="entity-link">individuals, communities, and society</a>, <a href="#entity-inscrutability" class="entity-link">inscrutability</a>, <a href="#entity-organizational-culture" class="entity-link">organizational culture</a>, <a href="#entity-risk-management" class="entity-link">risk management</a>, <a href="#entity-senior-leadership" class="entity-link">senior leadership</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Practice, Function, Document</div>
      <ul>
        
        <li>Around Practice, Function, Document.</li>
         
        <li><a href="#entity-ai-risk-management-training" class="entity-link">AI risk management training</a>, <a href="#entity-govern-function" class="entity-link">GOVERN function</a>, <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>, <a href="#entity-nist-trustworthy-and-responsible-ai-resource-center" class="entity-link">NIST Trustworthy and Responsible AI Resource Center</a>, <a href="#entity-executive-leadership" class="entity-link">executive leadership</a>, <a href="#entity-human-oversight-processes" class="entity-link">human oversight processes</a>, <a href="#entity-organizational-risk-priorities" class="entity-link">organizational risk priorities</a>, <a href="#entity-risk-criteria" class="entity-link">risk criteria</a>, <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Document</div>
      <ul>
        
        <li>Around Document.</li>
         
        <li><a href="#entity-govern-5.2" class="entity-link">GOVERN 5.2</a>, <a href="#entity-system-design" class="entity-link">system design</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Document</div>
      <ul>
        
        <li>Around Document.</li>
         
        <li><a href="#entity-govern-6" class="entity-link">GOVERN 6</a>, <a href="#entity-third-party-software" class="entity-link">third-party software</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Document</div>
      <ul>
        
        <li>Around Document.</li>
         
        <li><a href="#entity-govern-6.2" class="entity-link">GOVERN 6.2</a>, <a href="#entity-high-risk-systems" class="entity-link">high-risk systems</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Person, Risk</div>
      <ul>
        
        <li>Around Person, Risk.</li>
         
        <li><a href="#entity-interdisciplinary-ai-actors" class="entity-link">interdisciplinary AI actors</a>, <a href="#entity-organizational-risk-tolerances" class="entity-link">organizational risk tolerances</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Function</div>
      <ul>
        
        <li>Around Function.</li>
         
        <li><a href="#entity-ai-system-performance-and-trustworthiness" class="entity-link">AI system performance and trustworthiness</a>, <a href="#entity-operator-and-practitioner-proficiency" class="entity-link">operator and practitioner proficiency</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Category, Function, Organization</div>
      <ul>
        
        <li>Around Category, Function, Organization.</li>
         
        <li><a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, <a href="#entity-ai-risk-management-activities" class="entity-link">AI risk management activities</a>, <a href="#entity-ai-risks" class="entity-link">AI risks</a>, <a href="#entity-ai-system-categorization" class="entity-link">AI system categorization</a>, <a href="#entity-framework-users" class="entity-link">Framework users</a>, <a href="#entity-govern-5.1" class="entity-link">GOVERN 5.1</a>, <a href="#entity-human-factors" class="entity-link">Human Factors</a>, <a href="#entity-manage-function" class="entity-link">MANAGE function</a>, <a href="#entity-map-function" class="entity-link">MAP function</a>, <a href="#entity-measure-function" class="entity-link">MEASURE function</a>, <a href="#entity-other-ai-actors" class="entity-link">Other AI actors</a>, <a href="#entity-tevv-considerations" class="entity-link">TEVV considerations</a>, <a href="#entity-tevv-metrics" class="entity-link">TEVV metrics</a>, <a href="#entity-tevv-processes" class="entity-link">TEVV processes</a>, <a href="#entity-affected-communities" class="entity-link">affected communities</a>, <a href="#entity-contextually-sensitive-evaluations" class="entity-link">contextually sensitive evaluations</a>, <a href="#entity-cybersecurity" class="entity-link">cybersecurity</a>, <a href="#entity-domain-experts" class="entity-link">domain experts</a>, <a href="#entity-individuals-and-communities" class="entity-link">individuals and communities</a>, <a href="#entity-large-organizations" class="entity-link">large organizations</a>, <a href="#entity-metrics-and-measurement-methodologies" class="entity-link">metrics and measurement methodologies</a>, <a href="#entity-non-ai-alternative-systems" class="entity-link">non-AI alternative systems</a>, <a href="#entity-organizational-risk-tolerance" class="entity-link">organizational risk tolerance</a>, <a href="#entity-risk-monitoring-and-response-efforts" class="entity-link">risk monitoring and response efforts</a>, <a href="#entity-scientific-integrity" class="entity-link">scientific integrity</a>, <a href="#entity-small-to-medium-sized-organizations" class="entity-link">small to medium-sized organizations</a>, <a href="#entity-system-trustworthiness" class="entity-link">system trustworthiness</a>, <a href="#entity-trustworthy-ai-system" class="entity-link">trustworthy AI system</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Document</div>
      <ul>
        
        <li>Around Document.</li>
         
        <li><a href="#entity-current-profile" class="entity-link">Current Profile</a>, <a href="#entity-target-profile" class="entity-link">Target Profile</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Technology, Risk, Category</div>
      <ul>
        
        <li>Around Technology, Risk, Category.</li>
         
        <li><a href="#entity-ai-based-technology" class="entity-link">AI-based technology</a>, <a href="#entity-data-quality-issues" class="entity-link">data quality issues</a>, <a href="#entity-traditional-software" class="entity-link">traditional software</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Resource</div>
      <ul>
        
        <li>Around Resource.</li>
         
        <li><a href="#entity-ai-products-or-services" class="entity-link">AI products or services</a>, <a href="#entity-third-party-data" class="entity-link">third-party data</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Risk</div>
      <ul>
        
        <li>Around Risk.</li>
         
        <li><a href="#entity-emergent-risks" class="entity-link">emergent risks</a>, <a href="#entity-risk-management-efforts" class="entity-link">risk management efforts</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Category</div>
      <ul>
        
        <li>Around Category.</li>
         
        <li><a href="#entity-reliable-metrics" class="entity-link">reliable metrics</a>, <a href="#entity-risk-measurement-challenge" class="entity-link">risk measurement challenge</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Sector</div>
      <ul>
        
        <li>Around Sector.</li>
         
        <li><a href="#entity-civil-society" class="entity-link">civil society</a>, <a href="#entity-methods" class="entity-link">methods</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Standard</div>
      <ul>
        
        <li>Around Standard.</li>
         
        <li><a href="#entity-iso-9000:2015" class="entity-link">ISO 9000:2015</a>, <a href="#entity-validation" class="entity-link">validation</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Document</div>
      <ul>
        
        <li>Around Document.</li>
         
        <li><a href="#entity-govern-6.1" class="entity-link">GOVERN 6.1</a>, <a href="#entity-third-party-entities" class="entity-link">third-party entities</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">System</div>
      <ul>
        
        <li>Around System.</li>
         
        <li><a href="#entity-ai-system-components" class="entity-link">AI system components</a>, <a href="#entity-internal-risk-controls" class="entity-link">internal risk controls</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Standard</div>
      <ul>
        
        <li>Around Standard.</li>
         
        <li><a href="#entity-evaluations-involving-human-subjects" class="entity-link">evaluations involving human subjects</a>, <a href="#entity-human-subject-protection" class="entity-link">human subject protection</a></li>
        
      </ul>
    </div>
    
    <div class="card">
      <div class="tags">Organization</div>
      <ul>
        
        <li>Around Organization.</li>
         
        <li><a href="#entity-ai-actors" class="entity-link">AI Actors</a>, <a href="#entity-end-users" class="entity-link">End users</a>, <a href="#entity-third-party-entities" class="entity-link">Third-party entities</a></li>
        
      </ul>
    </div>
     
    <h2 id="chunks">Content</h2>
    
    <div class="toggle" onclick="toggleChunk('chunk-0')">
      Show original text
    </div>
    <div class="card" id="chunk-0-rephrase">
      <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: Artificial Intelligence <a href="#entity-risk-management-framework" class="entity-link">Risk Management Framework</a> (<a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>) is a publication from the <a href="#entity-u.s.-department-of-commerce" class="entity-link">U.S. Department of Commerce</a>, released in January 2023. It is available for free at https://doi.org/10.6028/<a href="#entity-nist.ai.100-1" class="entity-link">NIST.AI.100-1</a>. The Secretary of Commerce is <a href="#entity-gina-m.-raimondo" class="entity-link">Gina M. Raimondo</a>, and the Director of <a href="#entity-nist" class="entity-link">NIST</a> is <a href="#entity-laurie-e.-locascio" class="entity-link">Laurie E. Locascio</a>. 

This document may mention specific companies, equipment, or materials to explain procedures or concepts, but this does not mean that <a href="#entity-nist" class="entity-link">NIST</a> recommends or endorses them as the best options. 

The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> is designed to be updated regularly. <a href="#entity-nist" class="entity-link">NIST</a> will assess its content and usefulness, with a formal review involving the <a href="#entity-ai-community" class="entity-link">AI community</a> expected by 2028. The Framework will use a two-number versioning system: the first number indicates the main version (e.g., 1.0), which changes with major updates, while minor updates will be indicated by an additional number (e.g., 1.1). A <a href="#entity-version-control-table" class="entity-link">Version Control Table</a> will track all changes, including version numbers, dates, and descriptions.
    </div>
    <div class="chunk-original" id="chunk-0-original">
      NIST AI 100-1
Artificial Intelligence Risk Management
Framework (AI RMF 1.0)


NIST AI 100-1
Artificial Intelligence Risk Management
Framework (AI RMF 1.0)
This publication is available free of charge from:
https://doi.org/10.6028/NIST.AI.100-1
January 2023
U.S. Department of Commerce
Gina M. Raimondo, Secretary
National Institute of Standards and Technology
Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology


Certain commercial entities, equipment, or materials may be identified in this document in order to describe 
an experimental procedure or concept adequately. Such identification is not intended to imply recommenda-
tion or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that 
the entities, materials, or equipment are necessarily the best available for the purpose. 
This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1
Update Schedule and Versions
The Artificial Intelligence Risk Management Framework (AI RMF) is intended to be a living document.
NIST will review the content and usefulness of the Framework regularly to determine if an update is appro-
priate; a review with formal input from the AI community is expected to take place no later than 2028. The
Framework will employ a two-number versioning system to track and identify major and minor changes. The
first number will represent the generation of the AI RMF and its companion documents (e.g., 1.0) and will
change only with major revisions. Minor revisions will be tracked using “.n” after the generation number
(e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including
version number, date of change, and description of change.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-1')">
      Show original text
    </div>
    <div class="card" id="chunk-1-rephrase">
      Changes will be tracked using a format that includes a generation number followed by &#x27;.n&#x27; (for example, 1.1). A <a href="#entity-version-control-table" class="entity-link">Version Control Table</a> will document the history of changes, including the version number, date of change, and a description of the change. <a href="#entity-nist" class="entity-link">NIST</a> intends to update the <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a> regularly. Feedback on the <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a> can be sent anytime to AIframework@<a href="#entity-nist" class="entity-link">nist</a>.gov and will be reviewed and incorporated every six months.

Table of Contents
Executive Summary
1
Part 1: <a href="#entity-foundational-information" class="entity-link">Foundational Information</a>
4
1
Framing Risk
4
1.1
Understanding and Addressing Risks, Impacts, and Harms
4
1.2
Challenges for <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a>
5
1.2.1
Risk Measurement
5
1.2.2
<a href="#entity-risk-tolerance" class="entity-link">Risk Tolerance</a>
7
1.2.3
Risk Prioritization
7
1.2.4
Organizational Integration and Management of Risk
8
2
Audience
9
3
<a href="#entity-ai-risks-and-trustworthiness" class="entity-link">AI Risks and Trustworthiness</a>
12
3.1
Valid and Reliable
13
3.2
Safe
14
3.3
Secure and Resilient
15
3.4
Accountable and Transparent
15
3.5
Explainable and Interpretable
16
3.6
<a href="#entity-privacy" class="entity-link">Privacy</a>-Enhanced
17
3.7
Fair – with <a href="#entity-harmful-bias" class="entity-link">Harmful Bias</a> Managed
17
4
Effectiveness of the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>
19
Part 2: <a href="#entity-core-and-profiles" class="entity-link">Core and Profiles</a>
20
5
<a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>
20
5.1
<a href="#entity-govern" class="entity-link">Govern</a>
21
5.2
<a href="#entity-map" class="entity-link">Map</a>
24
5.3
<a href="#entity-measure" class="entity-link">Measure</a>
28
5.
    </div>
    <div class="chunk-original" id="chunk-1-original">
       will be tracked using “.n” after the generation number
(e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including
version number, date of change, and description of change. NIST plans to update the AI RMF Playbook
frequently. Comments on the AI RMF Playbook may be sent via email to AIframework@nist.gov at any time
and will be reviewed and integrated on a semi-annual basis.


Table of Contents
Executive Summary
1
Part 1: Foundational Information
4
1
Framing Risk
4
1.1
Understanding and Addressing Risks, Impacts, and Harms
4
1.2
Challenges for AI Risk Management
5
1.2.1
Risk Measurement
5
1.2.2
Risk Tolerance
7
1.2.3
Risk Prioritization
7
1.2.4
Organizational Integration and Management of Risk
8
2
Audience
9
3
AI Risks and Trustworthiness
12
3.1
Valid and Reliable
13
3.2
Safe
14
3.3
Secure and Resilient
15
3.4
Accountable and Transparent
15
3.5
Explainable and Interpretable
16
3.6
Privacy-Enhanced
17
3.7
Fair – with Harmful Bias Managed
17
4
Effectiveness of the AI RMF
19
Part 2: Core and Profiles
20
5
AI RMF Core
20
5.1
Govern
21
5.2
Map
24
5.3
Measure
28
5.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-2')">
      Show original text
    </div>
    <div class="card" id="chunk-2-rephrase">
      The document outlines the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) and is divided into several sections. It includes the following key parts: 

1. **<a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>**: This section covers four main functions: 
   - **<a href="#entity-govern" class="entity-link">Govern</a>** (Page 21) 
   - **<a href="#entity-map" class="entity-link">Map</a>** (Page 24) 
   - **<a href="#entity-measure" class="entity-link">Measure</a>** (Page 28) 
   - **<a href="#entity-manage" class="entity-link">Manage</a>** (Page 31) 

2. **<a href="#entity-ai-rmf-profiles" class="entity-link">AI RMF Profiles</a>** (Page 33) 

3. **Appendices**: 
   - Appendix A: Describes tasks performed by <a href="#entity-ai-actors" class="entity-link">AI actors</a> (Page 35) 
   - Appendix B: Discusses how <a href="#entity-ai-risks" class="entity-link">AI risks</a> differ from <a href="#entity-traditional-software" class="entity-link">traditional software</a> risks (Page 38) 
   - Appendix C: Covers <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> and human-AI interaction (Page 40) 
   - Appendix D: Lists attributes of the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> (Page 42) 

**Tables**: 
- Table 1: Categories for the <a href="#entity-govern-function" class="entity-link">GOVERN function</a> (Page 22) 
- Table 2: Categories for the <a href="#entity-map-function" class="entity-link">MAP function</a> (Page 26) 
- Table 3: Categories for the <a href="#entity-measure-function" class="entity-link">MEASURE function</a> (Page 29) 
- Table 4: Categories for the <a href="#entity-manage-function" class="entity-link">MANAGE function</a> (Page 32) 

**Figures**: 
- Figure 1: Shows potential harms related to <a href="#entity-ai-systems" class="entity-link">AI systems</a> and how <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a> can reduce risks (Page 5) 
- Figure 2: Illustrates the lifecycle and key dimensions of an <a href="#entity-ai-system" class="entity-link">AI system</a>, emphasizing the importance of <a href="#entity-risk-management" class="entity-link">risk management</a> throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> (Page 10) 
- Figure 3: Depicts <a href="#entity-ai-actors" class="entity-link">AI actors</a> involved in different stages of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, with detailed task descriptions in Appendix A.
    </div>
    <div class="chunk-original" id="chunk-2-original">
       RMF
19
Part 2: Core and Profiles
20
5
AI RMF Core
20
5.1
Govern
21
5.2
Map
24
5.3
Measure
28
5.4
Manage
31
6
AI RMF Profiles
33
Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3
35
Appendix B: How AI Risks Differ from Traditional Software Risks
38
Appendix C: AI Risk Management and Human-AI Interaction
40
Appendix D: Attributes of the AI RMF
42
List of Tables
Table 1 Categories and subcategories for the GOVERN function.
22
Table 2 Categories and subcategories for the MAP function.
26
Table 3 Categories and subcategories for the MEASURE function.
29
Table 4 Categories and subcategories for the MANAGE function.
32
i


NIST AI 100-1
AI RMF 1.0
List of Figures
Fig. 1
Examples of potential harms related to AI systems. Trustworthy AI systems
and their responsible use can mitigate negative risks and contribute to bene-
fits for people, organizations, and ecosystems.
5
Fig. 2
Lifecycle and Key Dimensions of an AI System. Modified from OECD
(2022) OECD Framework for the Classification of AI systems — OECD
Digital Economy Papers. The two inner circles show AI systems’ key di-
mensions and the outer circle shows AI lifecycle stages. Ideally, risk man-
agement efforts start with the Plan and Design function in the application
context and are performed throughout the AI system lifecycle. See Figure 3
for representative AI actors.
10
Fig. 3
AI actors across AI lifecycle stages. See Appendix A for detailed descrip-
tions of AI actor tasks, including details about testing, evaluation, verifica-
tion, and validation tasks.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-3')">
      Show original text
    </div>
    <div class="card" id="chunk-3-rephrase">
      This document discusses the roles of <a href="#entity-ai-actors" class="entity-link">AI actors</a> throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. For detailed descriptions of their tasks, including testing and validation, refer to Appendix A. It is important to note that in the AI Model dimension, those who build and use AI models are separated from those who verify and validate them as a best practice. 

The characteristics of <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a> are outlined, with &#x27;Valid &amp; Reliable&#x27; as the foundation of trustworthiness. &#x27;Accountable &amp; Transparent&#x27; is emphasized as it relates to all other characteristics. 

Additionally, <a href="#entity-ai-risk-management-activities" class="entity-link">AI risk management activities</a> are organized into functions that <a href="#entity-govern" class="entity-link">govern</a>, <a href="#entity-map" class="entity-link">map</a>, <a href="#entity-measure" class="entity-link">measure</a>, and <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a>. Governance is intended to be integrated across all functions. 

In the Executive Summary of <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a> <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>, it highlights that <a href="#entity-ai-technologies" class="entity-link">AI technologies</a> have the potential to greatly impact society positively, enhancing areas like commerce, health, transportation, <a href="#entity-cybersecurity" class="entity-link">cybersecurity</a>, and environmental conditions. However, these technologies also carry risks that can adversely affect individuals, groups, <a href="#entity-organizations" class="entity-link">organizations</a>, and the environment. <a href="#entity-ai-risks" class="entity-link">AI risks</a> can vary in duration, probability, scope, and impact.
    </div>
    <div class="chunk-original" id="chunk-3-original">
      
for representative AI actors.
10
Fig. 3
AI actors across AI lifecycle stages. See Appendix A for detailed descrip-
tions of AI actor tasks, including details about testing, evaluation, verifica-
tion, and validation tasks. Note that AI actors in the AI Model dimension
(Figure 2) are separated as a best practice, with those building and using the
models separated from those verifying and validating the models.
11
Fig. 4
Characteristics of trustworthy AI systems. Valid &amp; Reliable is a necessary
condition of trustworthiness and is shown as the base for other trustworthi-
ness characteristics. Accountable &amp; Transparent is shown as a vertical box
because it relates to all other characteristics.
12
Fig. 5
Functions organize AI risk management activities at their highest level to
govern, map, measure, and manage AI risks. Governance is designed to be
a cross-cutting function to inform and be infused throughout the other three
functions.
20
Page ii


NIST AI 100-1
AI RMF 1.0
Executive Summary
Artificial intelligence (AI) technologies have significant potential to transform society and
people’s lives – from commerce and health to transportation and cybersecurity to the envi-
ronment and our planet. AI technologies can drive inclusive economic growth and support
scientific advancements that improve the conditions of our world. AI technologies, how-
ever, also pose risks that can negatively impact individuals, groups, organizations, commu-
nities, society, the environment, and the planet. Like risks for other types of technology, AI
risks can emerge in a variety of ways and can be characterized as long- or short-term, high-
or low-probability, systemic or localized, and high- or low-impact.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-4')">
      Show original text
    </div>
    <div class="card" id="chunk-4-rephrase">
      <a href="#entity-ai-technology" class="entity-link">AI technology</a>, like other technologies, carries various risks that can be categorized in different ways: they can be long-term or short-term, high-probability or low-probability, systemic or localized, and high-impact or low-impact. The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) defines an <a href="#entity-ai-system" class="entity-link">AI system</a> as a machine-based system designed to achieve specific goals by producing outputs such as predictions, recommendations, or decisions that affect real or virtual environments. These systems can operate with different levels of autonomy (Adapted from: <a href="#entity-oecd-recommendation-on-ai:2019" class="entity-link">OECD Recommendation on AI:2019</a>; <a href="#entity-iso/iec-22989:2022" class="entity-link">ISO/IEC 22989:2022</a>). 

While there are many standards and best practices to help <a href="#entity-organizations" class="entity-link">organizations</a> <a href="#entity-manage" class="entity-link">manage</a> risks associated with <a href="#entity-traditional-software" class="entity-link">traditional software</a>, <a href="#entity-ai-systems" class="entity-link">AI systems</a> present unique challenges (See Appendix B). For instance, <a href="#entity-ai-systems" class="entity-link">AI systems</a> can be trained on data that changes over time, sometimes in unexpected ways, which can impact their functionality and reliability. Additionally, the complexity of <a href="#entity-ai-systems" class="entity-link">AI systems</a> and their deployment contexts makes it hard to identify and address failures. 

<a href="#entity-ai-systems" class="entity-link">AI systems</a> are also socio-technical, meaning they are affected by social dynamics and human behavior. The risks and benefits of AI arise from the interaction between technical features and societal factors, including how the system is used, its interactions with other <a href="#entity-ai-systems" class="entity-link">AI systems</a>, who operates it, and the social context of its deployment. 

These factors make AI a particularly challenging technology for <a href="#entity-organizations" class="entity-link">organizations</a> and society to implement and use. Without proper controls, <a href="#entity-ai-systems" class="entity-link">AI systems</a> can worsen inequities or lead to negative outcomes for individuals and communities. However, with the right controls in place, <a href="#entity-ai-systems" class="entity-link">AI systems</a> can help reduce and <a href="#entity-manage" class="entity-link">manage</a> these inequities. 

Effective <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> is essential for the responsible development and use of <a href="#entity-ai-systems" class="entity-link">AI systems</a>.
    </div>
    <div class="chunk-original" id="chunk-4-original">
       Like risks for other types of technology, AI
risks can emerge in a variety of ways and can be characterized as long- or short-term, high-
or low-probability, systemic or localized, and high- or low-impact.
The AI RMF refers to an AI system as an engineered or machine-based system that
can, for a given set of objectives, generate outputs such as predictions, recommenda-
tions, or decisions influencing real or virtual environments. AI systems are designed
to operate with varying levels of autonomy (Adapted from: OECD Recommendation
on AI:2019; ISO/IEC 22989:2022).
While there are myriad standards and best practices to help organizations mitigate the risks
of traditional software or information-based systems, the risks posed by AI systems are in
many ways unique (See Appendix B). AI systems, for example, may be trained on data that
can change over time, sometimes significantly and unexpectedly, affecting system function-
ality and trustworthiness in ways that are hard to understand. AI systems and the contexts
in which they are deployed are frequently complex, making it difficult to detect and respond
to failures when they occur. AI systems are inherently socio-technical in nature, meaning
they are influenced by societal dynamics and human behavior. AI risks – and benefits –
can emerge from the interplay of technical aspects combined with societal factors related
to how a system is used, its interactions with other AI systems, who operates it, and the
social context in which it is deployed.
These risks make AI a uniquely challenging technology to deploy and utilize both for orga-
nizations and within society. Without proper controls, AI systems can amplify, perpetuate,
or exacerbate inequitable or undesirable outcomes for individuals and communities. With
proper controls, AI systems can mitigate and manage inequitable outcomes.
AI risk management is a key component of responsible development and use of AI sys-
tems.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-5')">
      Show original text
    </div>
    <div class="card" id="chunk-5-rephrase">
      <a href="#entity-ai-systems" class="entity-link">AI systems</a> can sometimes create or worsen unfair outcomes for people and communities. However, with the right controls, these systems can help reduce and <a href="#entity-manage" class="entity-link">manage</a> these issues. Managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> is essential for the responsible development and use of AI. By following <a href="#entity-responsible-ai" class="entity-link">responsible AI</a> practices, <a href="#entity-organizations" class="entity-link">organizations</a> can ensure that their decisions about designing, developing, and using AI align with their goals and values. Key principles of <a href="#entity-responsible-ai" class="entity-link">responsible AI</a> focus on putting people first, being socially responsible, and promoting sustainability. Effective <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> encourages <a href="#entity-organizations" class="entity-link">organizations</a> and their teams to carefully consider the context and possible positive or negative effects of AI. By understanding and managing these risks, <a href="#entity-organizations" class="entity-link">organizations</a> can build trust and foster public confidence in <a href="#entity-ai-technologies" class="entity-link">AI technologies</a>.

Social responsibility means that <a href="#entity-organizations" class="entity-link">organizations</a> must consider how their decisions and actions affect society and the environment, acting transparently and ethically. Sustainability refers to meeting current needs without compromising future generations&#x27; ability to meet theirs. <a href="#entity-responsible-ai" class="entity-link">Responsible AI</a> aims to create technology that is fair and accountable. <a href="#entity-organizations" class="entity-link">Organizations</a> are expected to follow professional responsibility, which means that those who design, develop, or deploy <a href="#entity-ai-systems" class="entity-link">AI systems</a> should recognize their influence on society and the future of AI, as defined by ISO standards.
    </div>
    <div class="chunk-original" id="chunk-5-original">
      uate,
or exacerbate inequitable or undesirable outcomes for individuals and communities. With
proper controls, AI systems can mitigate and manage inequitable outcomes.
AI risk management is a key component of responsible development and use of AI sys-
tems. Responsible AI practices can help align the decisions about AI system design, de-
velopment, and uses with intended aim and values. Core concepts in responsible AI em-
phasize human centricity, social responsibility, and sustainability. AI risk management can
drive responsible uses and practices by prompting organizations and their internal teams
who design, develop, and deploy AI to think more critically about context and potential
or unexpected negative and positive impacts. Understanding and managing the risks of AI
systems will help to enhance trustworthiness, and in turn, cultivate public trust.
Page 1


NIST AI 100-1
AI RMF 1.0
Social responsibility can refer to the organization’s responsibility “for the impacts
of its decisions and activities on society and the environment through transparent
and ethical behavior” (ISO 26000:2010). Sustainability refers to the “state of the
global system, including environmental, social, and economic aspects, in which the
needs of the present are met without compromising the ability of future generations
to meet their own needs” (ISO/IEC TR 24368:2022). Responsible AI is meant to
result in technology that is also equitable and accountable. The expectation is that
organizational practices are carried out in accord with “professional responsibility,”
defined by ISO as an approach that “aims to ensure that professionals who design,
develop, or deploy AI systems and applications or AI-based products or systems,
recognize their unique position to exert influence on people, society, and the future
of AI” (ISO/IEC TR 24368:2022).
As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-6')">
      Show original text
    </div>
    <div class="card" id="chunk-6-rephrase">
      The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> (Artificial Intelligence <a href="#entity-risk-management-framework" class="entity-link">Risk Management Framework</a>) aims to help <a href="#entity-organizations" class="entity-link">organizations</a> that design, develop, deploy, or use <a href="#entity-ai-systems" class="entity-link">AI systems</a> <a href="#entity-manage" class="entity-link">manage</a> risks and promote <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a> practices. This initiative is guided by the <a href="#entity-national-artificial-intelligence-initiative-act-of-2020" class="entity-link">National Artificial Intelligence Initiative Act of 2020</a>. The Framework is voluntary, respects rights, and is applicable across various sectors and use cases, allowing <a href="#entity-organizations" class="entity-link">organizations</a> of all sizes to adopt its approaches. It is designed to help both <a href="#entity-organizations" class="entity-link">organizations</a> and individuals, known as <a href="#entity-ai-actors" class="entity-link">AI actors</a>, enhance the trustworthiness of <a href="#entity-ai-systems" class="entity-link">AI systems</a> and ensure responsible practices in their design, development, deployment, and use. <a href="#entity-ai-actors" class="entity-link">AI actors</a> are defined by the <a href="#entity-oecd" class="entity-link">OECD</a> as those actively involved in the <a href="#entity-ai-system-lifecycle" class="entity-link">AI system lifecycle</a>, including <a href="#entity-organizations" class="entity-link">organizations</a> and individuals who deploy or operate AI. The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> is practical and adaptable, aiming to evolve alongside <a href="#entity-ai-technologies" class="entity-link">AI technologies</a> while ensuring societal benefits and protection from potential harms. The Framework and its resources will be regularly updated based on technological advancements, global standards, and feedback from the <a href="#entity-ai-community" class="entity-link">AI community</a>. <a href="#entity-nist" class="entity-link">NIST</a> will continue to align the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> with relevant international standards and guidelines.
    </div>
    <div class="chunk-original" id="chunk-6-original">
      recognize their unique position to exert influence on people, society, and the future
of AI” (ISO/IEC TR 24368:2022).
As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283),
the goal of the AI RMF is to offer a resource to the organizations designing, developing,
deploying, or using AI systems to help manage the many risks of AI and promote trustwor-
thy and responsible development and use of AI systems. The Framework is intended to be
voluntary, rights-preserving, non-sector-specific, and use-case agnostic, providing flexibil-
ity to organizations of all sizes and in all sectors and throughout society to implement the
approaches in the Framework.
The Framework is designed to equip organizations and individuals – referred to here as
AI actors – with approaches that increase the trustworthiness of AI systems, and to help
foster the responsible design, development, deployment, and use of AI systems over time.
AI actors are defined by the Organisation for Economic Co-operation and Development
(OECD) as “those who play an active role in the AI system lifecycle, including organiza-
tions and individuals that deploy or operate AI” [OECD (2019) Artificial Intelligence in
Society—OECD iLibrary] (See Appendix A).
The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies
continue to develop, and to be operationalized by organizations in varying degrees and
capacities so society can benefit from AI while also being protected from its potential
harms.
The Framework and supporting resources will be updated, expanded, and improved based
on evolving technology, the standards landscape around the world, and AI community ex-
perience and feedback. NIST will continue to align the AI RMF and related guidance with
applicable international standards, guidelines, and practices.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-7')">
      Show original text
    </div>
    <div class="card" id="chunk-7-rephrase">
      The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) is being updated to keep pace with new technology, global standards, and feedback from the <a href="#entity-ai-community" class="entity-link">AI community</a>. <a href="#entity-nist" class="entity-link">NIST</a> will ensure that the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> aligns with relevant international standards and guidelines. As <a href="#entity-organizations" class="entity-link">organizations</a> start using the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>, they will gain insights that will help improve future versions and resources. 

The Framework has two main parts. Part 1 explains how <a href="#entity-organizations" class="entity-link">organizations</a> can identify and <a href="#entity-manage" class="entity-link">manage</a> AI-related risks and describes who should use the framework. It also analyzes <a href="#entity-ai-risks" class="entity-link">AI risks</a> and what makes <a href="#entity-ai-systems" class="entity-link">AI systems</a> trustworthy. <a href="#entity-trustworthy-ai-systems" class="entity-link">Trustworthy AI systems</a> should be valid, reliable, safe, secure, resilient, accountable, transparent, explainable, interpretable, <a href="#entity-privacy" class="entity-link">privacy</a>-enhancing, and fair, with efforts to <a href="#entity-manage" class="entity-link">manage</a> harmful biases. 

Part 2 is the core of the Framework and outlines four key functions to help <a href="#entity-organizations" class="entity-link">organizations</a> <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a>: <a href="#entity-govern" class="entity-link">GOVERN</a>, <a href="#entity-map" class="entity-link">MAP</a>, <a href="#entity-measure" class="entity-link">MEASURE</a>, and <a href="#entity-manage" class="entity-link">MANAGE</a>. These functions are further divided into categories and subcategories. <a href="#entity-govern" class="entity-link">GOVERN</a> applies to all stages of <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>, while <a href="#entity-map" class="entity-link">MAP</a>, <a href="#entity-measure" class="entity-link">MEASURE</a>, and <a href="#entity-manage" class="entity-link">MANAGE</a> are specific to certain contexts and stages of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. 

Additional resources related to the Framework can be found in the <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>, available on the <a href="#entity-nist" class="entity-link">NIST</a> <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> website: https://www.<a href="#entity-nist" class="entity-link">nist</a>.gov/itl/ai-risk-management-framework. 

<a href="#entity-nist" class="entity-link">NIST</a> developed the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> in collaboration with both private and public sectors, in line with broader AI initiatives outlined in the <a href="#entity-national-ai-initiative-act-of-2020" class="entity-link">National AI Initiative Act of 2020</a>, recommendations from the <a href="#entity-national-security-commission-on-artificial-intelligence" class="entity-link">National Security Commission on Artificial Intelligence</a>, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools.
    </div>
    <div class="chunk-original" id="chunk-7-original">
      , and improved based
on evolving technology, the standards landscape around the world, and AI community ex-
perience and feedback. NIST will continue to align the AI RMF and related guidance with
applicable international standards, guidelines, and practices. As the AI RMF is put into
use, additional lessons will be learned to inform future updates and additional resources.
The Framework is divided into two parts. Part 1 discusses how organizations can frame
the risks related to AI and describes the intended audience. Next, AI risks and trustworthi-
ness are analyzed, outlining the characteristics of trustworthy AI systems, which include
Page 2


NIST AI 100-1
AI RMF 1.0
valid and reliable, safe, secure and resilient, accountable and transparent, explainable and
interpretable, privacy enhanced, and fair with their harmful biases managed.
Part 2 comprises the “Core” of the Framework. It describes four specific functions to help
organizations address the risks of AI systems in practice. These functions – GOVERN,
MAP, MEASURE, and MANAGE – are broken down further into categories and subcate-
gories. While GOVERN applies to all stages of organizations’ AI risk management pro-
cesses and procedures, the MAP, MEASURE, and MANAGE functions can be applied in AI
system-specific contexts and at specific stages of the AI lifecycle.
Additional resources related to the Framework are included in the AI RMF Playbook,
which is available via the NIST AI RMF website:
https://www.nist.gov/itl/ai-risk-management-framework.
Development of the AI RMF by NIST in collaboration with the private and public sec-
tors is directed and consistent with its broader AI efforts called for by the National AI
Initiative Act of 2020, the National Security Commission on Artificial Intelligence recom-
mendations, and the Plan for Federal Engagement in Developing Technical Standards and
Related Tools.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-8')">
      Show original text
    </div>
    <div class="card" id="chunk-8-rephrase">
      The <a href="#entity-national-institute-of-standards-and-technology" class="entity-link">National Institute of Standards and Technology</a> (<a href="#entity-nist" class="entity-link">NIST</a>) is working on an <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>) as part of its broader AI initiatives, which are guided by the <a href="#entity-national-ai-initiative-act-of-2020" class="entity-link">National AI Initiative Act of 2020</a>, recommendations from the <a href="#entity-national-security-commission-on-artificial-intelligence" class="entity-link">National Security Commission on Artificial Intelligence</a>, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools. To develop this framework, <a href="#entity-nist" class="entity-link">NIST</a> engaged with the <a href="#entity-ai-community" class="entity-link">AI community</a> through various means, including a formal Request for Information, three workshops, public comments on a concept paper and two drafts, discussions at public forums, and small group meetings. This engagement has shaped the <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a> and influenced AI research and development by <a href="#entity-nist" class="entity-link">NIST</a> and others. Future research and guidance to improve this framework will be included in an <a href="#entity-ai-risk-management-framework-roadmap" class="entity-link">AI Risk Management Framework Roadmap</a>, which will allow contributions from <a href="#entity-nist" class="entity-link">NIST</a> and the wider community.

Part 1: <a href="#entity-foundational-information" class="entity-link">Foundational Information</a>

1. Framing Risk
<a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> aims to reduce the potential negative effects of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, such as threats to civil liberties, while also maximizing their positive impacts. Effectively addressing and managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> can lead to more <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a>.

1.1 Understanding and Addressing Risks, Impacts, and Harms
In the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> context, risk is defined as the likelihood of an event occurring combined with the severity of its consequences. The impacts of <a href="#entity-ai-systems" class="entity-link">AI systems</a> can be positive, negative, or both, leading to either opportunities or threats (Adapted from: <a href="#entity-iso-31000:2018" class="entity-link">ISO 31000:2018</a>).
    </div>
    <div class="chunk-original" id="chunk-8-original">
       and consistent with its broader AI efforts called for by the National AI
Initiative Act of 2020, the National Security Commission on Artificial Intelligence recom-
mendations, and the Plan for Federal Engagement in Developing Technical Standards and
Related Tools. Engagement with the AI community during this Framework’s development
– via responses to a formal Request for Information, three widely attended workshops,
public comments on a concept paper and two drafts of the Framework, discussions at mul-
tiple public forums, and many small group meetings – has informed development of the AI
RMF 1.0 as well as AI research and development and evaluation conducted by NIST and
others. Priority research and additional guidance that will enhance this Framework will be
captured in an associated AI Risk Management Framework Roadmap to which NIST and
the broader community can contribute.
Page 3


NIST AI 100-1
AI RMF 1.0
Part 1: Foundational Information
1.
Framing Risk
AI risk management offers a path to minimize potential negative impacts of AI systems,
such as threats to civil liberties and rights, while also providing opportunities to maximize
positive impacts. Addressing, documenting, and managing AI risks and potential negative
impacts effectively can lead to more trustworthy AI systems.
1.1
Understanding and Addressing Risks, Impacts, and Harms
In the context of the AI RMF, risk refers to the composite measure of an event’s probability
of occurring and the magnitude or degree of the consequences of the corresponding event.
The impacts, or consequences, of AI systems can be positive, negative, or both and can
result in opportunities or threats (Adapted from: ISO 31000:2018).
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-9')">
      Show original text
    </div>
    <div class="card" id="chunk-9-rephrase">
      The consequences of <a href="#entity-ai-systems" class="entity-link">AI systems</a> can be positive, negative, or a mix of both, leading to opportunities or threats (Adapted from: <a href="#entity-iso-31000:2018" class="entity-link">ISO 31000:2018</a>). When evaluating the negative effects of a potential event, risk is determined by two factors: 1) the severity of harm that could occur if the event happens, and 2) the likelihood of that event occurring (Adapted from: OMB Circular A-130:2016). Negative impacts can affect individuals, groups, communities, <a href="#entity-organizations" class="entity-link">organizations</a>, society, the environment, and the planet.

<a href="#entity-risk-management" class="entity-link">Risk management</a> involves organized efforts to guide and control an organization regarding risks (Source: <a href="#entity-iso-31000:2018" class="entity-link">ISO 31000:2018</a>). While traditional <a href="#entity-risk-management" class="entity-link">risk management</a> focuses on addressing negative impacts, this framework also aims to reduce expected negative effects of <a href="#entity-ai-systems" class="entity-link">AI systems</a> and identify ways to enhance positive impacts. By effectively managing potential harms, we can create more <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a> that benefit individuals, communities, <a href="#entity-organizations" class="entity-link">organizations</a>, and ecosystems. <a href="#entity-risk-management" class="entity-link">Risk management</a> helps AI developers and users understand the impacts of their systems and recognize the limitations and uncertainties involved, which can improve overall performance and trustworthiness, increasing the chances that <a href="#entity-ai-technologies" class="entity-link">AI technologies</a> will be used beneficially.

The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) is designed to adapt to new risks as they arise, which is crucial since some impacts may not be easily predicted and AI applications are constantly evolving. Although some risks and benefits of AI are well understood, assessing negative impacts and the extent of harm can be difficult. Figure 1 illustrates examples of potential harms associated with <a href="#entity-ai-systems" class="entity-link">AI systems</a>.
    </div>
    <div class="chunk-original" id="chunk-9-original">
       magnitude or degree of the consequences of the corresponding event.
The impacts, or consequences, of AI systems can be positive, negative, or both and can
result in opportunities or threats (Adapted from: ISO 31000:2018). When considering the
negative impact of a potential event, risk is a function of 1) the negative impact, or magni-
tude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of
occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be
experienced by individuals, groups, communities, organizations, society, the environment,
and the planet.
“Risk management refers to coordinated activities to direct and control an organiza-
tion with regard to risk” (Source: ISO 31000:2018).
While risk management processes generally address negative impacts, this Framework of-
fers approaches to minimize anticipated negative impacts of AI systems and identify op-
portunities to maximize positive impacts. Effectively managing the risk of potential harms
could lead to more trustworthy AI systems and unleash potential benefits to people (individ-
uals, communities, and society), organizations, and systems/ecosystems. Risk management
can enable AI developers and users to understand impacts and account for the inherent lim-
itations and uncertainties in their models and systems, which in turn can improve overall
system performance and trustworthiness and the likelihood that AI technologies will be
used in ways that are beneficial.
The AI RMF is designed to address new risks as they emerge. This flexibility is particularly
important where impacts are not easily foreseeable and applications are evolving. While
some AI risks and benefits are well-known, it can be challenging to assess negative impacts
and the degree of harms. Figure 1 provides examples of potential harms that can be related
to AI systems.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-10')">
      Show original text
    </div>
    <div class="card" id="chunk-10-rephrase">
      AI applications are rapidly evolving, and while some risks and benefits of AI are well understood, it can be difficult to evaluate the negative impacts and extent of harm. Figure 1 shows examples of potential harms associated with <a href="#entity-ai-systems" class="entity-link">AI systems</a>. 

When managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>, it&#x27;s important to recognize that people often assume <a href="#entity-ai-systems" class="entity-link">AI systems</a> function effectively in all situations. Many believe AI is more objective than humans or has greater capabilities than <a href="#entity-traditional-software" class="entity-link">traditional software</a>, regardless of whether this is true. 

<a href="#entity-trustworthy-ai-systems" class="entity-link">Trustworthy AI systems</a>, when used responsibly, can help reduce risks and provide benefits to individuals, <a href="#entity-organizations" class="entity-link">organizations</a>, and ecosystems. 

### Challenges in <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> 

There are several challenges to consider when managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> to ensure trustworthiness: 

1. **Risk Measurement**: It is hard to <a href="#entity-measure" class="entity-link">measure</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a> or failures that are not clearly defined or understood. Just because a risk is difficult to <a href="#entity-measure" class="entity-link">measure</a> does not mean it is necessarily high or low. Some challenges in risk measurement include: 
   - **<a href="#entity-third-party-software,-hardware,-and-data" class="entity-link">Third-party Software, Hardware, and Data</a>**: Using <a href="#entity-third-party-data" class="entity-link">third-party data</a> or systems can speed up research and development but can also complicate risk measurement. Risks can arise from the <a href="#entity-third-party-data" class="entity-link">third-party data</a>, software, or hardware itself, as well as from how they are used. The <a href="#entity-risk-metrics" class="entity-link">risk metrics</a> used by the organization developing the AI may not match those used by the organization deploying or operating it. Additionally, the developers may not be transparent about the <a href="#entity-risk-metrics" class="entity-link">risk metrics</a> or methods they employed.
    </div>
    <div class="chunk-original" id="chunk-10-original">
       easily foreseeable and applications are evolving. While
some AI risks and benefits are well-known, it can be challenging to assess negative impacts
and the degree of harms. Figure 1 provides examples of potential harms that can be related
to AI systems.
AI risk management efforts should consider that humans may assume that AI systems work
– and work well – in all settings. For example, whether correct or not, AI systems are
often perceived as being more objective than humans or as offering greater capabilities
than general software.
Page 4


NIST AI 100-1
AI RMF 1.0
Fig. 1. Examples of potential harms related to AI systems. Trustworthy AI systems and their
responsible use can mitigate negative risks and contribute to benefits for people, organizations, and
ecosystems.
1.2
Challenges for AI Risk Management
Several challenges are described below. They should be taken into account when managing
risks in pursuit of AI trustworthiness.
1.2.1
Risk Measurement
AI risks or failures that are not well-defined or adequately understood are difficult to mea-
sure quantitatively or qualitatively. The inability to appropriately measure AI risks does not
imply that an AI system necessarily poses either a high or low risk. Some risk measurement
challenges include:
Risks related to third-party software, hardware, and data: Third-party data or systems
can accelerate research and development and facilitate technology transition. They also
may complicate risk measurement. Risk can emerge both from third-party data, software or
hardware itself and how it is used. Risk metrics or methodologies used by the organization
developing the AI system may not align with the risk metrics or methodologies uses by
the organization deploying or operating the system. Also, the organization developing
the AI system may not be transparent about the risk metrics or methodologies it used.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-11')">
      Show original text
    </div>
    <div class="card" id="chunk-11-rephrase">
      The organization that creates the <a href="#entity-ai-system" class="entity-link">AI system</a> might not use the same risk assessment methods as the organization that implements or operates it. Additionally, the developers may not clearly share the risk assessment methods they used. Managing risks can become more complex when customers incorporate <a href="#entity-third-party-data" class="entity-link">third-party data</a> or systems into AI products, especially if there are no strong internal governance or technical safeguards in place. Nonetheless, all parties involved in developing, deploying, or using <a href="#entity-ai-systems" class="entity-link">AI systems</a>—whether as standalone products or integrated components—should actively <a href="#entity-manage" class="entity-link">manage</a> risks. 

To improve <a href="#entity-risk-management" class="entity-link">risk management</a>, <a href="#entity-organizations" class="entity-link">organizations</a> should identify and monitor emerging risks and explore ways to <a href="#entity-measure" class="entity-link">measure</a> them. <a href="#entity-ai-system" class="entity-link">AI system</a> impact assessments can help <a href="#entity-stakeholders" class="entity-link">stakeholders</a> understand potential harms in specific situations. 

A major challenge in measuring <a href="#entity-ai-risks" class="entity-link">AI risks</a> is the lack of agreement on effective and reliable methods for assessing risk and trustworthiness across different AI applications. When trying to <a href="#entity-measure" class="entity-link">measure</a> negative risks or harms, there are potential issues, such as the fact that creating metrics is often a collective effort and may unintentionally include irrelevant factors. Furthermore, measurement methods can be overly simplistic, manipulated, lack important details, be misused, or fail to consider the differences among affected groups and contexts. 

Effective approaches to measuring impacts on populations should acknowledge that context is important, that harms can affect different groups in various ways, and that communities or sub-groups at risk may not always be direct users of the <a href="#entity-ai-system" class="entity-link">AI system</a>.
    </div>
    <div class="chunk-original" id="chunk-11-original">
       the organization
developing the AI system may not align with the risk metrics or methodologies uses by
the organization deploying or operating the system. Also, the organization developing
the AI system may not be transparent about the risk metrics or methodologies it used. Risk
measurement and management can be complicated by how customers use or integrate third-
party data or systems into AI products or services, particularly without sufficient internal
governance structures and technical safeguards. Regardless, all parties and AI actors should
manage risk in the AI systems they develop, deploy, or use as standalone or integrated
components.
Tracking emergent risks: Organizations’ risk management efforts will be enhanced by
identifying and tracking emergent risks and considering techniques for measuring them.
Page 5


NIST AI 100-1
AI RMF 1.0
AI system impact assessment approaches can help AI actors understand potential impacts
or harms within specific contexts.
Availability of reliable metrics: The current lack of consensus on robust and verifiable
measurement methods for risk and trustworthiness, and applicability to different AI use
cases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure
negative risk or harms include the reality that development of metrics is often an institu-
tional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In
addition, measurement approaches can be oversimplified, gamed, lack critical nuance, be-
come relied upon in unexpected ways, or fail to account for differences in affected groups
and contexts.
Approaches for measuring impacts on a population work best if they recognize that contexts
matter, that harms may affect varied groups or sub-groups differently, and that communities
or other sub-groups who may be harmed are not always direct users of a system.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-12')">
      Show original text
    </div>
    <div class="card" id="chunk-12-rephrase">
      To effectively <a href="#entity-measure" class="entity-link">measure</a> the impact of AI on a population, it&#x27;s important to consider the context, as different groups may be affected in various ways. Not all communities that might be harmed are direct users of the <a href="#entity-ai-system" class="entity-link">AI system</a>. Risks can vary at different stages of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>; assessing risks early may yield different insights than evaluating them later, as some risks may not be apparent initially but can grow as <a href="#entity-ai-systems" class="entity-link">AI systems</a> change. Different <a href="#entity-stakeholders" class="entity-link">stakeholders</a> in the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, such as developers and deployers, may have different views on risks. For instance, a developer providing pre-trained AI models may not see the same risks as someone using those models in a specific application. All parties involved share the responsibility to create <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a> that meet their intended purposes. Additionally, measuring <a href="#entity-ai-risks" class="entity-link">AI risks</a> in controlled environments can provide useful information, but these findings may not fully reflect the risks that arise in real-world situations. The complexity of <a href="#entity-ai-systems" class="entity-link">AI systems</a> can make risk measurement challenging, especially when the systems are difficult to understand or lack <a href="#entity-transparency" class="entity-link">transparency</a>. Finally, managing risks for <a href="#entity-ai-systems" class="entity-link">AI systems</a> that are meant to assist or replace human tasks requires a baseline for comparison, which is complicated because AI performs tasks differently than humans.
    </div>
    <div class="chunk-original" id="chunk-12-original">
      Approaches for measuring impacts on a population work best if they recognize that contexts
matter, that harms may affect varied groups or sub-groups differently, and that communities
or other sub-groups who may be harmed are not always direct users of a system.
Risk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI
lifecycle may yield different results than measuring risk at a later stage; some risks may
be latent at a given point in time and may increase as AI systems adapt and evolve. Fur-
thermore, different AI actors across the AI lifecycle can have different risk perspectives.
For example, an AI developer who makes AI software available, such as pre-trained mod-
els, can have a different risk perspective than an AI actor who is responsible for deploying
that pre-trained model in a specific use case. Such deployers may not recognize that their
particular uses could entail risks which differ from those perceived by the initial developer.
All involved AI actors share responsibilities for designing, developing, and deploying a
trustworthy AI system that is fit for purpose.
Risk in real-world settings: While measuring AI risks in a laboratory or a controlled
environment may yield important insights pre-deployment, these measurements may differ
from risks that emerge in operational, real-world settings.
Inscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability
can be a result of the opaque nature of AI systems (limited explainability or interpretabil-
ity), lack of transparency or documentation in AI system development or deployment, or
inherent uncertainties in AI systems.
Human baseline: Risk management of AI systems that are intended to augment or replace
human activity, for example decision making, requires some form of baseline metrics for
comparison. This is difficult to systematize since AI systems carry out different tasks – and
perform tasks differently – than humans.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-13')">
      Show original text
    </div>
    <div class="card" id="chunk-13-rephrase">
      <a href="#entity-ai-systems" class="entity-link">AI systems</a> that aim to enhance or replace human tasks, like decision-making, need baseline metrics for comparison. However, this is challenging because AI performs tasks differently than humans. 

The <a href="#entity-nist" class="entity-link">NIST</a> <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> (Artificial Intelligence <a href="#entity-risk-management-framework" class="entity-link">Risk Management Framework</a>) can help prioritize risks but does not define what <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a> is. <a href="#entity-risk-tolerance" class="entity-link">Risk tolerance</a> is the willingness of an organization or AI user to accept risk to achieve their goals. This tolerance can be shaped by legal and regulatory requirements. It varies based on the specific context and use case, and can be influenced by policies set by <a href="#entity-ai-system" class="entity-link">AI system</a> owners, <a href="#entity-organizations" class="entity-link">organizations</a>, industries, communities, or policymakers. <a href="#entity-risk-tolerance" class="entity-link">Risk tolerance</a> may change over time as <a href="#entity-ai-systems" class="entity-link">AI systems</a> and related policies evolve. Different <a href="#entity-organizations" class="entity-link">organizations</a> may have different risk tolerances based on their unique priorities and resources. 

New knowledge and methods for evaluating harm versus benefits will continue to be developed and discussed among businesses, governments, academia, and <a href="#entity-civil-society" class="entity-link">civil society</a>. If challenges in defining AI risk tolerances remain, there may be situations where a <a href="#entity-risk-management-framework" class="entity-link">risk management framework</a> is not easily applicable to reduce <a href="#entity-negative-ai-risks" class="entity-link">negative AI risks</a>. 

The Framework is designed to be adaptable and should complement existing <a href="#entity-risk-management" class="entity-link">risk management</a> practices that comply with relevant laws, regulations, and norms. <a href="#entity-organizations" class="entity-link">Organizations</a> should adhere to current regulations and guidelines regarding <a href="#entity-risk-criteria" class="entity-link">risk criteria</a>, tolerance, and responses as established by their specific sector or professional standards.
    </div>
    <div class="chunk-original" id="chunk-13-original">
       that are intended to augment or replace
human activity, for example decision making, requires some form of baseline metrics for
comparison. This is difficult to systematize since AI systems carry out different tasks – and
perform tasks differently – than humans.
Page 6


NIST AI 100-1
AI RMF 1.0
1.2.2
Risk Tolerance
While the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk
tolerance refers to the organization’s or AI actor’s (see Appendix A) readiness to bear the
risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula-
tory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that
is acceptable to organizations or society are highly contextual and application and use-case
specific. Risk tolerances can be influenced by policies and norms established by AI sys-
tem owners, organizations, industries, communities, or policy makers. Risk tolerances are
likely to change over time as AI systems, policies, and norms evolve. Different organiza-
tions may have varied risk tolerances due to their particular organizational priorities and
resource considerations.
Emerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con-
tinue to be developed and debated by businesses, governments, academia, and civil society.
To the extent that challenges for specifying AI risk tolerances remain unresolved, there may
be contexts where a risk management framework is not yet readily applicable for mitigating
negative AI risks.
The Framework is intended to be flexible and to augment existing risk practices
which should align with applicable laws, regulations, and norms. Organizations
should follow existing regulations and guidelines for risk criteria, tolerance, and
response established by organizational, domain, discipline, sector, or professional
requirements.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-14')">
      Show original text
    </div>
    <div class="card" id="chunk-14-rephrase">
      <a href="#entity-organizations" class="entity-link">Organizations</a> should enhance their <a href="#entity-risk-management" class="entity-link">risk management</a> practices to comply with relevant laws and regulations. They need to follow established guidelines for <a href="#entity-risk-criteria" class="entity-link">risk criteria</a>, tolerance, and response based on their specific sector or professional standards. Some industries may have clear definitions of harm and specific requirements for documentation and reporting. In cases where no guidelines exist, <a href="#entity-organizations" class="entity-link">organizations</a> should set reasonable <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a> levels. Once these levels are established, the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) can be used to <a href="#entity-manage" class="entity-link">manage</a> and document <a href="#entity-risk-management" class="entity-link">risk management</a> processes.

When it comes to prioritizing risks, trying to eliminate all negative risks is often impractical, as not every incident can be avoided. Having unrealistic expectations about risk can lead to inefficient use of resources. A strong <a href="#entity-risk-management-culture" class="entity-link">risk management culture</a> helps <a href="#entity-organizations" class="entity-link">organizations</a> understand that not all <a href="#entity-ai-risks" class="entity-link">AI risks</a> are equal, allowing for better resource allocation. Effective <a href="#entity-risk-management" class="entity-link">risk management</a> should provide clear guidelines for evaluating the trustworthiness of each <a href="#entity-ai-system" class="entity-link">AI system</a>. Policies and resources should be prioritized based on the assessed risk level and the potential impact of the <a href="#entity-ai-system" class="entity-link">AI system</a>. The ability to customize an <a href="#entity-ai-system" class="entity-link">AI system</a> for its specific use context can also influence <a href="#entity-risk-management" class="entity-link">risk management</a> priorities.

When using the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>, <a href="#entity-organizations" class="entity-link">organizations</a> should focus on the highest risks associated with their <a href="#entity-ai-systems" class="entity-link">AI systems</a> in a specific context, ensuring these risks receive urgent attention and thorough management.
    </div>
    <div class="chunk-original" id="chunk-14-original">
       to augment existing risk practices
which should align with applicable laws, regulations, and norms. Organizations
should follow existing regulations and guidelines for risk criteria, tolerance, and
response established by organizational, domain, discipline, sector, or professional
requirements. Some sectors or industries may have established definitions of harm or
established documentation, reporting, and disclosure requirements. Within sectors,
risk management may depend on existing guidelines for specific applications and
use case settings. Where established guidelines do not exist, organizations should
define reasonable risk tolerance. Once tolerance is defined, this AI RMF can be used
to manage risks and to document risk management processes.
1.2.3
Risk Prioritization
Attempting to eliminate negative risk entirely can be counterproductive in practice because
not all incidents and failures can be eliminated. Unrealistic expectations about risk may
lead organizations to allocate resources in a manner that makes risk triage inefficient or
impractical or wastes scarce resources. A risk management culture can help organizations
recognize that not all AI risks are the same, and resources can be allocated purposefully.
Actionable risk management efforts lay out clear guidelines for assessing trustworthiness
of each AI system an organization develops or deploys. Policies and resources should be
prioritized based on the assessed risk level and potential impact of an AI system. The extent
to which an AI system may be customized or tailored to the specific context of use by the
AI deployer can be a contributing factor.
Page 7


NIST AI 100-1
AI RMF 1.0
When applying the AI RMF, risks which the organization determines to be highest for the
AI systems within a given context of use call for the most urgent prioritization and most
thorough risk management process.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-15')">
      Show original text
    </div>
    <div class="card" id="chunk-15-rephrase">
      <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>
When using the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>, <a href="#entity-organizations" class="entity-link">organizations</a> should focus on the highest risks associated with their <a href="#entity-ai-systems" class="entity-link">AI systems</a> based on how they will be used. If an <a href="#entity-ai-system" class="entity-link">AI system</a> poses serious risks—like imminent severe harm or catastrophic dangers—its development and use should be paused safely until those risks are managed. Conversely, if an <a href="#entity-ai-system" class="entity-link">AI system</a> is deemed low-risk in a specific context, it may require less immediate attention.
Risk prioritization can vary between <a href="#entity-ai-systems" class="entity-link">AI systems</a> that interact directly with people and those that do not. Systems trained on sensitive data, such as personal information, or those that affect humans directly or indirectly should be prioritized more highly. In contrast, <a href="#entity-ai-systems" class="entity-link">AI systems</a> that only interact with other computer systems and use non-sensitive data may require less urgent attention. However, it is still crucial to regularly assess and prioritize risks based on context, as even non-human-facing AI can have safety or social consequences.
<a href="#entity-residual-risk" class="entity-link">Residual risk</a>, which is the risk that remains after treatment (according to <a href="#entity-iso-guide-73" class="entity-link">ISO GUIDE 73</a>), affects <a href="#entity-end-users" class="entity-link">end users</a> and communities. Documenting these residual risks is essential for the system provider to understand the implications of deploying the AI product and to inform users about potential negative effects of using the system.
Organizational Integration and Management of Risk
<a href="#entity-ai-risks" class="entity-link">AI risks</a> should not be viewed in isolation. Different <a href="#entity-stakeholders" class="entity-link">stakeholders</a> in the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> have varying responsibilities and levels of awareness regarding these risks.
    </div>
    <div class="chunk-original" id="chunk-15-original">
      AI RMF 1.0
When applying the AI RMF, risks which the organization determines to be highest for the
AI systems within a given context of use call for the most urgent prioritization and most
thorough risk management process. In cases where an AI system presents unacceptable
negative risk levels – such as where significant negative impacts are imminent, severe harms
are actually occurring, or catastrophic risks are present – development and deployment
should cease in a safe manner until risks can be sufficiently managed. If an AI system’s
development, deployment, and use cases are found to be low-risk in a specific context, that
may suggest potentially lower prioritization.
Risk prioritization may differ between AI systems that are designed or deployed to directly
interact with humans as compared to AI systems that are not. Higher initial prioritization
may be called for in settings where the AI system is trained on large datasets comprised of
sensitive or protected data such as personally identifiable information, or where the outputs
of the AI systems have direct or indirect impact on humans. AI systems designed to interact
only with computational systems and trained on non-sensitive datasets (for example, data
collected from the physical environment) may call for lower initial prioritization. Nonethe-
less, regularly assessing and prioritizing risk based on context remains important because
non-human-facing AI systems can have downstream safety or social implications.
Residual risk – defined as risk remaining after risk treatment (Source: ISO GUIDE 73) –
directly impacts end users or affected individuals and communities. Documenting residual
risks will call for the system provider to fully consider the risks of deploying the AI product
and will inform end users about potential negative impacts of interacting with the system.
1.2.4
Organizational Integration and Management of Risk
AI risks should not be considered in isolation. Different AI actors have different responsi-
bilities and awareness depending on their roles in the lifecycle.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-16')">
      Show original text
    </div>
    <div class="card" id="chunk-16-rephrase">
      1.2.4 Organizational Integration and Management of Risk

<a href="#entity-ai-risks" class="entity-link">AI risks</a> should be viewed in the context of the entire organization. Different people involved in <a href="#entity-ai-development" class="entity-link">AI development</a> have varying responsibilities and levels of awareness based on their roles. For instance, <a href="#entity-organizations" class="entity-link">organizations</a> that create <a href="#entity-ai-systems" class="entity-link">AI systems</a> may not know how those systems will be used. Therefore, managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> should be part of the overall <a href="#entity-risk-management" class="entity-link">risk management</a> strategies of the organization. By addressing <a href="#entity-ai-risks" class="entity-link">AI risks</a> alongside other important risks, like <a href="#entity-cybersecurity" class="entity-link">cybersecurity</a> and <a href="#entity-privacy" class="entity-link">privacy</a>, <a href="#entity-organizations" class="entity-link">organizations</a> can achieve better integration and efficiency.

The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) can be used alongside other guidelines for managing AI and enterprise risks. Some risks associated with <a href="#entity-ai-systems" class="entity-link">AI systems</a> are similar to those found in other software development processes. Common risks include: <a href="#entity-privacy" class="entity-link">privacy</a> issues from using data to train AI; environmental impacts from high energy consumption; <a href="#entity-security" class="entity-link">security</a> risks related to the confidentiality and integrity of the system and its data; and the overall <a href="#entity-security" class="entity-link">security</a> of the software and hardware used in <a href="#entity-ai-systems" class="entity-link">AI systems</a>.

<a href="#entity-organizations" class="entity-link">Organizations</a> must create and uphold clear <a href="#entity-accountability" class="entity-link">accountability</a>, roles, responsibilities, culture, and incentives for effective <a href="#entity-risk-management" class="entity-link">risk management</a>. Simply using the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> will not bring about these changes or provide the necessary incentives. Successful <a href="#entity-risk-management" class="entity-link">risk management</a> requires commitment from <a href="#entity-senior-leadership" class="entity-link">senior leadership</a> and may involve cultural shifts within the organization or industry. Additionally, <a href="#entity-small-to-medium-sized-organizations" class="entity-link">small to medium-sized organizations</a> may face different challenges in managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> compared to larger <a href="#entity-organizations" class="entity-link">organizations</a>, based on their resources and capabilities.
    </div>
    <div class="chunk-original" id="chunk-16-original">
       impacts of interacting with the system.
1.2.4
Organizational Integration and Management of Risk
AI risks should not be considered in isolation. Different AI actors have different responsi-
bilities and awareness depending on their roles in the lifecycle. For example, organizations
developing an AI system often will not have information about how the system may be
used. AI risk management should be integrated and incorporated into broader enterprise
risk management strategies and processes. Treating AI risks along with other critical risks,
such as cybersecurity and privacy, will yield a more integrated outcome and organizational
efficiencies.
The AI RMF may be utilized along with related guidance and frameworks for managing
AI system risks or broader enterprise risks. Some risks related to AI systems are common
across other types of software development and deployment. Examples of overlapping risks
include: privacy concerns related to the use of underlying data to train AI systems; the en-
ergy and environmental implications associated with resource-heavy computing demands;
security concerns related to the confidentiality, integrity, and availability of the system and
its training and output data; and general security of the underlying software and hardware
for AI systems.
Page 8


NIST AI 100-1
AI RMF 1.0
Organizations need to establish and maintain the appropriate accountability mechanisms,
roles and responsibilities, culture, and incentive structures for risk management to be ef-
fective. Use of the AI RMF alone will not lead to these changes or provide the appropriate
incentives. Effective risk management is realized through organizational commitment at
senior levels and may require cultural change within an organization or industry. In addi-
tion, small to medium-sized organizations managing AI risks or implementing the AI RMF
may face different challenges than large organizations, depending on their capabilities and
resources.
2.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-17')">
      Show original text
    </div>
    <div class="card" id="chunk-17-rephrase">
      Managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> may require changes in culture within <a href="#entity-organizations" class="entity-link">organizations</a> or industries. <a href="#entity-small-to-medium-sized-organizations" class="entity-link">Small to medium-sized organizations</a> may face different challenges than large ones when dealing with <a href="#entity-ai-risks" class="entity-link">AI risks</a> or implementing the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>), based on their resources and capabilities.

The process of identifying and managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>, both positive and negative, involves a variety of perspectives and participants throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. It is important for these participants to come from diverse backgrounds and have different experiences and expertise. The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> is designed for use by all actors involved in the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>.

The <a href="#entity-oecd" class="entity-link">OECD</a> has created a framework that classifies <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> activities into five key socio-technical dimensions relevant for AI policy and governance, including <a href="#entity-risk-management" class="entity-link">risk management</a>. <a href="#entity-nist" class="entity-link">NIST</a> has slightly modified this framework to emphasize the importance of testing, evaluation, verification, and validation (<a href="#entity-tevv" class="entity-link">TEVV</a>) processes throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> and to generalize the operational context of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. 

The dimensions highlighted include Application Context, Data and Input, AI Model, and Task and Output. The main audience for the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> consists of <a href="#entity-ai-actors" class="entity-link">AI actors</a> who are involved in designing, developing, deploying, evaluating, and using <a href="#entity-ai-systems" class="entity-link">AI systems</a>, and who are responsible for managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 

Figure 3 lists representative <a href="#entity-ai-actors" class="entity-link">AI actors</a> across these lifecycle dimensions, with detailed descriptions in Appendix A. All <a href="#entity-ai-actors" class="entity-link">AI actors</a> within the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> collaborate to <a href="#entity-manage" class="entity-link">manage</a> risks and promote trustworthy and <a href="#entity-responsible-ai" class="entity-link">responsible AI</a>. Those with <a href="#entity-tevv" class="entity-link">TEVV</a> expertise are integrated throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> and will particularly benefit from this framework.
    </div>
    <div class="chunk-original" id="chunk-17-original">
       and may require cultural change within an organization or industry. In addi-
tion, small to medium-sized organizations managing AI risks or implementing the AI RMF
may face different challenges than large organizations, depending on their capabilities and
resources.
2.
Audience
Identifying and managing AI risks and potential impacts – both positive and negative – re-
quires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will
represent a diversity of experience, expertise, and backgrounds and comprise demograph-
ically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors
across the AI lifecycle and dimensions.
The OECD has developed a framework for classifying AI lifecycle activities according to
five key socio-technical dimensions, each with properties relevant for AI policy and gover-
nance, including risk management [OECD (2022) OECD Framework for the Classification
of AI systems — OECD Digital Economy Papers]. Figure 2 shows these dimensions,
slightly modified by NIST for purposes of this framework. The NIST modification high-
lights the importance of test, evaluation, verification, and validation (TEVV) processes
throughout an AI lifecycle and generalizes the operational context of an AI system.
AI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI
Model, and Task and Output. AI actors involved in these dimensions who perform or
manage the design, development, deployment, evaluation, and use of AI systems and drive
AI risk management efforts are the primary AI RMF audience.
Representative AI actors across the lifecycle dimensions are listed in Figure 3 and described
in detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks
and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific
expertise are integrated throughout the AI lifecycle and are especially likely to benefit from
the Framework.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-18')">
      Show original text
    </div>
    <div class="card" id="chunk-18-rephrase">
      The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) encourages collaboration among all AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> to <a href="#entity-manage" class="entity-link">manage</a> risks and promote trustworthy and <a href="#entity-responsible-ai" class="entity-link">responsible AI</a>. Experts in Technology, Ethics, Values, and Verification (<a href="#entity-tevv" class="entity-link">TEVV</a>) are involved throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> and can greatly benefit from this Framework. Regular <a href="#entity-tevv" class="entity-link">TEVV</a> tasks help provide insights into technical, societal, legal, and ethical standards, allowing for better anticipation of impacts and assessment of emerging risks. By incorporating <a href="#entity-tevv" class="entity-link">TEVV</a> as a regular part of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, <a href="#entity-organizations" class="entity-link">organizations</a> can address issues during development and <a href="#entity-manage" class="entity-link">manage</a> risks after deployment.

The &#x27;People &amp; Planet&#x27; aspect, shown in Figure 2, focuses on human rights and the overall well-being of society and the environment. The <a href="#entity-ai-actors" class="entity-link">AI actors</a> in this area form a distinct audience within the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>, providing input to the main audience. These actors may include trade associations, <a href="#entity-organizations" class="entity-link">organizations</a> that develop standards, researchers, advocacy groups, environmental <a href="#entity-organizations" class="entity-link">organizations</a>, <a href="#entity-civil-society" class="entity-link">civil society</a> groups, <a href="#entity-end-users" class="entity-link">end users</a>, and individuals or communities that may be affected by <a href="#entity-ai-systems" class="entity-link">AI systems</a>.

Figure 2 illustrates the lifecycle and key dimensions of an <a href="#entity-ai-system" class="entity-link">AI system</a>, based on the <a href="#entity-oecd-framework-for-the-classification-of-ai-systems" class="entity-link">OECD Framework for the Classification of AI systems</a>. The inner circles represent the main dimensions of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, while the outer circle outlines the stages of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. Ideally, <a href="#entity-risk-management" class="entity-link">risk management</a> should begin with the planning and design phase and continue throughout the entire lifecycle of the <a href="#entity-ai-system" class="entity-link">AI system</a>. Figure 3 provides examples of relevant <a href="#entity-ai-actors" class="entity-link">AI actors</a>.
    </div>
    <div class="chunk-original" id="chunk-18-original">
       the AI RMF, all AI actors work together to manage risks
and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific
expertise are integrated throughout the AI lifecycle and are especially likely to benefit from
the Framework. Performed regularly, TEVV tasks can provide insights relative to technical,
societal, legal, and ethical standards or norms, and can assist with anticipating impacts and
assessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV
allows for both mid-course remediation and post-hoc risk management.
The People &amp; Planet dimension at the center of Figure 2 represents human rights and the
broader well-being of society and the planet. The AI actors in this dimension comprise
a separate AI RMF audience who informs the primary audience. These AI actors may in-
clude trade associations, standards developing organizations, researchers, advocacy groups,
Page 9


NIST AI 100-1
AI RMF 1.0
Fig. 2. Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD
Framework for the Classification of AI systems — OECD Digital Economy Papers. The two inner
circles show AI systems’ key dimensions and the outer circle shows AI lifecycle stages. Ideally,
risk management efforts start with the Plan and Design function in the application context and are
performed throughout the AI system lifecycle. See Figure 3 for representative AI actors.
environmental groups, civil society organizations, end users, and potentially impacted in-
dividuals and communities.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-19')">
      Show original text
    </div>
    <div class="card" id="chunk-19-rephrase">
      The Plan and Design functions are important in the context of AI applications and are carried out throughout the entire lifecycle of an <a href="#entity-ai-system" class="entity-link">AI system</a>. Figure 3 illustrates the key <a href="#entity-ai-actors" class="entity-link">AI actors</a> involved, which include <a href="#entity-environmental-groups" class="entity-link">environmental groups</a>, <a href="#entity-civil-society-organizations" class="entity-link">civil society organizations</a>, <a href="#entity-end-users" class="entity-link">end users</a>, and individuals or communities that may be affected. These actors can: 
- Provide context and insights on the potential and actual impacts of AI; 
- Offer formal or informal guidelines for managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>; 
- Set boundaries for AI operations in terms of technical, societal, legal, and ethical aspects; 
- Encourage discussions about the trade-offs necessary to balance societal values, such as civil liberties, equity, environmental concerns, and economic priorities. 

Effective <a href="#entity-risk-management" class="entity-link">risk management</a> relies on a shared sense of responsibility among the <a href="#entity-ai-actors" class="entity-link">AI actors</a> shown in Figure 3. The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) functions, detailed in Section 5, benefit from a variety of perspectives, disciplines, and experiences. Diverse teams foster open discussions about the goals and functions of technology, making previously unspoken assumptions clear. This collective viewpoint helps identify problems and recognize both existing and emerging risks. 

Figure 3 also shows <a href="#entity-ai-actors" class="entity-link">AI actors</a> at different stages of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. For more information on the tasks of these actors, including testing, evaluation, verification, and validation, refer to Appendix A. Note that in the AI Model dimension (Figure 2), those who build and use the models are separated from those who verify and validate them. 

For <a href="#entity-ai-systems" class="entity-link">AI systems</a> to be considered trustworthy, they must often meet various criteria that are important to <a href="#entity-stakeholders" class="entity-link">stakeholders</a>.
    </div>
    <div class="chunk-original" id="chunk-19-original">
       Plan and Design function in the application context and are
performed throughout the AI system lifecycle. See Figure 3 for representative AI actors.
environmental groups, civil society organizations, end users, and potentially impacted in-
dividuals and communities. These actors can:
• assist in providing context and understanding potential and actual impacts;
• be a source of formal or quasi-formal norms and guidance for AI risk management;
• designate boundaries for AI operation (technical, societal, legal, and ethical); and
• promote discussion of the tradeoffs needed to balance societal values and priorities
related to civil liberties and rights, equity, the environment and the planet, and the
economy.
Successful risk management depends upon a sense of collective responsibility among AI
actors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse
perspectives, disciplines, professions, and experiences. Diverse teams contribute to more
open sharing of ideas and assumptions about the purposes and functions of technology –
making these implicit aspects more explicit. This broader collective perspective creates
opportunities for surfacing problems and identifying existing and emergent risks.
Page 10


NIST AI 100-1
AI RMF 1.0
Fig. 3. AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing,
evaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with
those building and using the models separated from those verifying and validating the models.
Page 11


NIST AI 100-1
AI RMF 1.0
3.
AI Risks and Trustworthiness
For AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-
teria that are of value to interested parties.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-20')">
      Show original text
    </div>
    <div class="card" id="chunk-20-rephrase">
      <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a> outlines the importance of trustworthiness in <a href="#entity-ai-systems" class="entity-link">AI systems</a>. For AI to be considered trustworthy, it must meet various criteria valued by <a href="#entity-stakeholders" class="entity-link">stakeholders</a>. Enhancing trustworthiness can help minimize negative risks associated with AI. The framework identifies key characteristics of <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a>, which include: being valid and reliable, safe, secure, resilient, accountable, transparent, explainable, interpretable, <a href="#entity-privacy" class="entity-link">privacy</a>-focused, and fair while managing harmful biases. Achieving <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a> requires balancing these characteristics based on how the <a href="#entity-ai-system" class="entity-link">AI system</a> will be used. While all these traits are related to social and technical aspects, <a href="#entity-accountability" class="entity-link">accountability</a> and <a href="#entity-transparency" class="entity-link">transparency</a> are particularly important for both the internal processes of the <a href="#entity-ai-system" class="entity-link">AI system</a> and its external environment. Ignoring these traits can lead to increased risks and negative outcomes. The characteristics of <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a> are interconnected with social behavior, the data used, the choice of <a href="#entity-ai-models-and-algorithms" class="entity-link">AI models and algorithms</a>, and the decisions made by developers, as well as the interactions with users who provide oversight. <a href="#entity-human-judgment" class="entity-link">Human judgment</a> is essential in determining the specific metrics for trustworthiness and their acceptable levels. Addressing each characteristic separately won&#x27;t guarantee overall trustworthiness, as trade-offs are often necessary, and not all characteristics will be relevant in every situation.
    </div>
    <div class="chunk-original" id="chunk-20-original">
      1
AI RMF 1.0
3.
AI Risks and Trustworthiness
For AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-
teria that are of value to interested parties. Approaches which enhance AI trustworthiness
can reduce negative AI risks. This Framework articulates the following characteristics of
trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI
systems include: valid and reliable, safe, secure and resilient, accountable and trans-
parent, explainable and interpretable, privacy-enhanced, and fair with harmful bias
managed. Creating trustworthy AI requires balancing each of these characteristics based
on the AI system’s context of use. While all characteristics are socio-technical system at-
tributes, accountability and transparency also relate to the processes and activities internal
to an AI system and its external setting. Neglecting these characteristics can increase the
probability and magnitude of negative consequences.
Fig. 4. Characteristics of trustworthy AI systems. Valid &amp; Reliable is a necessary condition of
trustworthiness and is shown as the base for other trustworthiness characteristics. Accountable &amp;
Transparent is shown as a vertical box because it relates to all other characteristics.
Trustworthiness characteristics (shown in Figure 4) are inextricably tied to social and orga-
nizational behavior, the datasets used by AI systems, selection of AI models and algorithms
and the decisions made by those who build them, and the interactions with the humans who
provide insight from and oversight of such systems. Human judgment should be employed
when deciding on the specific metrics related to AI trustworthiness characteristics and the
precise threshold values for those metrics.
Addressing AI trustworthiness characteristics individually will not ensure AI system trust-
worthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-
ting, and some will be more or less important in any given situation.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-21')">
      Show original text
    </div>
    <div class="card" id="chunk-21-rephrase">
      Addressing the individual characteristics of <a href="#entity-ai-trustworthiness" class="entity-link">AI trustworthiness</a> alone won&#x27;t guarantee that an <a href="#entity-ai-system" class="entity-link">AI system</a> is trustworthy. There are often trade-offs involved, and not all characteristics will be relevant in every situation. Some characteristics may be more important than others depending on the context. Trustworthiness is a social concept that varies and is only as strong as its weakest aspect.

<a href="#entity-organizations" class="entity-link">Organizations</a> managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> often face tough choices when balancing these characteristics. For instance, they may need to choose between making an <a href="#entity-ai-system" class="entity-link">AI system</a> easy to understand (<a href="#entity-interpretability" class="entity-link">interpretability</a>) and protecting user <a href="#entity-privacy" class="entity-link">privacy</a>. In other cases, they might have to decide between <a href="#entity-accuracy" class="entity-link">accuracy</a> in predictions and <a href="#entity-interpretability" class="entity-link">interpretability</a>. Additionally, using <a href="#entity-privacy" class="entity-link">privacy</a>-enhancing techniques can sometimes reduce <a href="#entity-accuracy" class="entity-link">accuracy</a>, which can impact <a href="#entity-fairness" class="entity-link">fairness</a> and other important values, especially when data is limited.

To navigate these trade-offs, <a href="#entity-organizations" class="entity-link">organizations</a> must consider the specific decision-making context. While analyses can reveal the existence and extent of trade-offs, they do not provide solutions on how to <a href="#entity-manage" class="entity-link">manage</a> them. The resolution depends on the values relevant to the situation and should be handled transparently and justifiably.

There are various ways to improve contextual awareness throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. For example, subject matter experts can help evaluate findings and collaborate with product and deployment teams to ensure that parameters align with requirements and conditions. By involving a diverse range of <a href="#entity-stakeholders" class="entity-link">stakeholders</a> throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, <a href="#entity-organizations" class="entity-link">organizations</a> can better assess context-sensitive evaluations and identify the benefits and positive impacts of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. This approach can enhance the management of risks that arise in social contexts.
    </div>
    <div class="chunk-original" id="chunk-21-original">
       metrics.
Addressing AI trustworthiness characteristics individually will not ensure AI system trust-
worthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-
ting, and some will be more or less important in any given situation. Ultimately, trustwor-
thiness is a social concept that ranges across a spectrum and is only as strong as its weakest
characteristics.
When managing AI risks, organizations can face difficult decisions in balancing these char-
acteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for
interpretability and achieving privacy. In other cases, organizations might face a tradeoff
between predictive accuracy and interpretability. Or, under certain conditions such as data
sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions
Page 12


NIST AI 100-1
AI RMF 1.0
about fairness and other values in certain domains. Dealing with tradeoffs requires tak-
ing into account the decision-making context. These analyses can highlight the existence
and extent of tradeoffs between different measures, but they do not answer questions about
how to navigate the tradeoff. Those depend on the values at play in the relevant context and
should be resolved in a manner that is both transparent and appropriately justifiable.
There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For
example, subject matter experts can assist in the evaluation of TEVV findings and work
with product and deployment teams to align TEVV parameters to requirements and de-
ployment conditions. When properly resourced, increasing the breadth and diversity of
input from interested parties and relevant AI actors throughout the AI lifecycle can en-
hance opportunities for informing contextually sensitive evaluations, and for identifying
AI system benefits and positive impacts. These practices can increase the likelihood that
risks arising in social contexts are managed appropriately.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-22')">
      Show original text
    </div>
    <div class="card" id="chunk-22-rephrase">
      Involving relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> can improve evaluations that consider the specific context and help identify the benefits and positive impacts of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. This approach can also better <a href="#entity-manage" class="entity-link">manage</a> risks that arise in social situations. The understanding of trustworthiness in AI depends on the role of each stakeholder in the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. For example, an <a href="#entity-ai-designer" class="entity-link">AI designer</a> or developer may view trustworthiness differently than the person deploying the <a href="#entity-ai-system" class="entity-link">AI system</a>. The characteristics of trustworthiness discussed in this document are interconnected. Systems that are secure but unfair, accurate but difficult to understand, or inaccurate yet secure and transparent are all problematic. A thorough <a href="#entity-risk-management" class="entity-link">risk management</a> strategy must balance these <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>. It is the shared responsibility of all AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> to assess whether <a href="#entity-ai-technology" class="entity-link">AI technology</a> is suitable for a specific context and how to use it responsibly. Decisions to create or implement an <a href="#entity-ai-system" class="entity-link">AI system</a> should be based on a careful evaluation of <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>, along with the associated risks, impacts, costs, and benefits, and should involve a wide range of interested parties. Validation is the process of confirming, with objective evidence, that an <a href="#entity-ai-system" class="entity-link">AI system</a> meets the requirements for its intended use (Source: <a href="#entity-iso-9000:2015" class="entity-link">ISO 9000:2015</a>). Deploying <a href="#entity-ai-systems" class="entity-link">AI systems</a> that are inaccurate, unreliable, or not well-suited to new data and settings increases negative risks and undermines trustworthiness. Reliability, as defined by the same standard, is the ability of a system to perform as expected without failure for a specified time and under certain conditions (Source: <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>).
    </div>
    <div class="chunk-original" id="chunk-22-original">
       and relevant AI actors throughout the AI lifecycle can en-
hance opportunities for informing contextually sensitive evaluations, and for identifying
AI system benefits and positive impacts. These practices can increase the likelihood that
risks arising in social contexts are managed appropriately.
Understanding and treatment of trustworthiness characteristics depends on an AI actor’s
particular role within the AI lifecycle. For any given AI system, an AI designer or developer
may have a different perception of the characteristics than the deployer.
Trustworthiness characteristics explained in this document influence each other.
Highly secure but unfair systems, accurate but opaque and uninterpretable systems,
and inaccurate but secure, privacy-enhanced, and transparent systems are all unde-
sirable. A comprehensive approach to risk management calls for balancing tradeoffs
among the trustworthiness characteristics. It is the joint responsibility of all AI ac-
tors to determine whether AI technology is an appropriate or necessary tool for a
given context or purpose, and how to use it responsibly. The decision to commission
or deploy an AI system should be based on a contextual assessment of trustworthi-
ness characteristics and the relative risks, impacts, costs, and benefits, and informed
by a broad set of interested parties.
3.1
Valid and Reliable
Validation is the “confirmation, through the provision of objective evidence, that the re-
quirements for a specific intended use or application have been fulfilled” (Source: ISO
9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener-
alized to data and settings beyond their training creates and increases negative AI risks and
reduces trustworthiness.
Reliability is defined in the same standard as the “ability of an item to perform as required,
without failure, for a given time interval, under given conditions” (Source: ISO/IEC TS
5723:2022).
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-23')">
      Show original text
    </div>
    <div class="card" id="chunk-23-rephrase">
      Reliability refers to an item&#x27;s ability to function as intended without failure for a specific time and under certain conditions, according to <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>. For <a href="#entity-ai-systems" class="entity-link">AI systems</a>, reliability means they should operate correctly during expected use over their entire lifespan.

<a href="#entity-accuracy" class="entity-link">Accuracy</a> and <a href="#entity-robustness" class="entity-link">robustness</a> are important for the validity and trustworthiness of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, but they can sometimes conflict with each other. <a href="#entity-accuracy" class="entity-link">Accuracy</a>, as defined by <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>, is how close the results of observations or calculations are to the true values. To <a href="#entity-measure" class="entity-link">measure</a> <a href="#entity-accuracy" class="entity-link">accuracy</a>, it&#x27;s essential to consider factors like false positives and false negatives, human-AI collaboration, and ensure that results can be generalized beyond the training conditions. <a href="#entity-accuracy" class="entity-link">Accuracy</a> assessments should be based on well-defined and realistic test sets that reflect expected use conditions, and the methodology used should be documented. Additionally, <a href="#entity-accuracy" class="entity-link">accuracy</a> results may be broken down by different data segments.

<a href="#entity-robustness" class="entity-link">Robustness</a>, also defined by <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>, is the ability of a system to maintain its performance across various situations. This means that an <a href="#entity-ai-system" class="entity-link">AI system</a> should not only work well under expected conditions but also minimize potential harm if it operates in unexpected scenarios.
    </div>
    <div class="chunk-original" id="chunk-23-original">
      iness.
Reliability is defined in the same standard as the “ability of an item to perform as required,
without failure, for a given time interval, under given conditions” (Source: ISO/IEC TS
5723:2022). Reliability is a goal for overall correctness of AI system operation under the
conditions of expected use and over a given period of time, including the entire lifetime of
the system.
Page 13


NIST AI 100-1
AI RMF 1.0
Accuracy and robustness contribute to the validity and trustworthiness of AI systems, and
can be in tension with one another in AI systems.
Accuracy is defined by ISO/IEC TS 5723:2022 as “closeness of results of observations,
computations, or estimates to the true values or the values accepted as being true.” Mea-
sures of accuracy should consider computational-centric measures (e.g., false positive and
false negative rates), human-AI teaming, and demonstrate external validity (generalizable
beyond the training conditions). Accuracy measurements should always be paired with
clearly defined and realistic test sets – that are representative of conditions of expected use
– and details about test methodology; these should be included in associated documen-
tation. Accuracy measurements may include disaggregation of results for different data
segments.
Robustness or generalizability is defined as the “ability of a system to maintain its level
of performance under a variety of circumstances” (Source: ISO/IEC TS 5723:2022). Ro-
bustness is a goal for appropriate system functionality in a broad set of conditions and
circumstances, including uses of AI systems not initially anticipated. Robustness requires
not only that the system perform exactly as it does under expected uses, but also that it
should perform in ways that minimize potential harms to people if it is operating in an
unexpected setting.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-24')">
      Show original text
    </div>
    <div class="card" id="chunk-24-rephrase">
      <a href="#entity-ai-systems" class="entity-link">AI systems</a> can behave in unexpected ways, so they need to be robust. This means they should not only work well in expected situations but also minimize harm if they operate in unforeseen circumstances. To ensure <a href="#entity-ai-systems" class="entity-link">AI systems</a> are valid and reliable, they should undergo continuous testing and monitoring to confirm they perform as intended. Assessing their validity, <a href="#entity-accuracy" class="entity-link">accuracy</a>, <a href="#entity-robustness" class="entity-link">robustness</a>, and reliability builds trust, especially since some failures can cause significant harm. <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> should focus on reducing negative impacts and may require human intervention if the AI cannot identify or fix its own errors.

<a href="#entity-ai-systems" class="entity-link">AI systems</a> must not endanger human life, health, property, or the environment under any defined conditions (Source: <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>). To ensure safe operation, the following practices are essential:
- Responsible design, development, and deployment.
- Clear guidance for users on how to use the system responsibly.
- Thoughtful decision-making by both deployers and <a href="#entity-end-users" class="entity-link">end users</a>.
- Documentation of risks based on real incidents.

Different <a href="#entity-safety-risks" class="entity-link">safety risks</a> may need specific <a href="#entity-risk-management" class="entity-link">risk management</a> strategies depending on the context and severity. Risks that could lead to serious injury or death should be prioritized and managed thoroughly. Incorporating safety considerations from the planning and design stages can help prevent dangerous failures.
    </div>
    <div class="chunk-original" id="chunk-24-original">
       AI systems not initially anticipated. Robustness requires
not only that the system perform exactly as it does under expected uses, but also that it
should perform in ways that minimize potential harms to people if it is operating in an
unexpected setting.
Validity and reliability for deployed AI systems are often assessed by ongoing testing or
monitoring that confirms a system is performing as intended. Measurement of validity,
accuracy, robustness, and reliability contribute to trustworthiness and should take into con-
sideration that certain types of failures can cause greater harm. AI risk management efforts
should prioritize the minimization of potential negative impacts, and may need to include
human intervention in cases where the AI system cannot detect or correct errors.
3.2
Safe
AI systems should “not under defined conditions, lead to a state in which human life,
health, property, or the environment is endangered” (Source: ISO/IEC TS 5723:2022). Safe
operation of AI systems is improved through:
• responsible design, development, and deployment practices;
• clear information to deployers on responsible use of the system;
• responsible decision-making by deployers and end users; and
• explanations and documentation of risks based on empirical evidence of incidents.
Different types of safety risks may require tailored AI risk management approaches based
on context and the severity of potential risks presented. Safety risks that pose a potential
risk of serious injury or death call for the most urgent prioritization and most thorough risk
management process.
Page 14


NIST AI 100-1
AI RMF 1.0
Employing safety considerations during the lifecycle and starting as early as possible with
planning and design can prevent failures or conditions that can render a system dangerous.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-25')">
      Show original text
    </div>
    <div class="card" id="chunk-25-rephrase">
      <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a> <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a> emphasizes the importance of considering safety throughout the entire lifecycle of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, starting from the planning and design stages. This proactive approach can help prevent dangerous failures. Effective AI safety measures include thorough simulations, testing within the relevant domain, real-time monitoring, and having the capability to shut down or modify systems, as well as allowing for human intervention if the system behaves unexpectedly. AI safety <a href="#entity-risk-management" class="entity-link">risk management</a> should draw from safety practices in fields like transportation and healthcare and align with existing guidelines specific to different sectors or applications.

<a href="#entity-ai-systems" class="entity-link">AI systems</a> and their environments are considered resilient if they can handle unexpected events or changes while maintaining their functions. They should also be able to safely degrade when necessary. Common <a href="#entity-security" class="entity-link">security</a> issues include adversarial attacks, data poisoning, and unauthorized access to models or <a href="#entity-training-data" class="entity-link">training data</a>. Secure <a href="#entity-ai-systems" class="entity-link">AI systems</a> protect confidentiality, integrity, and availability by preventing unauthorized access. The <a href="#entity-nist-cybersecurity-framework" class="entity-link">NIST Cybersecurity Framework</a> and <a href="#entity-risk-management-framework" class="entity-link">Risk Management Framework</a> provide relevant guidelines for ensuring <a href="#entity-security" class="entity-link">security</a>.

While <a href="#entity-security-and-resilience" class="entity-link">security and resilience</a> are related, they are not the same. <a href="#entity-resilience" class="entity-link">Resilience</a> refers to the ability to return to normal operations after an unexpected event, whereas <a href="#entity-security" class="entity-link">security</a> includes <a href="#entity-resilience" class="entity-link">resilience</a> but also involves measures to prevent, protect against, respond to, and recover from attacks.
    </div>
    <div class="chunk-original" id="chunk-25-original">
       14


NIST AI 100-1
AI RMF 1.0
Employing safety considerations during the lifecycle and starting as early as possible with
planning and design can prevent failures or conditions that can render a system dangerous.
Other practical approaches for AI safety often relate to rigorous simulation and in-domain
testing, real-time monitoring, and the ability to shut down, modify, or have human inter-
vention into systems that deviate from intended or expected functionality.
AI safety risk management approaches should take cues from efforts and guidelines for
safety in fields such as transportation and healthcare, and align with existing sector- or
application-specific guidelines or standards.
3.3
Secure and Resilient
AI systems, as well as the ecosystems in which they are deployed, may be said to be re-
silient if they can withstand unexpected adverse events or unexpected changes in their envi-
ronment or use – or if they can maintain their functions and structure in the face of internal
and external change and degrade safely and gracefully when this is necessary (Adapted
from: ISO/IEC TS 5723:2022). Common security concerns relate to adversarial examples,
data poisoning, and the exfiltration of models, training data, or other intellectual property
through AI system endpoints. AI systems that can maintain confidentiality, integrity, and
availability through protection mechanisms that prevent unauthorized access and use may
be said to be secure. Guidelines in the NIST Cybersecurity Framework and Risk Manage-
ment Framework are among those which are applicable here.
Security and resilience are related but distinct characteristics. While resilience is the abil-
ity to return to normal function after an unexpected adverse event, security includes re-
silience but also encompasses protocols to avoid, protect against, respond to, or recover
from attacks.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-26')">
      Show original text
    </div>
    <div class="card" id="chunk-26-rephrase">
      <a href="#entity-resilience" class="entity-link">Resilience</a> and <a href="#entity-security" class="entity-link">security</a> are related but different concepts. <a href="#entity-resilience" class="entity-link">Resilience</a> is the ability to bounce back to normal after an unexpected negative event, while <a href="#entity-security" class="entity-link">security</a> includes <a href="#entity-resilience" class="entity-link">resilience</a> and also involves measures to prevent, protect against, respond to, or recover from attacks. <a href="#entity-resilience" class="entity-link">Resilience</a> focuses on strength and goes beyond just the source of the data; it also considers unexpected or harmful uses of the model or data.

<a href="#entity-accountability" class="entity-link">Accountability</a> and <a href="#entity-transparency" class="entity-link">transparency</a> are essential for <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a>. <a href="#entity-accountability" class="entity-link">Accountability</a> requires <a href="#entity-transparency" class="entity-link">transparency</a>, which means providing information about an <a href="#entity-ai-system" class="entity-link">AI system</a> and its outputs to users, even if they are not aware they are interacting with it. Meaningful <a href="#entity-transparency" class="entity-link">transparency</a> offers the right level of information based on the AI&#x27;s lifecycle stage and the knowledge of the users. This helps users understand the system better and builds their confidence in it.

<a href="#entity-transparency" class="entity-link">Transparency</a> covers everything from design choices and <a href="#entity-training-data" class="entity-link">training data</a> to how the model is trained, its structure, intended uses, and decisions made during deployment and afterward. It is crucial for addressing issues when AI outputs are incorrect or harmful. <a href="#entity-transparency" class="entity-link">Transparency</a> should also consider how users are informed about potential or actual negative outcomes caused by the AI. While a transparent system is not automatically accurate, secure, or fair, it is challenging to assess whether a non-transparent system has these qualities, especially as complex systems change over time.
    </div>
    <div class="chunk-original" id="chunk-26-original">
       are related but distinct characteristics. While resilience is the abil-
ity to return to normal function after an unexpected adverse event, security includes re-
silience but also encompasses protocols to avoid, protect against, respond to, or recover
from attacks. Resilience relates to robustness and goes beyond the provenance of the data
to encompass unexpected or adversarial use (or abuse or misuse) of the model or data.
3.4
Accountable and Transparent
Trustworthy AI depends upon accountability. Accountability presupposes transparency.
Transparency reflects the extent to which information about an AI system and its outputs is
available to individuals interacting with such a system – regardless of whether they are even
aware that they are doing so. Meaningful transparency provides access to appropriate levels
of information based on the stage of the AI lifecycle and tailored to the role or knowledge
of AI actors or individuals interacting with or using the AI system. By promoting higher
levels of understanding, transparency increases confidence in the AI system.
This characteristic’s scope spans from design decisions and training data to model train-
ing, the structure of the model, its intended use cases, and how and when deployment,
post-deployment, or end user decisions were made and by whom. Transparency is often
necessary for actionable redress related to AI system outputs that are incorrect or otherwise
lead to negative impacts. Transparency should consider human-AI interaction: for exam-
Page 15


NIST AI 100-1
AI RMF 1.0
ple, how a human operator or user is notified when a potential or actual adverse outcome
caused by an AI system is detected. A transparent system is not necessarily an accurate,
privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an
opaque system possesses such characteristics, and to do so over time as complex systems
evolve.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-27')">
      Show original text
    </div>
    <div class="card" id="chunk-27-rephrase">
      A transparent system does not automatically mean it is accurate, secure, fair, or protects <a href="#entity-privacy" class="entity-link">privacy</a>. It can be challenging to assess whether a non-transparent system has these qualities, especially as complex systems change over time. When looking for <a href="#entity-accountability" class="entity-link">accountability</a> in <a href="#entity-ai-systems" class="entity-link">AI systems</a>, it&#x27;s important to consider the role of AI developers and users. The relationship between risk and <a href="#entity-accountability" class="entity-link">accountability</a> varies based on cultural, legal, and societal factors. In situations where the stakes are high, such as when lives are at risk, AI developers should actively improve their <a href="#entity-transparency" class="entity-link">transparency</a> and <a href="#entity-accountability" class="entity-link">accountability</a> practices. Implementing <a href="#entity-risk-management" class="entity-link">risk management</a> and harm reduction strategies can lead to more responsible systems. Efforts to increase <a href="#entity-transparency" class="entity-link">transparency</a> and <a href="#entity-accountability" class="entity-link">accountability</a> should also take into account the resources needed and the protection of proprietary information. Keeping track of the <a href="#entity-training-data" class="entity-link">training data</a> used for AI and linking AI decisions back to specific data can help with <a href="#entity-transparency" class="entity-link">transparency</a> and <a href="#entity-accountability" class="entity-link">accountability</a>. <a href="#entity-training-data" class="entity-link">Training data</a> may be protected by copyright and must comply with intellectual property laws. As tools for <a href="#entity-transparency" class="entity-link">transparency</a> in <a href="#entity-ai-systems" class="entity-link">AI systems</a> develop, AI developers should work with users to test these tools to ensure <a href="#entity-ai-systems" class="entity-link">AI systems</a> function as intended. <a href="#entity-explainability" class="entity-link">Explainability</a> means understanding how <a href="#entity-ai-systems" class="entity-link">AI systems</a> work, while <a href="#entity-interpretability" class="entity-link">interpretability</a> refers to understanding the meaning of their outputs based on their intended purpose. Together, these concepts help users and overseers of <a href="#entity-ai-systems" class="entity-link">AI systems</a> gain insights into how the systems operate and how trustworthy they are.
    </div>
    <div class="chunk-original" id="chunk-27-original">
       detected. A transparent system is not necessarily an accurate,
privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an
opaque system possesses such characteristics, and to do so over time as complex systems
evolve.
The role of AI actors should be considered when seeking accountability for the outcomes of
AI systems. The relationship between risk and accountability associated with AI and tech-
nological systems more broadly differs across cultural, legal, sectoral, and societal contexts.
When consequences are severe, such as when life and liberty are at stake, AI developers
and deployers should consider proportionally and proactively adjusting their transparency
and accountability practices. Maintaining organizational practices and governing structures
for harm reduction, like risk management, can help lead to more accountable systems.
Measures to enhance transparency and accountability should also consider the impact of
these efforts on the implementing entity, including the level of necessary resources and the
need to safeguard proprietary information.
Maintaining the provenance of training data and supporting attribution of the AI system’s
decisions to subsets of training data can assist with both transparency and accountability.
Training data may also be subject to copyright and should follow applicable intellectual
property rights laws.
As transparency tools for AI systems and related documentation continue to evolve, devel-
opers of AI systems are encouraged to test different types of transparency tools in cooper-
ation with AI deployers to ensure that AI systems are used as intended.
3.5
Explainable and Interpretable
Explainability refers to a representation of the mechanisms underlying AI systems’ oper-
ation, whereas interpretability refers to the meaning of AI systems’ output in the context
of their designed functional purposes. Together, explainability and interpretability assist
those operating or overseeing an AI system, as well as users of an AI system, to gain
deeper insights into the functionality and trustworthiness of the system, including its out-
puts.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-28')">
      Show original text
    </div>
    <div class="card" id="chunk-28-rephrase">
      <a href="#entity-explainability" class="entity-link">Explainability</a> and <a href="#entity-interpretability" class="entity-link">interpretability</a> are important for both those managing <a href="#entity-ai-systems" class="entity-link">AI systems</a> and the users of these systems. They help everyone understand how the AI works and how trustworthy it is, including its results. The main idea is that people often feel uncertain about AI because they can&#x27;t easily understand its outputs. <a href="#entity-ai-systems" class="entity-link">AI systems</a> that are explainable and interpretable provide information that helps users grasp their purpose and potential effects. To reduce risks from a lack of <a href="#entity-explainability" class="entity-link">explainability</a>, it&#x27;s helpful to explain how <a href="#entity-ai-systems" class="entity-link">AI systems</a> operate, adjusting the explanations based on the user&#x27;s role, knowledge, and skills. Explainable systems are easier to debug, monitor, and document, which supports better governance. Risks related to <a href="#entity-interpretability" class="entity-link">interpretability</a> can be managed by explaining why an <a href="#entity-ai-system" class="entity-link">AI system</a> made a specific prediction or recommendation. <a href="#entity-transparency" class="entity-link">Transparency</a>, <a href="#entity-explainability" class="entity-link">explainability</a>, and <a href="#entity-interpretability" class="entity-link">interpretability</a> are different but interconnected features. <a href="#entity-transparency" class="entity-link">Transparency</a> answers &#x27;what happened,&#x27; <a href="#entity-explainability" class="entity-link">explainability</a> answers &#x27;how&#x27; a decision was made, and <a href="#entity-interpretability" class="entity-link">interpretability</a> answers &#x27;why&#x27; a decision was made and its significance to the user. <a href="#entity-privacy" class="entity-link">Privacy</a> involves protecting individuals&#x27; autonomy, identity, and dignity by limiting intrusion and allowing people to control their personal information.
    </div>
    <div class="chunk-original" id="chunk-28-original">
      . Together, explainability and interpretability assist
those operating or overseeing an AI system, as well as users of an AI system, to gain
deeper insights into the functionality and trustworthiness of the system, including its out-
puts. The underlying assumption is that perceptions of negative risk stem from a lack of
ability to make sense of, or contextualize, system output appropriately. Explainable and
interpretable AI systems offer information that will help end users understand the purposes
and potential impact of an AI system.
Risk from lack of explainability may be managed by describing how AI systems function,
with descriptions tailored to individual differences such as the user’s role, knowledge, and
skill level. Explainable systems can be debugged and monitored more easily, and they lend
themselves to more thorough documentation, audit, and governance.
Page 16


NIST AI 100-1
AI RMF 1.0
Risks to interpretability often can be addressed by communicating a description of why
an AI system made a particular prediction or recommendation. (See “Four Principles of
Explainable Artificial Intelligence” and “Psychological Foundations of Explainability and
Interpretability in Artificial Intelligence” found here.)
Transparency, explainability, and interpretability are distinct characteristics that support
each other. Transparency can answer the question of “what happened” in the system. Ex-
plainability can answer the question of “how” a decision was made in the system. Inter-
pretability can answer the question of “why” a decision was made by the system and its
meaning or context to the user.
3.6
Privacy-Enhanced
Privacy refers generally to the norms and practices that help to safeguard human autonomy,
identity, and dignity. These norms and practices typically address freedom from intrusion,
limiting observation, or individuals’ agency to consent to disclosure or control of facets of
their identities (e.g., body, data, reputation).
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-29')">
      Show original text
    </div>
    <div class="card" id="chunk-29-rephrase">
      To protect human autonomy, identity, and dignity, we need to follow certain norms and practices that ensure individuals have control over their personal information, such as their body, data, and reputation. The <a href="#entity-nist-privacy-framework" class="entity-link">NIST Privacy Framework</a> provides guidance on improving <a href="#entity-privacy" class="entity-link">privacy</a> through <a href="#entity-enterprise-risk-management" class="entity-link">enterprise risk management</a>. When designing, developing, and deploying <a href="#entity-ai-systems" class="entity-link">AI systems</a>, we should prioritize <a href="#entity-privacy" class="entity-link">privacy</a> values like anonymity, confidentiality, and control. <a href="#entity-privacy" class="entity-link">Privacy</a> risks can affect <a href="#entity-security" class="entity-link">security</a>, <a href="#entity-bias" class="entity-link">bias</a>, and <a href="#entity-transparency" class="entity-link">transparency</a>, and there may be trade-offs between these aspects. Specific technical features of <a href="#entity-ai-systems" class="entity-link">AI systems</a> can either enhance or compromise <a href="#entity-privacy" class="entity-link">privacy</a>. Additionally, AI can introduce new <a href="#entity-privacy" class="entity-link">privacy</a> risks by enabling the identification of individuals or revealing private information. <a href="#entity-privacy" class="entity-link">Privacy</a>-enhancing technologies (PETs) and methods like de-identification and aggregation can help create <a href="#entity-ai-systems" class="entity-link">AI systems</a> that prioritize <a href="#entity-privacy" class="entity-link">privacy</a>. However, using these techniques may sometimes reduce <a href="#entity-accuracy" class="entity-link">accuracy</a>, which can impact <a href="#entity-fairness" class="entity-link">fairness</a> and other important values. <a href="#entity-fairness-in-ai" class="entity-link">Fairness in AI</a> involves addressing <a href="#entity-harmful-bias" class="entity-link">harmful bias</a> and discrimination, but defining <a href="#entity-fairness" class="entity-link">fairness</a> can be complex due to cultural differences and varying perceptions. <a href="#entity-organizations" class="entity-link">Organizations</a> should consider these differences in their <a href="#entity-risk-management" class="entity-link">risk management</a> efforts. Even if harmful biases are reduced, a system may still not be fair if it is inaccessible to people with disabilities or if it worsens existing inequalities. <a href="#entity-bias" class="entity-link">Bias</a> in AI is more than just achieving demographic balance or data representativeness.
    </div>
    <div class="chunk-original" id="chunk-29-original">
       to safeguard human autonomy,
identity, and dignity. These norms and practices typically address freedom from intrusion,
limiting observation, or individuals’ agency to consent to disclosure or control of facets of
their identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool
for Improving Privacy through Enterprise Risk Management.)
Privacy values such as anonymity, confidentiality, and control generally should guide choices
for AI system design, development, and deployment. Privacy-related risks may influence
security, bias, and transparency and come with tradeoffs with these other characteristics.
Like safety and security, specific technical features of an AI system may promote or reduce
privacy. AI systems can also present new risks to privacy by allowing inference to identify
individuals or previously private information about individuals.
Privacy-enhancing technologies (“PETs”) for AI, as well as data minimizing methods such
as de-identification and aggregation for certain model outputs, can support design for
privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-
enhancing techniques can result in a loss in accuracy, affecting decisions about fairness
and other values in certain domains.
3.7
Fair – with Harmful Bias Managed
Fairness in AI includes concerns for equality and equity by addressing issues such as harm-
ful bias and discrimination. Standards of fairness can be complex and difficult to define be-
cause perceptions of fairness differ among cultures and may shift depending on application.
Organizations’ risk management efforts will be enhanced by recognizing and considering
these differences. Systems in which harmful biases are mitigated are not necessarily fair.
For example, systems in which predictions are somewhat balanced across demographic
groups may still be inaccessible to individuals with disabilities or affected by the digital
divide or may exacerbate existing disparities or systemic biases.
Page 17


NIST AI 100-1
AI RMF 1.0
Bias is broader than demographic balance and data representativeness.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-30')">
      Show original text
    </div>
    <div class="card" id="chunk-30-rephrase">
      <a href="#entity-bias" class="entity-link">Bias</a> in AI goes beyond just having a fair representation of different demographics. The <a href="#entity-national-institute-of-standards-and-technology" class="entity-link">National Institute of Standards and Technology</a> (<a href="#entity-nist" class="entity-link">NIST</a>) has identified three main types of AI <a href="#entity-bias" class="entity-link">bias</a> that need to be addressed: <a href="#entity-systemic-bias" class="entity-link">systemic bias</a>, <a href="#entity-computational-and-statistical-bias" class="entity-link">computational and statistical bias</a>, and <a href="#entity-human-cognitive-bias" class="entity-link">human-cognitive bias</a>. These biases can occur even without any intention to discriminate. 

<a href="#entity-systemic-bias" class="entity-link">Systemic bias</a> can be found in AI <a href="#entity-datasets" class="entity-link">datasets</a>, the practices and norms of <a href="#entity-organizations" class="entity-link">organizations</a>, and in society as a whole. Computational and statistical biases often arise from errors in AI <a href="#entity-datasets" class="entity-link">datasets</a> and algorithms, particularly when the data is not representative. Human-<a href="#entity-cognitive-biases" class="entity-link">cognitive biases</a> involve how people interpret AI information to make decisions or fill in gaps, and these biases are common throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, including in design, implementation, and maintenance.

<a href="#entity-bias" class="entity-link">Bias</a> can take many forms and may become embedded in automated systems that influence our lives. While <a href="#entity-bias" class="entity-link">bias</a> isn&#x27;t always negative, AI can speed up and amplify existing biases, potentially causing harm to individuals and communities. Addressing <a href="#entity-bias" class="entity-link">bias</a> is closely linked to ensuring <a href="#entity-transparency" class="entity-link">transparency</a> and <a href="#entity-fairness" class="entity-link">fairness</a> in society. For more details on <a href="#entity-bias" class="entity-link">bias</a> and its categories, refer to <a href="#entity-nist-special-publication-1270" class="entity-link">NIST Special Publication 1270</a>, which focuses on identifying and managing <a href="#entity-bias" class="entity-link">bias</a> in AI.
    </div>
    <div class="chunk-original" id="chunk-30-original">
       disabilities or affected by the digital
divide or may exacerbate existing disparities or systemic biases.
Page 17


NIST AI 100-1
AI RMF 1.0
Bias is broader than demographic balance and data representativeness. NIST has identified
three major categories of AI bias to be considered and managed: systemic, computational
and statistical, and human-cognitive. Each of these can occur in the absence of prejudice,
partiality, or discriminatory intent. Systemic bias can be present in AI datasets, the orga-
nizational norms, practices, and processes across the AI lifecycle, and the broader society
that uses AI systems. Computational and statistical biases can be present in AI datasets
and algorithmic processes, and often stem from systematic errors due to non-representative
samples. Human-cognitive biases relate to how an individual or group perceives AI sys-
tem information to make a decision or fill in missing information, or how humans think
about purposes and functions of an AI system. Human-cognitive biases are omnipresent
in decision-making processes across the AI lifecycle and system use, including the design,
implementation, operation, and maintenance of AI.
Bias exists in many forms and can become ingrained in the automated systems that help
make decisions about our lives. While bias is not always a negative phenomenon, AI sys-
tems can potentially increase the speed and scale of biases and perpetuate and amplify
harms to individuals, groups, communities, organizations, and society. Bias is tightly asso-
ciated with the concepts of transparency as well as fairness in society. (For more informa-
tion about bias, including the three categories, see NIST Special Publication 1270, Towards
a Standard for Identifying and Managing Bias in Artificial Intelligence.)
Page 18


NIST AI 100-1
AI RMF 1.0
4.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-31')">
      Show original text
    </div>
    <div class="card" id="chunk-31-rephrase">
      <a href="#entity-nist-special-publication-1270" class="entity-link">NIST Special Publication 1270</a> outlines three categories for identifying and managing <a href="#entity-bias" class="entity-link">bias</a> in Artificial Intelligence (AI). In the <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a> document, the effectiveness of the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) will be evaluated in future <a href="#entity-nist" class="entity-link">NIST</a> activities, focusing on how to <a href="#entity-measure" class="entity-link">measure</a> improvements in the trustworthiness of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. <a href="#entity-organizations" class="entity-link">Organizations</a> using the Framework are encouraged to regularly assess whether it has helped them <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a> better, including their policies, processes, and expected outcomes. <a href="#entity-nist" class="entity-link">NIST</a> plans to collaborate with the <a href="#entity-ai-community" class="entity-link">AI community</a> to create metrics and <a href="#entity-methodologies" class="entity-link">methodologies</a> for evaluating the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>&#x27;s effectiveness and to share the results widely. Users of the Framework can expect benefits such as: 
- Improved processes for governing and managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>, with clear documentation of outcomes. 
- Better understanding of the relationships and trade-offs between trustworthiness, socio-technical approaches, and <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 
- Clear procedures for making decisions on whether to deploy <a href="#entity-ai-systems" class="entity-link">AI systems</a>. 
- Established practices for enhancing <a href="#entity-accountability" class="entity-link">accountability</a> regarding <a href="#entity-ai-system" class="entity-link">AI system</a> risks. 
- A stronger <a href="#entity-organizational-culture" class="entity-link">organizational culture</a> that focuses on identifying and managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> and their impacts on individuals and society. 
- Improved information sharing about risks, decision-making, and continuous improvement practices. 
- Greater awareness of <a href="#entity-downstream-risks" class="entity-link">downstream risks</a>. 
- Enhanced engagement with <a href="#entity-stakeholders" class="entity-link">stakeholders</a> and relevant AI participants. 
- Increased capacity for testing and evaluating <a href="#entity-ai-systems" class="entity-link">AI systems</a> and their associated risks.
    </div>
    <div class="chunk-original" id="chunk-31-original">
       including the three categories, see NIST Special Publication 1270, Towards
a Standard for Identifying and Managing Bias in Artificial Intelligence.)
Page 18


NIST AI 100-1
AI RMF 1.0
4.
Effectiveness of the AI RMF
Evaluations of AI RMF effectiveness – including ways to measure bottom-line improve-
ments in the trustworthiness of AI systems – will be part of future NIST activities, in
conjunction with the AI community.
Organizations and other users of the Framework are encouraged to periodically evaluate
whether the AI RMF has improved their ability to manage AI risks, including but not lim-
ited to their policies, processes, practices, implementation plans, indicators, measurements,
and expected outcomes. NIST intends to work collaboratively with others to develop met-
rics, methodologies, and goals for evaluating the AI RMF’s effectiveness, and to broadly
share results and supporting information. Framework users are expected to benefit from:
• enhanced processes for governing, mapping, measuring, and managing AI risk, and
clearly documenting outcomes;
• improved awareness of the relationships and tradeoffs among trustworthiness char-
acteristics, socio-technical approaches, and AI risks;
• explicit processes for making go/no-go system commissioning and deployment deci-
sions;
• established policies, processes, practices, and procedures for improving organiza-
tional accountability efforts related to AI system risks;
• enhanced organizational culture which prioritizes the identification and management
of AI system risks and potential impacts to individuals, communities, organizations,
and society;
• better information sharing within and across organizations about risks, decision-
making processes, responsibilities, common pitfalls, TEVV practices, and approaches
for continuous improvement;
• greater contextual knowledge for increased awareness of downstream risks;
• strengthened engagement with interested parties and relevant AI actors; and
• augmented capacity for TEVV of AI systems and associated risks.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-32')">
      Show original text
    </div>
    <div class="card" id="chunk-32-rephrase">
      The text discusses the challenges and practices related to <a href="#entity-tevv" class="entity-link">TEVV</a> (Trustworthy, Ethical, Verifiable, and Validated) in <a href="#entity-ai-systems" class="entity-link">AI systems</a>, emphasizing the need for: 
- Better understanding of potential risks that may arise later on. 
- Improved collaboration with <a href="#entity-stakeholders" class="entity-link">stakeholders</a> and relevant AI professionals. 
- Enhanced ability to evaluate the <a href="#entity-tevv" class="entity-link">TEVV</a> of <a href="#entity-ai-systems" class="entity-link">AI systems</a> and their associated risks.

The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> (<a href="#entity-risk-management-framework" class="entity-link">Risk Management Framework</a>) Core outlines key outcomes and actions to facilitate discussions and <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a> effectively, ensuring the development of <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a>. It consists of four main functions: <a href="#entity-govern" class="entity-link">GOVERN</a>, <a href="#entity-map" class="entity-link">MAP</a>, <a href="#entity-measure" class="entity-link">MEASURE</a>, and <a href="#entity-manage" class="entity-link">MANAGE</a>. Each function is divided into categories and subcategories, which further break down into specific actions and outcomes. These actions are not a strict checklist or a sequential process.

<a href="#entity-risk-management" class="entity-link">Risk management</a> should be an ongoing process throughout the entire lifecycle of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. The <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a> functions should incorporate diverse perspectives, including input from AI professionals outside the organization. A <a href="#entity-diverse-team" class="entity-link">diverse team</a> fosters open discussions about the technology&#x27;s purpose and functions, helping to identify potential problems and emerging risks.
    </div>
    <div class="chunk-original" id="chunk-32-original">
       pitfalls, TEVV practices, and approaches
for continuous improvement;
• greater contextual knowledge for increased awareness of downstream risks;
• strengthened engagement with interested parties and relevant AI actors; and
• augmented capacity for TEVV of AI systems and associated risks.
Page 19


NIST AI 100-1
AI RMF 1.0
Part 2: Core and Profiles
5.
AI RMF Core
The AI RMF Core provides outcomes and actions that enable dialogue, understanding, and
activities to manage AI risks and responsibly develop trustworthy AI systems. As illus-
trated in Figure 5, the Core is composed of four functions: GOVERN, MAP, MEASURE,
and MANAGE. Each of these high-level functions is broken down into categories and sub-
categories. Categories and subcategories are subdivided into specific actions and outcomes.
Actions do not constitute a checklist, nor are they necessarily an ordered set of steps.
Fig. 5. Functions organize AI risk management activities at their highest level to govern, map,
measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform
and be infused throughout the other three functions.
Risk management should be continuous, timely, and performed throughout the AI system
lifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects
diverse and multidisciplinary perspectives, potentially including the views of AI actors out-
side the organization. Having a diverse team contributes to more open sharing of ideas and
assumptions about purposes and functions of the technology being designed, developed,
Page 20


NIST AI 100-1
AI RMF 1.0
deployed, or evaluated – which can create opportunities to surface problems and identify
existing and emergent risks.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-33')">
      Show original text
    </div>
    <div class="card" id="chunk-33-rephrase">
      The technology being designed, developed, deployed, or evaluated can reveal problems and risks. To assist <a href="#entity-organizations" class="entity-link">organizations</a> in using the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>), <a href="#entity-nist" class="entity-link">NIST</a> has created an online resource called the <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>. This Playbook offers practical actions that <a href="#entity-organizations" class="entity-link">organizations</a> can take based on their specific needs and interests. It is voluntary, allowing users to customize guidance from the suggested materials and share their own ideas with the community. Both the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> and the Playbook are part of the <a href="#entity-nist-trustworthy-and-responsible-ai-resource-center" class="entity-link">NIST Trustworthy and Responsible AI Resource Center</a>.

<a href="#entity-organizations" class="entity-link">Organizations</a> can use the framework functions in a way that best fits their resources and capabilities. Some may choose specific categories and subcategories, while others may apply all of them. With a governance structure in place, users can perform functions in any order that adds value throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. Typically, after establishing outcomes in the <a href="#entity-govern" class="entity-link">GOVERN</a> phase, users would begin with the <a href="#entity-map-function" class="entity-link">MAP function</a> and then proceed to <a href="#entity-measure" class="entity-link">MEASURE</a> or <a href="#entity-manage" class="entity-link">MANAGE</a>. The process should be iterative, allowing for cross-referencing between functions as needed. Additionally, some categories and subcategories may apply to multiple functions or should logically occur before certain decisions are made.
    </div>
    <div class="chunk-original" id="chunk-33-original">
       of the technology being designed, developed,
Page 20


NIST AI 100-1
AI RMF 1.0
deployed, or evaluated – which can create opportunities to surface problems and identify
existing and emergent risks.
An online companion resource to the AI RMF, the NIST AI RMF Playbook, is available
to help organizations navigate the AI RMF and achieve its outcomes through suggested
tactical actions they can apply within their own contexts. Like the AI RMF, the Playbook
is voluntary and organizations can utilize the suggestions according to their needs and
interests. Playbook users can create tailored guidance selected from suggested material
for their own use and contribute their suggestions for sharing with the broader community.
Along with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI
Resource Center.
Framework users may apply these functions as best suits their needs for managing
AI risks based on their resources and capabilities. Some organizations may choose
to select from among the categories and subcategories; others may choose and have
the capacity to apply all categories and subcategories. Assuming a governance struc-
ture is in place, functions may be performed in any order across the AI lifecycle as
deemed to add value by a user of the framework. After instituting the outcomes in
GOVERN, most users of the AI RMF would start with the MAP function and con-
tinue to MEASURE or MANAGE. However users integrate the functions, the process
should be iterative, with cross-referencing between functions as necessary. Simi-
larly, there are categories and subcategories with elements that apply to multiple
functions, or that logically should take place before certain subcategory decisions.
5.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-34')">
      Show original text
    </div>
    <div class="card" id="chunk-34-rephrase">
      The process should be iterative, with functions referencing each other as needed. There are categories and subcategories that include elements relevant to multiple functions, or that should logically occur before certain decisions in subcategories.

**<a href="#entity-govern" class="entity-link">Govern</a>**
The <a href="#entity-govern-function" class="entity-link">GOVERN function</a>:
- Promotes and implements a culture of <a href="#entity-risk-management" class="entity-link">risk management</a> in <a href="#entity-organizations" class="entity-link">organizations</a> that design, develop, deploy, evaluate, or acquire <a href="#entity-ai-systems" class="entity-link">AI systems</a>.
- Establishes processes, documents, and organizational structures to anticipate, identify, and <a href="#entity-manage" class="entity-link">manage</a> risks posed by the system, including risks to users and society, along with procedures to achieve these goals.
- Includes processes to assess potential impacts.
- Provides a framework for aligning <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> with the organization’s principles, policies, and strategic priorities.
- Links the technical aspects of <a href="#entity-ai-system" class="entity-link">AI system</a> design and development to the organization’s values and principles, and supports the practices and skills of individuals involved in acquiring, training, deploying, and monitoring these systems.
- Addresses the entire product lifecycle and related processes, including legal issues concerning the use of third-party software, hardware, or data.

<a href="#entity-govern" class="entity-link">GOVERN</a> is a fundamental function that runs throughout <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> and supports other functions in the process. Elements of <a href="#entity-govern" class="entity-link">GOVERN</a>, particularly those related to compliance or evaluation, should be integrated into all other functions. Ongoing attention to governance is essential for effective <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> throughout the lifespan of an <a href="#entity-ai-system" class="entity-link">AI system</a> and within the organization’s structure.

Strong governance can enhance internal practices and norms, fostering a culture of risk awareness within the organization. <a href="#entity-governing-authorities" class="entity-link">Governing authorities</a> set the main policies that guide the organization’s mission, goals, values, culture, and <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>. <a href="#entity-senior-leadership" class="entity-link">Senior leadership</a> plays a crucial role in establishing the approach to <a href="#entity-risk-management" class="entity-link">risk management</a> and shaping the <a href="#entity-organizational-culture" class="entity-link">organizational culture</a>.
    </div>
    <div class="chunk-original" id="chunk-34-original">
      
should be iterative, with cross-referencing between functions as necessary. Simi-
larly, there are categories and subcategories with elements that apply to multiple
functions, or that logically should take place before certain subcategory decisions.
5.1
Govern
The GOVERN function:
• cultivates and implements a culture of risk management within organizations design-
ing, developing, deploying, evaluating, or acquiring AI systems;
• outlines processes, documents, and organizational schemes that anticipate, identify,
and manage the risks a system can pose, including to users and others across society
– and procedures to achieve those outcomes;
• incorporates processes to assess potential impacts;
• provides a structure by which AI risk management functions can align with organi-
zational principles, policies, and strategic priorities;
• connects technical aspects of AI system design and development to organizational
values and principles, and enables organizational practices and competencies for the
individuals involved in acquiring, training, deploying, and monitoring such systems;
and
• addresses full product lifecycle and associated processes, including legal and other
issues concerning use of third-party software or hardware systems and data.
Page 21


NIST AI 100-1
AI RMF 1.0
GOVERN is a cross-cutting function that is infused throughout AI risk management and
enables the other functions of the process. Aspects of GOVERN, especially those related to
compliance or evaluation, should be integrated into each of the other functions. Attention
to governance is a continual and intrinsic requirement for effective AI risk management
over an AI system’s lifespan and the organization’s hierarchy.
Strong governance can drive and enhance internal practices and norms to facilitate orga-
nizational risk culture. Governing authorities can determine the overarching policies that
direct an organization’s mission, goals, values, culture, and risk tolerance. Senior leader-
ship sets the tone for risk management within an organization, and with it, organizational
culture.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-35')">
      Show original text
    </div>
    <div class="card" id="chunk-35-rephrase">
      <a href="#entity-governing-authorities" class="entity-link">Governing authorities</a> set the main policies that guide an organization&#x27;s mission, goals, values, culture, and approach to risk. <a href="#entity-senior-leadership" class="entity-link">Senior leadership</a> plays a crucial role in shaping the organization&#x27;s attitude towards <a href="#entity-risk-management" class="entity-link">risk management</a> and its overall culture. Management ensures that the technical aspects of <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> align with these policies and operations. Proper documentation can improve <a href="#entity-transparency" class="entity-link">transparency</a>, enhance human review processes, and increase <a href="#entity-accountability" class="entity-link">accountability</a> within AI teams.

Once <a href="#entity-organizations" class="entity-link">organizations</a> establish the structures, systems, processes, and teams outlined in the <a href="#entity-govern-function" class="entity-link">GOVERN function</a>, they can foster a culture focused on understanding and managing risks. It is essential for users of the Framework to keep implementing the <a href="#entity-govern-function" class="entity-link">GOVERN function</a> as knowledge, culture, and expectations from AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> change over time.

The <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a> details practices for governing <a href="#entity-ai-risks" class="entity-link">AI risks</a>. Below is a summary of the categories and subcategories of the <a href="#entity-govern-function" class="entity-link">GOVERN function</a>:

**<a href="#entity-govern" class="entity-link">GOVERN</a> 1:** Policies, processes, procedures, and practices for identifying, measuring, and managing <a href="#entity-ai-risks" class="entity-link">AI risks</a> are established, clear, and effectively implemented.
- **<a href="#entity-govern" class="entity-link">GOVERN</a> 1.1:** Legal and regulatory requirements related to AI are understood, managed, and documented.
- **<a href="#entity-govern" class="entity-link">GOVERN</a> 1.2:** The principles of <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a> are incorporated into the organization&#x27;s policies, processes, procedures, and practices.
- **<a href="#entity-govern" class="entity-link">GOVERN</a> 1.3:** There are processes and procedures to determine the appropriate level of <a href="#entity-risk-management" class="entity-link">risk management</a> based on the organization&#x27;s <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>.
- **<a href="#entity-govern" class="entity-link">GOVERN</a> 1.4:** The <a href="#entity-risk-management" class="entity-link">risk management</a> process and its results are defined through clear policies, procedures, and controls that reflect the organization&#x27;s risk priorities.
    </div>
    <div class="chunk-original" id="chunk-35-original">
       culture. Governing authorities can determine the overarching policies that
direct an organization’s mission, goals, values, culture, and risk tolerance. Senior leader-
ship sets the tone for risk management within an organization, and with it, organizational
culture. Management aligns the technical aspects of AI risk management to policies and
operations. Documentation can enhance transparency, improve human review processes,
and bolster accountability in AI system teams.
After putting in place the structures, systems, processes, and teams described in the GOV-
ERN function, organizations should benefit from a purpose-driven culture focused on risk
understanding and management. It is incumbent on Framework users to continue to ex-
ecute the GOVERN function as knowledge, cultures, and needs or expectations from AI
actors evolve over time.
Practices related to governing AI risks are described in the NIST AI RMF Playbook. Table
1 lists the GOVERN function’s categories and subcategories.
Table 1: Categories and subcategories for the GOVERN function.
GOVERN 1:
Policies, processes,
procedures, and
practices across the
organization related
to the mapping,
measuring, and
managing of AI
risks are in place,
transparent, and
implemented
effectively.
GOVERN 1.1: Legal and regulatory requirements involving AI
are understood, managed, and documented.
GOVERN 1.2: The characteristics of trustworthy AI are inte-
grated into organizational policies, processes, procedures, and
practices.
GOVERN 1.3: Processes, procedures, and practices are in place
to determine the needed level of risk management activities based
on the organization’s risk tolerance.
GOVERN 1.4: The risk management process and its outcomes are
established through transparent policies, procedures, and other
controls based on organizational risk priorities.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-36')">
      Show original text
    </div>
    <div class="card" id="chunk-36-rephrase">
      <a href="#entity-organizations" class="entity-link">Organizations</a> must <a href="#entity-manage" class="entity-link">manage</a> risks according to their <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>. The <a href="#entity-risk-management" class="entity-link">risk management</a> process and its results should be guided by clear policies and procedures that reflect the organization&#x27;s risk priorities. 

1. Ongoing monitoring and regular reviews of the <a href="#entity-risk-management" class="entity-link">risk management</a> process should be planned, with defined roles and responsibilities, including how often these reviews occur. 
2. There should be systems in place to keep track of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, with resources allocated based on the organization&#x27;s risk priorities. 
3. Safe procedures must be established for decommissioning <a href="#entity-ai-systems" class="entity-link">AI systems</a> to avoid increasing risks or harming the organization&#x27;s trustworthiness. 

<a href="#entity-accountability" class="entity-link">Accountability</a> structures must ensure that the right teams and individuals are empowered, responsible, and trained to identify, <a href="#entity-measure" class="entity-link">measure</a>, and <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 

1. Roles, responsibilities, and communication lines regarding <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> should be documented and understood by everyone in the organization. 
2. Staff and partners should receive training on <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> to fulfill their roles in line with relevant policies and agreements. 
3. <a href="#entity-executive-leadership" class="entity-link">Executive leadership</a> must take responsibility for decisions related to the risks of developing and deploying <a href="#entity-ai-systems" class="entity-link">AI systems</a>.
    </div>
    <div class="chunk-original" id="chunk-36-original">
       the needed level of risk management activities based
on the organization’s risk tolerance.
GOVERN 1.4: The risk management process and its outcomes are
established through transparent policies, procedures, and other
controls based on organizational risk priorities.
Categories
Subcategories
Continued on next page
Page 22


NIST AI 100-1
AI RMF 1.0
Table 1: Categories and subcategories for the GOVERN function. (Continued)
GOVERN 1.5: Ongoing monitoring and periodic review of the
risk management process and its outcomes are planned and or-
ganizational roles and responsibilities clearly defined, including
determining the frequency of periodic review.
GOVERN 1.6: Mechanisms are in place to inventory AI systems
and are resourced according to organizational risk priorities.
GOVERN 1.7: Processes and procedures are in place for decom-
missioning and phasing out AI systems safely and in a man-
ner that does not increase risks or decrease the organization’s
trustworthiness.
GOVERN 2:
Accountability
structures are in
place so that the
appropriate teams
and individuals are
empowered,
responsible, and
trained for mapping,
measuring, and
managing AI risks.
GOVERN 2.1: Roles and responsibilities and lines of communi-
cation related to mapping, measuring, and managing AI risks are
documented and are clear to individuals and teams throughout
the organization.
GOVERN 2.2: The organization’s personnel and partners receive
AI risk management training to enable them to perform their du-
ties and responsibilities consistent with related policies, proce-
dures, and agreements.
GOVERN 2.3: Executive leadership of the organization takes re-
sponsibility for decisions about risks associated with AI system
development and deployment.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-37')">
      Show original text
    </div>
    <div class="card" id="chunk-37-rephrase">
      The organization has specific duties and responsibilities that align with its policies, procedures, and agreements. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 2.3:** The <a href="#entity-executive-leadership" class="entity-link">executive leadership</a> is responsible for making decisions about the risks involved in developing and deploying <a href="#entity-ai-systems" class="entity-link">AI systems</a>. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 3:** The organization prioritizes diversity, equity, inclusion, and accessibility when assessing <a href="#entity-ai-risks" class="entity-link">AI risks</a> throughout its lifecycle. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 3.1:** A <a href="#entity-diverse-team" class="entity-link">diverse team</a>, representing various demographics, disciplines, experiences, and expertise, informs decisions related to assessing <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 3.2:** There are clear policies and procedures that define roles and responsibilities for managing human-AI interactions and overseeing <a href="#entity-ai-systems" class="entity-link">AI systems</a>. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 4:** Teams within the organization are dedicated to fostering a culture that acknowledges and communicates <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 4.1:** Policies and practices encourage critical thinking and a safety-first approach in the design, development, deployment, and use of <a href="#entity-ai-systems" class="entity-link">AI systems</a> to reduce potential negative impacts. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 4.2:** Teams document the risks and potential impacts of the <a href="#entity-ai-technologies" class="entity-link">AI technologies</a> they work with and share this information widely. 

**<a href="#entity-govern" class="entity-link">GOVERN</a> 4.3:** The organization has practices in place for testing AI, identifying incidents, and sharing information. 

**<a href="#entity-govern-5" class="entity-link">GOVERN 5</a>:** There are processes for engaging effectively with relevant <a href="#entity-stakeholders" class="entity-link">stakeholders</a> in the AI field.
    </div>
    <div class="chunk-original" id="chunk-37-original">
       du-
ties and responsibilities consistent with related policies, proce-
dures, and agreements.
GOVERN 2.3: Executive leadership of the organization takes re-
sponsibility for decisions about risks associated with AI system
development and deployment.
GOVERN 3:
Workforce diversity,
equity, inclusion,
and accessibility
processes are
prioritized in the
mapping,
measuring, and
managing of AI
risks throughout the
lifecycle.
GOVERN 3.1: Decision-making related to mapping, measuring,
and managing AI risks throughout the lifecycle is informed by a
diverse team (e.g., diversity of demographics, disciplines, expe-
rience, expertise, and backgrounds).
GOVERN 3.2: Policies and procedures are in place to define and
differentiate roles and responsibilities for human-AI configura-
tions and oversight of AI systems.
GOVERN 4:
Organizational
teams are committed
to a culture
GOVERN 4.1: Organizational policies and practices are in place
to foster a critical thinking and safety-first mindset in the design,
development, deployment, and uses of AI systems to minimize
potential negative impacts.
Categories
Subcategories
Continued on next page
Page 23


NIST AI 100-1
AI RMF 1.0
Table 1: Categories and subcategories for the GOVERN function. (Continued)
that considers and
communicates AI
risk.
GOVERN 4.2: Organizational teams document the risks and po-
tential impacts of the AI technology they design, develop, deploy,
evaluate, and use, and they communicate about the impacts more
broadly.
GOVERN 4.3: Organizational practices are in place to enable AI
testing, identification of incidents, and information sharing.
GOVERN 5:
Processes are in
place for robust
engagement with
relevant AI actors.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-38')">
      Show original text
    </div>
    <div class="card" id="chunk-38-rephrase">
      <a href="#entity-govern" class="entity-link">GOVERN</a> 4.3: There are organizational practices in place for testing AI, identifying incidents, and sharing information. 
<a href="#entity-govern-5" class="entity-link">GOVERN 5</a>: There are processes for effective engagement with relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a>. 
<a href="#entity-govern-5.1" class="entity-link">GOVERN 5.1</a>: The organization has policies to gather, consider, prioritize, and integrate feedback from people outside the team that developed or deployed the <a href="#entity-ai-system" class="entity-link">AI system</a>, focusing on potential individual and societal impacts related to <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 
<a href="#entity-govern-5.2" class="entity-link">GOVERN 5.2</a>: Mechanisms are in place for the team that developed or deployed <a href="#entity-ai-systems" class="entity-link">AI systems</a> to regularly include validated feedback from relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> into the design and implementation of the system. 
<a href="#entity-govern-6" class="entity-link">GOVERN 6</a>: There are policies and procedures to <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a> and benefits that arise from third-party software, data, and supply chain issues. 
<a href="#entity-govern-6.1" class="entity-link">GOVERN 6.1</a>: Policies are established to address <a href="#entity-ai-risks" class="entity-link">AI risks</a> linked to <a href="#entity-third-party-entities" class="entity-link">third-party entities</a>, including potential infringements of their intellectual property or other rights. 
<a href="#entity-govern-6.2" class="entity-link">GOVERN 6.2</a>: Contingency plans are in place to <a href="#entity-manage" class="entity-link">manage</a> failures or incidents involving high-risk <a href="#entity-third-party-data" class="entity-link">third-party data</a> or <a href="#entity-ai-systems" class="entity-link">AI systems</a>. 
The <a href="#entity-map-function" class="entity-link">MAP function</a> helps to frame risks related to an <a href="#entity-ai-system" class="entity-link">AI system</a>. The <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> includes many interconnected activities involving various actors. Often, those responsible for one part of the process do not have complete visibility or control over other parts and their contexts. These interdependencies can make it challenging to predict the impacts of <a href="#entity-ai-systems" class="entity-link">AI systems</a> reliably.
    </div>
    <div class="chunk-original" id="chunk-38-original">
      ly.
GOVERN 4.3: Organizational practices are in place to enable AI
testing, identification of incidents, and information sharing.
GOVERN 5:
Processes are in
place for robust
engagement with
relevant AI actors.
GOVERN 5.1: Organizational policies and practices are in place
to collect, consider, prioritize, and integrate feedback from those
external to the team that developed or deployed the AI system
regarding the potential individual and societal impacts related to
AI risks.
GOVERN 5.2: Mechanisms are established to enable the team
that developed or deployed AI systems to regularly incorporate
adjudicated feedback from relevant AI actors into system design
and implementation.
GOVERN 6: Policies
and procedures are
in place to address
AI risks and benefits
arising from
third-party software
and data and other
supply chain issues.
GOVERN 6.1: Policies and procedures are in place that address
AI risks associated with third-party entities, including risks of in-
fringement of a third-party’s intellectual property or other rights.
GOVERN 6.2: Contingency processes are in place to handle
failures or incidents in third-party data or AI systems deemed to
be high-risk.
Categories
Subcategories
5.2
Map
The MAP function establishes the context to frame risks related to an AI system. The AI
lifecycle consists of many interdependent activities involving a diverse set of actors (See
Figure 3). In practice, AI actors in charge of one part of the process often do not have full
visibility or control over other parts and their associated contexts. The interdependencies
between these activities, and among the relevant AI actors, can make it difficult to reliably
anticipate impacts of AI systems.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-39')">
      Show original text
    </div>
    <div class="card" id="chunk-39-rephrase">
      People involved in the AI process often lack complete visibility and control over different parts and their contexts. The connections between various activities and the AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> can make it hard to predict the effects of <a href="#entity-ai-systems" class="entity-link">AI systems</a> reliably. For instance, early choices about the goals of an <a href="#entity-ai-system" class="entity-link">AI system</a> can change how it behaves and what it can do, while the environment where it is deployed (like the <a href="#entity-end-users" class="entity-link">end users</a> or those affected) can influence the outcomes of the AI&#x27;s decisions. Therefore, even well-meaning actions in one part of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> can be negatively impacted by decisions and conditions in later stages.

This complexity and varying levels of visibility can create uncertainty in managing risks. By anticipating and addressing potential negative risks, <a href="#entity-organizations" class="entity-link">organizations</a> can reduce this uncertainty and improve their decision-making process. The information collected during the <a href="#entity-map-function" class="entity-link">MAP function</a> helps prevent negative risks and guides decisions related to model management and whether an AI solution is appropriate. The results from the <a href="#entity-map-function" class="entity-link">MAP function</a> are crucial for the <a href="#entity-measure" class="entity-link">MEASURE</a> and <a href="#entity-manage" class="entity-link">MANAGE</a> functions. Without understanding the context and being aware of risks, effective <a href="#entity-risk-management" class="entity-link">risk management</a> is challenging. The <a href="#entity-map-function" class="entity-link">MAP function</a> aims to improve an organization’s ability to identify risks and other contributing factors.

Implementing this function is more effective when it includes insights from a diverse <a href="#entity-internal-team" class="entity-link">internal team</a> and involves collaboration with people outside the team that created or deployed the <a href="#entity-ai-system" class="entity-link">AI system</a>. The level of engagement with external partners, <a href="#entity-end-users" class="entity-link">end users</a>, and <a href="#entity-affected-communities" class="entity-link">affected communities</a> may vary depending on the risk level of the <a href="#entity-ai-system" class="entity-link">AI system</a>, the composition of the <a href="#entity-internal-team" class="entity-link">internal team</a>, and the organization&#x27;s policies.
    </div>
    <div class="chunk-original" id="chunk-39-original">
       of the process often do not have full
visibility or control over other parts and their associated contexts. The interdependencies
between these activities, and among the relevant AI actors, can make it difficult to reliably
anticipate impacts of AI systems. For example, early decisions in identifying purposes and
objectives of an AI system can alter its behavior and capabilities, and the dynamics of de-
ployment setting (such as end users or impacted individuals) can shape the impacts of AI
system decisions. As a result, the best intentions within one dimension of the AI lifecycle
can be undermined via interactions with decisions and conditions in other, later activities.
Page 24


NIST AI 100-1
AI RMF 1.0
This complexity and varying levels of visibility can introduce uncertainty into risk man-
agement practices. Anticipating, assessing, and otherwise addressing potential sources of
negative risk can mitigate this uncertainty and enhance the integrity of the decision process.
The information gathered while carrying out the MAP function enables negative risk pre-
vention and informs decisions for processes such as model management, as well as an
initial decision about appropriateness or the need for an AI solution. Outcomes in the
MAP function are the basis for the MEASURE and MANAGE functions. Without contex-
tual knowledge, and awareness of risks within the identified contexts, risk management is
difficult to perform. The MAP function is intended to enhance an organization’s ability to
identify risks and broader contributing factors.
Implementation of this function is enhanced by incorporating perspectives from a diverse
internal team and engagement with those external to the team that developed or deployed
the AI system. Engagement with external collaborators, end users, potentially impacted
communities, and others may vary based on the risk level of a particular AI system, the
makeup of the internal team, and organizational policies.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-40')">
      Show original text
    </div>
    <div class="card" id="chunk-40-rephrase">
      The development and deployment of an <a href="#entity-ai-system" class="entity-link">AI system</a> involves engaging with <a href="#entity-external-collaborators" class="entity-link">external collaborators</a>, <a href="#entity-end-users" class="entity-link">end users</a>, and <a href="#entity-affected-communities" class="entity-link">affected communities</a>. The level of engagement depends on the risk associated with the <a href="#entity-ai-system" class="entity-link">AI system</a>, the <a href="#entity-internal-team" class="entity-link">internal team</a> composition, and the organization&#x27;s policies. Gathering diverse perspectives helps <a href="#entity-organizations" class="entity-link">organizations</a> prevent negative risks and build more <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a> by: 
- Enhancing understanding of different contexts; 
- Validating assumptions about how the AI will be used; 
- Recognizing when the <a href="#entity-ai-system" class="entity-link">AI system</a> is not functioning as intended; 
- Identifying beneficial uses of existing <a href="#entity-ai-systems" class="entity-link">AI systems</a>; 
- Understanding the limitations of AI and machine learning processes; 
- Recognizing real-world constraints that could lead to negative outcomes; 
- Identifying known and potential negative impacts of the AI&#x27;s intended use; 
- Anticipating risks associated with unintended uses of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. 
After completing the <a href="#entity-map-function" class="entity-link">MAP function</a>, users of the Framework should have enough knowledge about the impacts of the <a href="#entity-ai-system" class="entity-link">AI system</a> to make an initial decision on whether to proceed with its design, development, or deployment. If they choose to move forward, <a href="#entity-organizations" class="entity-link">organizations</a> should use the <a href="#entity-measure" class="entity-link">MEASURE</a> and <a href="#entity-manage" class="entity-link">MANAGE</a> functions, along with the policies established in the <a href="#entity-govern-function" class="entity-link">GOVERN function</a>, to <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a>. It is important for users to continually apply the <a href="#entity-map-function" class="entity-link">MAP function</a> as contexts, capabilities, risks, benefits, and impacts change over time. Practices for mapping <a href="#entity-ai-risks" class="entity-link">AI risks</a> are outlined in the <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>. Table 2 provides the categories and subcategories for the <a href="#entity-map-function" class="entity-link">MAP function</a>.
    </div>
    <div class="chunk-original" id="chunk-40-original">
       developed or deployed
the AI system. Engagement with external collaborators, end users, potentially impacted
communities, and others may vary based on the risk level of a particular AI system, the
makeup of the internal team, and organizational policies. Gathering such broad perspec-
tives can help organizations proactively prevent negative risks and develop more trustwor-
thy AI systems by:
• improving their capacity for understanding contexts;
• checking their assumptions about context of use;
• enabling recognition of when systems are not functional within or out of their in-
tended context;
• identifying positive and beneficial uses of their existing AI systems;
• improving understanding of limitations in AI and ML processes;
• identifying constraints in real-world applications that may lead to negative impacts;
• identifying known and foreseeable negative impacts related to intended use of AI
systems; and
• anticipating risks of the use of AI systems beyond intended use.
After completing the MAP function, Framework users should have sufficient contextual
knowledge about AI system impacts to inform an initial go/no-go decision about whether
to design, develop, or deploy an AI system. If a decision is made to proceed, organizations
should utilize the MEASURE and MANAGE functions along with policies and procedures
put into place in the GOVERN function to assist in AI risk management efforts. It is incum-
bent on Framework users to continue applying the MAP function to AI systems as context,
capabilities, risks, benefits, and potential impacts evolve over time.
Practices related to mapping AI risks are described in the NIST AI RMF Playbook. Table
2 lists the MAP function’s categories and subcategories.
Page 25


NIST AI 100-1
AI RMF 1.0
Table 2: Categories and subcategories for the MAP function.
MAP 1: Context is
established and
understood.
MAP 1.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-41')">
      Show original text
    </div>
    <div class="card" id="chunk-41-rephrase">
      <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a> outlines the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (RMF) 1.0, specifically focusing on the <a href="#entity-map-function" class="entity-link">MAP function</a>. 

**<a href="#entity-map" class="entity-link">MAP</a> 1: Establishing Context** 
- **<a href="#entity-map" class="entity-link">MAP</a> 1.1:** Understand and document the intended purposes of the <a href="#entity-ai-system" class="entity-link">AI system</a>, its potential benefits, relevant laws, norms, and the environments where it will be used. This includes identifying the types of users and their expectations, assessing the positive and negative impacts on individuals, communities, <a href="#entity-organizations" class="entity-link">organizations</a>, society, and the environment, and recognizing any assumptions and limitations regarding the <a href="#entity-ai-system" class="entity-link">AI system</a>&#x27;s purposes and risks throughout its lifecycle. 
- **<a href="#entity-map" class="entity-link">MAP</a> 1.2:** Ensure that the team involved in the AI project includes diverse interdisciplinary members with various skills and experiences, and document their participation. Encourage collaboration across different fields. 
- **<a href="#entity-map" class="entity-link">MAP</a> 1.3:** Clearly understand and document the organization’s mission and goals related to <a href="#entity-ai-technology" class="entity-link">AI technology</a>. 
- **<a href="#entity-map" class="entity-link">MAP</a> 1.4:** Define or reassess the business value or context for using the <a href="#entity-ai-system" class="entity-link">AI system</a>. 
- **<a href="#entity-map" class="entity-link">MAP</a> 1.5:** Identify and document the organization’s risk tolerances. 
- **<a href="#entity-map" class="entity-link">MAP</a> 1.6:** Gather and understand system requirements from relevant <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, ensuring that design decisions consider the social and technical implications to <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 

**<a href="#entity-map" class="entity-link">MAP</a> 2: Categorizing the <a href="#entity-ai-system" class="entity-link">AI System</a>** 
- **<a href="#entity-map" class="entity-link">MAP</a> 2:** Perform a categorization of the <a href="#entity-ai-system" class="entity-link">AI system</a>.
    </div>
    <div class="chunk-original" id="chunk-41-original">
       25


NIST AI 100-1
AI RMF 1.0
Table 2: Categories and subcategories for the MAP function.
MAP 1: Context is
established and
understood.
MAP 1.1: Intended purposes, potentially beneficial uses, context-
specific laws, norms and expectations, and prospective settings in
which the AI system will be deployed are understood and docu-
mented. Considerations include: the specific set or types of users
along with their expectations; potential positive and negative im-
pacts of system uses to individuals, communities, organizations,
society, and the planet; assumptions and related limitations about
AI system purposes, uses, and risks across the development or
product AI lifecycle; and related TEVV and system metrics.
MAP 1.2: Interdisciplinary AI actors, competencies, skills, and
capacities for establishing context reflect demographic diversity
and broad domain and user experience expertise, and their par-
ticipation is documented. Opportunities for interdisciplinary col-
laboration are prioritized.
MAP 1.3: The organization’s mission and relevant goals for AI
technology are understood and documented.
MAP 1.4: The business value or context of business use has been
clearly defined or – in the case of assessing existing AI systems
– re-evaluated.
MAP 1.5: Organizational risk tolerances are determined and
documented.
MAP 1.6: System requirements (e.g., “the system shall respect
the privacy of its users”) are elicited from and understood by rel-
evant AI actors. Design decisions take socio-technical implica-
tions into account to address AI risks.
MAP 2:
Categorization of
the AI system is
performed.
MAP 2.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-42')">
      Show original text
    </div>
    <div class="card" id="chunk-42-rephrase">
      The design of <a href="#entity-ai-systems" class="entity-link">AI systems</a> considers social and technical factors to <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a>. 

**<a href="#entity-map" class="entity-link">MAP</a> 2: Categorization of the <a href="#entity-ai-system" class="entity-link">AI System</a>** 
- **<a href="#entity-map" class="entity-link">MAP</a> 2.1:** Define the specific tasks and methods the <a href="#entity-ai-system" class="entity-link">AI system</a> will support, such as classifiers, generative models, or recommenders. 
- **<a href="#entity-map" class="entity-link">MAP</a> 2.2:** Document the <a href="#entity-ai-system" class="entity-link">AI system</a>&#x27;s knowledge limits and how its outputs can be used and monitored by humans. This documentation helps relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> make informed decisions and take appropriate actions. 
- **<a href="#entity-map" class="entity-link">MAP</a> 2.3:** Identify and document considerations related to <a href="#entity-scientific-integrity" class="entity-link">scientific integrity</a> and <a href="#entity-tevv" class="entity-link">TEVV</a> (Trustworthiness, Effectiveness, Validity, and Value), including aspects of experimental design, data collection, and <a href="#entity-system-trustworthiness" class="entity-link">system trustworthiness</a>. 

**<a href="#entity-map" class="entity-link">MAP</a> 3: Understanding AI Capabilities and Impacts** 
- **<a href="#entity-map" class="entity-link">MAP</a> 3.1:** Examine and document the potential benefits of the <a href="#entity-ai-system" class="entity-link">AI system</a>&#x27;s functionality and performance. 
- **<a href="#entity-map" class="entity-link">MAP</a> 3.2:** Assess and document potential costs, including non-monetary costs, that may arise from AI errors or issues with system functionality and trustworthiness, in relation to the organization&#x27;s <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>. 
- **<a href="#entity-map" class="entity-link">MAP</a> 3.3:** Specify and document the targeted application scope based on the <a href="#entity-ai-system" class="entity-link">AI system</a>&#x27;s capabilities, context, and categorization.
    </div>
    <div class="chunk-original" id="chunk-42-original">
       from and understood by rel-
evant AI actors. Design decisions take socio-technical implica-
tions into account to address AI risks.
MAP 2:
Categorization of
the AI system is
performed.
MAP 2.1: The specific tasks and methods used to implement the
tasks that the AI system will support are defined (e.g., classifiers,
generative models, recommenders).
MAP 2.2: Information about the AI system’s knowledge limits
and how system output may be utilized and overseen by humans
is documented. Documentation provides sufficient information
to assist relevant AI actors when making decisions and taking
subsequent actions.
Categories
Subcategories
Continued on next page
Page 26


NIST AI 100-1
AI RMF 1.0
Table 2: Categories and subcategories for the MAP function. (Continued)
MAP 2.3: Scientific integrity and TEVV considerations are iden-
tified and documented, including those related to experimental
design, data collection and selection (e.g., availability, repre-
sentativeness, suitability), system trustworthiness, and construct
validation.
MAP 3: AI
capabilities, targeted
usage, goals, and
expected benefits
and costs compared
with appropriate
benchmarks are
understood.
MAP 3.1: Potential benefits of intended AI system functionality
and performance are examined and documented.
MAP 3.2: Potential costs, including non-monetary costs, which
result from expected or realized AI errors or system functionality
and trustworthiness – as connected to organizational risk toler-
ance – are examined and documented.
MAP 3.3: Targeted application scope is specified and docu-
mented based on the system’s capability, established context, and
AI system categorization.
MAP 3.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-43')">
      Show original text
    </div>
    <div class="card" id="chunk-43-rephrase">
      The <a href="#entity-organizational-risk-tolerance" class="entity-link">organizational risk tolerance</a> is examined and documented. The targeted application scope is specified and documented based on the system&#x27;s capabilities, context, and <a href="#entity-ai-system-categorization" class="entity-link">AI system categorization</a>. Processes for ensuring <a href="#entity-operator-and-practitioner-proficiency" class="entity-link">operator and practitioner proficiency</a> with <a href="#entity-ai-system" class="entity-link">AI system</a> performance and trustworthiness, along with relevant technical standards and certifications, are defined, assessed, and documented. Human oversight processes are also defined, assessed, and documented according to the organization&#x27;s policies from the <a href="#entity-govern-function" class="entity-link">GOVERN function</a>. Risks and benefits of all components of the <a href="#entity-ai-system" class="entity-link">AI system</a>, including <a href="#entity-third-party-software-and-data" class="entity-link">third-party software and data</a>, are mapped. Approaches for identifying and documenting <a href="#entity-ai-technology" class="entity-link">AI technology</a> and legal risks, including those related to <a href="#entity-third-party-data" class="entity-link">third-party data</a> or software and potential infringement of intellectual property rights, are established and followed. <a href="#entity-internal-risk-controls" class="entity-link">Internal risk controls</a> for the <a href="#entity-ai-system" class="entity-link">AI system</a>&#x27;s components, including third-party technologies, are identified and documented. The impacts on individuals, groups, communities, <a href="#entity-organizations" class="entity-link">organizations</a>, and society are characterized. The likelihood and magnitude of each identified impact, both beneficial and harmful, based on expected use, past uses of similar <a href="#entity-ai-systems" class="entity-link">AI systems</a>, public incident reports, and external feedback, are identified and documented.
    </div>
    <div class="chunk-original" id="chunk-43-original">
       to organizational risk toler-
ance – are examined and documented.
MAP 3.3: Targeted application scope is specified and docu-
mented based on the system’s capability, established context, and
AI system categorization.
MAP 3.4: Processes for operator and practitioner proficiency
with AI system performance and trustworthiness – and relevant
technical standards and certifications – are defined, assessed, and
documented.
MAP 3.5: Processes for human oversight are defined, assessed,
and documented in accordance with organizational policies from
the GOVERN function.
MAP 4: Risks and
benefits are mapped
for all components
of the AI system
including third-party
software and data.
MAP 4.1: Approaches for mapping AI technology and legal risks
of its components – including the use of third-party data or soft-
ware – are in place, followed, and documented, as are risks of in-
fringement of a third party’s intellectual property or other rights.
MAP 4.2: Internal risk controls for components of the AI sys-
tem, including third-party AI technologies, are identified and
documented.
MAP 5: Impacts to
individuals, groups,
communities,
organizations, and
society are
characterized.
MAP 5.1: Likelihood and magnitude of each identified impact
(both potentially beneficial and harmful) based on expected use,
past uses of AI systems in similar contexts, public incident re-
ports, feedback from those external to the team that developed
or deployed the AI system, or other data are identified and
documented.
Categories
Subcategories
Continued on next page
Page 27


NIST AI 100-1
AI RMF 1.0
Table 2: Categories and subcategories for the MAP function. (Continued)
MAP 5.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-44')">
      Show original text
    </div>
    <div class="card" id="chunk-44-rephrase">
      Subcategories
Continued on next page
Page 27

<a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>
<a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>
Table 2: Categories and subcategories for the <a href="#entity-map-function" class="entity-link">MAP function</a>. (Continued)

<a href="#entity-map" class="entity-link">MAP</a> 5.2: There are documented practices and personnel in place to regularly engage with relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> and gather feedback on the positive, negative, and unexpected impacts of <a href="#entity-ai-systems" class="entity-link">AI systems</a>.

Categories
Subcategories
5.3
<a href="#entity-measure" class="entity-link">Measure</a>
The <a href="#entity-measure-function" class="entity-link">MEASURE function</a> uses various tools and methods—quantitative, qualitative, or a mix of both—to analyze and monitor <a href="#entity-ai-risks" class="entity-link">AI risks</a> and their impacts. It builds on the <a href="#entity-ai-risks" class="entity-link">AI risks</a> identified in the <a href="#entity-map-function" class="entity-link">MAP function</a> and informs the <a href="#entity-manage-function" class="entity-link">MANAGE function</a>. <a href="#entity-ai-systems" class="entity-link">AI systems</a> should be tested before they are deployed and regularly while they are in use. Measuring <a href="#entity-ai-risks" class="entity-link">AI risks</a> involves documenting how well the systems function and their trustworthiness.

Tracking <a href="#entity-ai-risks" class="entity-link">AI risks</a> includes monitoring metrics related to <a href="#entity-trustworthy-characteristics" class="entity-link">trustworthy characteristics</a>, social impact, and human-AI interactions. The <a href="#entity-measure-function" class="entity-link">MEASURE function</a> should involve thorough software testing and performance assessments, including measures of uncertainty, comparisons to performance benchmarks, and formal reporting of results. Independent reviews can enhance testing effectiveness and reduce internal biases and conflicts of interest.

When there are trade-offs among <a href="#entity-trustworthy-characteristics" class="entity-link">trustworthy characteristics</a>, measurement provides a clear basis for management decisions. Possible actions include recalibrating the system, mitigating impacts, or even removing the system from design, development, production, or use. This may involve various controls such as compensating, detective, deterrent, directive, and recovery measures.

After completing the <a href="#entity-measure-function" class="entity-link">MEASURE function</a>, objective, repeatable, and scalable testing, evaluation, verification, and validation (<a href="#entity-tevv" class="entity-link">TEVV</a>) processes—including metrics, methods, and <a href="#entity-methodologies" class="entity-link">methodologies</a>—should be established, followed, and documented.
    </div>
    <div class="chunk-original" id="chunk-44-original">
      Subcategories
Continued on next page
Page 27


NIST AI 100-1
AI RMF 1.0
Table 2: Categories and subcategories for the MAP function. (Continued)
MAP 5.2: Practices and personnel for supporting regular en-
gagement with relevant AI actors and integrating feedback about
positive, negative, and unanticipated impacts are in place and
documented.
Categories
Subcategories
5.3
Measure
The MEASURE function employs quantitative, qualitative, or mixed-method tools, tech-
niques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related
impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs
the MANAGE function. AI systems should be tested before their deployment and regu-
larly while in operation. AI risk measurements include documenting aspects of systems’
functionality and trustworthiness.
Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact,
and human-AI configurations. Processes developed or adopted in the MEASURE function
should include rigorous software testing and performance assessment methodologies with
associated measures of uncertainty, comparisons to performance benchmarks, and formal-
ized reporting and documentation of results. Processes for independent review can improve
the effectiveness of testing and can mitigate internal biases and potential conflicts of inter-
est.
Where tradeoffs among the trustworthy characteristics arise, measurement provides a trace-
able basis to inform management decisions. Options may include recalibration, impact
mitigation, or removal of the system from design, development, production, or use, as well
as a range of compensating, detective, deterrent, directive, and recovery controls.
After completing the MEASURE function, objective, repeatable, or scalable test, evaluation,
verification, and validation (TEVV) processes including metrics, methods, and methodolo-
gies are in place, followed, and documented.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-45')">
      Show original text
    </div>
    <div class="card" id="chunk-45-rephrase">
      After completing the <a href="#entity-measure-function" class="entity-link">MEASURE function</a>, clear and repeatable processes for testing, evaluating, verifying, and validating (<a href="#entity-tevv" class="entity-link">TEVV</a>) <a href="#entity-ai-systems" class="entity-link">AI systems</a> are established, documented, and followed. These processes include specific metrics and methods that comply with scientific, legal, and ethical standards and are conducted transparently. New qualitative and quantitative measurement methods may need to be created. It&#x27;s important to evaluate how each type of measurement contributes unique and valuable insights into assessing <a href="#entity-ai-risks" class="entity-link">AI risks</a>. Users of the framework will improve their ability to thoroughly assess system reliability, identify and monitor existing and emerging risks, and confirm the effectiveness of the metrics. The results from these measurements will be used in the <a href="#entity-manage-function" class="entity-link">MANAGE function</a> to support <a href="#entity-risk-monitoring-and-response-efforts" class="entity-link">risk monitoring and response efforts</a>. <a href="#entity-framework-users" class="entity-link">Framework users</a> must continue to apply the <a href="#entity-measure-function" class="entity-link">MEASURE function</a> to <a href="#entity-ai-systems" class="entity-link">AI systems</a> as knowledge, methods, risks, and impacts change over time.

The <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a> outlines practices for measuring <a href="#entity-ai-risks" class="entity-link">AI risks</a>. Table 3 details the categories and subcategories of the <a href="#entity-measure-function" class="entity-link">MEASURE function</a>:

- <a href="#entity-measure" class="entity-link">MEASURE</a> 1: Appropriate methods and metrics are identified and applied.
  - <a href="#entity-measure" class="entity-link">MEASURE</a> 1.1: Metrics for measuring <a href="#entity-ai-risks" class="entity-link">AI risks</a> identified during the <a href="#entity-map-function" class="entity-link">MAP function</a> are selected for implementation, starting with the most significant risks. Any risks or <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a> that cannot be measured are documented.
  - <a href="#entity-measure" class="entity-link">MEASURE</a> 1.2: The suitability of AI metrics and the effectiveness of existing controls are regularly reviewed and updated, including reports on errors and their potential impacts on <a href="#entity-affected-communities" class="entity-link">affected communities</a>.
    </div>
    <div class="chunk-original" id="chunk-45-original">
       and recovery controls.
After completing the MEASURE function, objective, repeatable, or scalable test, evaluation,
verification, and validation (TEVV) processes including metrics, methods, and methodolo-
gies are in place, followed, and documented. Metrics and measurement methodologies
should adhere to scientific, legal, and ethical norms and be carried out in an open and trans-
parent process. New types of measurement, qualitative and quantitative, may need to be
developed. The degree to which each measurement type provides unique and meaningful
information to the assessment of AI risks should be considered. Framework users will en-
hance their capacity to comprehensively evaluate system trustworthiness, identify and track
existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will
be utilized in the MANAGE function to assist risk monitoring and response efforts. It is in-
cumbent on Framework users to continue applying the MEASURE function to AI systems
as knowledge, methodologies, risks, and impacts evolve over time.
Page 28


NIST AI 100-1
AI RMF 1.0
Practices related to measuring AI risks are described in the NIST AI RMF Playbook. Table
3 lists the MEASURE function’s categories and subcategories.
Table 3: Categories and subcategories for the MEASURE function.
MEASURE 1:
Appropriate
methods and metrics
are identified and
applied.
MEASURE 1.1: Approaches and metrics for measurement of AI
risks enumerated during the MAP function are selected for imple-
mentation starting with the most significant AI risks. The risks
or trustworthiness characteristics that will not – or cannot – be
measured are properly documented.
MEASURE 1.2: Appropriateness of AI metrics and effectiveness
of existing controls are regularly assessed and updated, including
reports of errors and potential impacts on affected communities.
MEASURE 1.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-46')">
      Show original text
    </div>
    <div class="card" id="chunk-46-rephrase">
      <a href="#entity-measure" class="entity-link">MEASURE</a> 1.2: The suitability of AI metrics and the effectiveness of current controls are regularly reviewed and updated. This includes documenting errors and their potential effects on impacted communities. <a href="#entity-measure" class="entity-link">MEASURE</a> 1.3: Internal experts who were not directly involved in developing the system, as well as independent assessors, participate in regular evaluations and updates. <a href="#entity-domain-experts" class="entity-link">Domain experts</a>, users, external AI professionals, and <a href="#entity-affected-communities" class="entity-link">affected communities</a> are consulted as needed based on the organization&#x27;s <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 2: <a href="#entity-ai-systems" class="entity-link">AI systems</a> are assessed for <a href="#entity-trustworthy-characteristics" class="entity-link">trustworthy characteristics</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.1: Documentation includes test sets, metrics, and details about the tools used during testing and evaluation. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.2: Evaluations involving human subjects comply with relevant requirements, including protections for those subjects, and are representative of the relevant population. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.3: The performance of the <a href="#entity-ai-system" class="entity-link">AI system</a> is measured both qualitatively and quantitatively under conditions similar to where it will be deployed, and these measures are documented. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.4: The functionality and behavior of the <a href="#entity-ai-system" class="entity-link">AI system</a> and its components, as identified in the <a href="#entity-map-function" class="entity-link">MAP function</a>, are monitored during production. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.5: The <a href="#entity-ai-system" class="entity-link">AI system</a> is proven to be valid and reliable, with documented limitations on its applicability beyond the conditions under which it was developed. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.6: The <a href="#entity-ai-system" class="entity-link">AI system</a> is regularly assessed for <a href="#entity-safety-risks" class="entity-link">safety risks</a>, as identified in the <a href="#entity-map-function" class="entity-link">MAP function</a>.
    </div>
    <div class="chunk-original" id="chunk-46-original">
       be
measured are properly documented.
MEASURE 1.2: Appropriateness of AI metrics and effectiveness
of existing controls are regularly assessed and updated, including
reports of errors and potential impacts on affected communities.
MEASURE 1.3: Internal experts who did not serve as front-line
developers for the system and/or independent assessors are in-
volved in regular assessments and updates.
Domain experts,
users, AI actors external to the team that developed or deployed
the AI system, and affected communities are consulted in support
of assessments as necessary per organizational risk tolerance.
MEASURE 2: AI
systems are
evaluated for
trustworthy
characteristics.
MEASURE 2.1: Test sets, metrics, and details about the tools used
during TEVV are documented.
MEASURE 2.2: Evaluations involving human subjects meet ap-
plicable requirements (including human subject protection) and
are representative of the relevant population.
MEASURE 2.3: AI system performance or assurance criteria
are measured qualitatively or quantitatively and demonstrated
for conditions similar to deployment setting(s). Measures are
documented.
MEASURE 2.4: The functionality and behavior of the AI sys-
tem and its components – as identified in the MAP function – are
monitored when in production.
MEASURE 2.5: The AI system to be deployed is demonstrated
to be valid and reliable. Limitations of the generalizability be-
yond the conditions under which the technology was developed
are documented.
Categories
Subcategories
Continued on next page
Page 29


NIST AI 100-1
AI RMF 1.0
Table 3: Categories and subcategories for the MEASURE function. (Continued)
MEASURE 2.6: The AI system is evaluated regularly for safety
risks – as identified in the MAP function.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-47')">
      Show original text
    </div>
    <div class="card" id="chunk-47-rephrase">
      F 1.0 Table 3 outlines the categories and subcategories for the <a href="#entity-measure-function" class="entity-link">MEASURE function</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.6 states that the <a href="#entity-ai-system" class="entity-link">AI system</a> is regularly checked for <a href="#entity-safety-risks" class="entity-link">safety risks</a> identified in the <a href="#entity-map-function" class="entity-link">MAP function</a>. It must be proven safe, with any remaining risks within acceptable limits, and it should be able to fail safely, especially when pushed beyond its knowledge limits. Safety metrics include system reliability, <a href="#entity-robustness" class="entity-link">robustness</a>, real-time monitoring, and response times for failures. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.7 involves evaluating and documenting the <a href="#entity-security-and-resilience" class="entity-link">security and resilience</a> of the <a href="#entity-ai-system" class="entity-link">AI system</a>, as noted in the <a href="#entity-map-function" class="entity-link">MAP function</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.8 requires examining and documenting risks related to <a href="#entity-transparency" class="entity-link">transparency</a> and <a href="#entity-accountability" class="entity-link">accountability</a>, also identified in the <a href="#entity-map-function" class="entity-link">MAP function</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.9 emphasizes that the AI model must be explained, validated, and documented, with its outputs interpreted in context to support responsible use and governance. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.10 focuses on examining and documenting <a href="#entity-privacy" class="entity-link">privacy</a> risks associated with the <a href="#entity-ai-system" class="entity-link">AI system</a>, as identified in the <a href="#entity-map-function" class="entity-link">MAP function</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.11 involves evaluating <a href="#entity-fairness-and-bias" class="entity-link">fairness and bias</a>, with results documented. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.12 assesses and documents the <a href="#entity-environmental-impact" class="entity-link">environmental impact</a> and sustainability of AI model training and management activities, as noted in the <a href="#entity-map-function" class="entity-link">MAP function</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 2.13 evaluates and documents the effectiveness of the <a href="#entity-tevv-metrics" class="entity-link">TEVV metrics</a> and processes used in the <a href="#entity-measure-function" class="entity-link">MEASURE function</a>. <a href="#entity-measure" class="entity-link">MEASURE</a> 3 establishes mechanisms for tracking identified <a href="#entity-ai-risks" class="entity-link">AI risks</a> over time.
    </div>
    <div class="chunk-original" id="chunk-47-original">
      F 1.0
Table 3: Categories and subcategories for the MEASURE function. (Continued)
MEASURE 2.6: The AI system is evaluated regularly for safety
risks – as identified in the MAP function. The AI system to be de-
ployed is demonstrated to be safe, its residual negative risk does
not exceed the risk tolerance, and it can fail safely, particularly if
made to operate beyond its knowledge limits. Safety metrics re-
flect system reliability and robustness, real-time monitoring, and
response times for AI system failures.
MEASURE 2.7: AI system security and resilience – as identified
in the MAP function – are evaluated and documented.
MEASURE 2.8: Risks associated with transparency and account-
ability – as identified in the MAP function – are examined and
documented.
MEASURE 2.9: The AI model is explained, validated, and docu-
mented, and AI system output is interpreted within its context –
as identified in the MAP function – to inform responsible use and
governance.
MEASURE 2.10: Privacy risk of the AI system – as identified in
the MAP function – is examined and documented.
MEASURE 2.11: Fairness and bias – as identified in the MAP
function – are evaluated and results are documented.
MEASURE 2.12: Environmental impact and sustainability of AI
model training and management activities – as identified in the
MAP function – are assessed and documented.
MEASURE 2.13: Effectiveness of the employed TEVV met-
rics and processes in the MEASURE function are evaluated and
documented.
MEASURE 3:
Mechanisms for
tracking identified
AI risks over time
are in place.
MEASURE 3.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-48')">
      Show original text
    </div>
    <div class="card" id="chunk-48-rephrase">
      The effectiveness of the <a href="#entity-tevv-metrics" class="entity-link">TEVV metrics</a> and processes used in the <a href="#entity-measure-function" class="entity-link">MEASURE function</a> is assessed and recorded.

**<a href="#entity-measure" class="entity-link">MEASURE</a> 3:** Mechanisms are established to track identified <a href="#entity-ai-risks" class="entity-link">AI risks</a> over time.

- **<a href="#entity-measure" class="entity-link">MEASURE</a> 3.1:** There are methods, personnel, and documentation in place to regularly identify and monitor existing, unexpected, and emerging <a href="#entity-ai-risks" class="entity-link">AI risks</a> based on both intended and actual performance in real-world applications.
- **<a href="#entity-measure" class="entity-link">MEASURE</a> 3.2:** Risk tracking methods are considered for situations where <a href="#entity-ai-risks" class="entity-link">AI risks</a> are hard to evaluate using current measurement techniques or where metrics are not yet available.

**<a href="#entity-measure" class="entity-link">MEASURE</a> 3.3:** Feedback systems are set up for <a href="#entity-end-users" class="entity-link">end users</a> and <a href="#entity-affected-communities" class="entity-link">affected communities</a> to report issues and challenge system outcomes, and these are integrated into the evaluation metrics of the <a href="#entity-ai-system" class="entity-link">AI system</a>.

**<a href="#entity-measure" class="entity-link">MEASURE</a> 4:** Feedback on the effectiveness of the measurement is collected and evaluated.

- **<a href="#entity-measure" class="entity-link">MEASURE</a> 4.1:** Measurement methods for identifying <a href="#entity-ai-risks" class="entity-link">AI risks</a> are linked to the deployment context and are developed with input from <a href="#entity-domain-experts" class="entity-link">domain experts</a> and other <a href="#entity-end-users" class="entity-link">end users</a>. These methods are documented.
- **<a href="#entity-measure" class="entity-link">MEASURE</a> 4.2:** Results from measuring the trustworthiness of the <a href="#entity-ai-system" class="entity-link">AI system</a> in its deployment context and throughout its lifecycle are validated with input from <a href="#entity-domain-experts" class="entity-link">domain experts</a> and relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> to ensure the system performs as intended. These results are documented.
- **<a href="#entity-measure" class="entity-link">MEASURE</a> 4.3:** Any measurable improvements or declines in performance, based on consultations with relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, including <a href="#entity-affected-communities" class="entity-link">affected communities</a>, and field data about context-specific risks and trustworthiness, are identified and documented.
    </div>
    <div class="chunk-original" id="chunk-48-original">
      iveness of the employed TEVV met-
rics and processes in the MEASURE function are evaluated and
documented.
MEASURE 3:
Mechanisms for
tracking identified
AI risks over time
are in place.
MEASURE 3.1: Approaches, personnel, and documentation are
in place to regularly identify and track existing, unanticipated,
and emergent AI risks based on factors such as intended and ac-
tual performance in deployed contexts.
MEASURE 3.2: Risk tracking approaches are considered for
settings where AI risks are difficult to assess using currently
available measurement techniques or where metrics are not yet
available.
Categories
Subcategories
Continued on next page
Page 30


NIST AI 100-1
AI RMF 1.0
Table 3: Categories and subcategories for the MEASURE function. (Continued)
MEASURE 3.3: Feedback processes for end users and impacted
communities to report problems and appeal system outcomes are
established and integrated into AI system evaluation metrics.
MEASURE 4:
Feedback about
efficacy of
measurement is
gathered and
assessed.
MEASURE 4.1: Measurement approaches for identifying AI risks
are connected to deployment context(s) and informed through
consultation with domain experts and other end users.
Ap-
proaches are documented.
MEASURE 4.2: Measurement results regarding AI system trust-
worthiness in deployment context(s) and across the AI lifecycle
are informed by input from domain experts and relevant AI ac-
tors to validate whether the system is performing consistently as
intended. Results are documented.
MEASURE 4.3: Measurable performance improvements or de-
clines based on consultations with relevant AI actors, in-
cluding affected communities, and field data about context-
relevant risks and trustworthiness characteristics are identified
and documented.
Categories
Subcategories
5.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-49')">
      Show original text
    </div>
    <div class="card" id="chunk-49-rephrase">
      Performance improvements or declines are measured by consulting with relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, including <a href="#entity-affected-communities" class="entity-link">affected communities</a>, and using field data about risks and trustworthiness. 

The <a href="#entity-manage-function" class="entity-link">MANAGE function</a> involves regularly allocating resources to identified risks, as determined by the <a href="#entity-govern-function" class="entity-link">GOVERN function</a>. This includes creating plans to respond to, recover from, and communicate about incidents. Information gathered from expert consultations and AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, established in the <a href="#entity-govern-function" class="entity-link">GOVERN function</a> and used in the <a href="#entity-map-function" class="entity-link">MAP function</a>, helps reduce the chances of system failures and negative impacts. Documentation practices from the <a href="#entity-govern-function" class="entity-link">GOVERN function</a>, along with those in <a href="#entity-map" class="entity-link">MAP</a> and <a href="#entity-measure" class="entity-link">MEASURE</a>, support <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> and enhance <a href="#entity-transparency" class="entity-link">transparency</a> and <a href="#entity-accountability" class="entity-link">accountability</a>. There are processes in place to assess new risks and mechanisms for ongoing improvement. 

After completing the <a href="#entity-manage-function" class="entity-link">MANAGE function</a>, there will be plans for prioritizing risks and for regular monitoring and improvement. Users of the framework will be better equipped to <a href="#entity-manage" class="entity-link">manage</a> the risks associated with deployed <a href="#entity-ai-systems" class="entity-link">AI systems</a> and allocate resources based on assessed risks. It is important for users to continue applying the <a href="#entity-manage-function" class="entity-link">MANAGE function</a> as methods, contexts, risks, and expectations from AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a> change over time. 

The <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a> describes practices for managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>. Table 4 outlines the categories and subcategories of the <a href="#entity-manage-function" class="entity-link">MANAGE function</a>.
    </div>
    <div class="chunk-original" id="chunk-49-original">
      : Measurable performance improvements or de-
clines based on consultations with relevant AI actors, in-
cluding affected communities, and field data about context-
relevant risks and trustworthiness characteristics are identified
and documented.
Categories
Subcategories
5.4
Manage
The MANAGE function entails allocating risk resources to mapped and measured risks on
a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to
respond to, recover from, and communicate about incidents or events.
Contextual information gleaned from expert consultation and input from relevant AI actors
– established in GOVERN and carried out in MAP – is utilized in this function to decrease
the likelihood of system failures and negative impacts. Systematic documentation practices
established in GOVERN and utilized in MAP and MEASURE bolster AI risk management
efforts and increase transparency and accountability. Processes for assessing emergent risks
are in place, along with mechanisms for continual improvement.
After completing the MANAGE function, plans for prioritizing risk and regular monitoring
and improvement will be in place. Framework users will have enhanced capacity to man-
age the risks of deployed AI systems and to allocate risk management resources based on
assessed and prioritized risks. It is incumbent on Framework users to continue to apply
the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or
expectations from relevant AI actors evolve over time.
Page 31


NIST AI 100-1
AI RMF 1.0
Practices related to managing AI risks are described in the NIST AI RMF Playbook. Table
4 lists the MANAGE function’s categories and subcategories.
Table 4: Categories and subcategories for the MANAGE function.
MANAGE 1: AI
risks based on
assessments and
other analytical
output from the
MAP and MEASURE
functions are
prioritized,
responded to, and
managed.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-50')">
      Show original text
    </div>
    <div class="card" id="chunk-50-rephrase">
      The <a href="#entity-manage-function" class="entity-link">MANAGE function</a> has two main categories for handling <a href="#entity-ai-risks" class="entity-link">AI risks</a> and benefits. 

**<a href="#entity-manage" class="entity-link">MANAGE</a> 1: <a href="#entity-ai-risks" class="entity-link">AI Risks</a>**  
1. **Assessing <a href="#entity-ai-risks" class="entity-link">AI Risks</a>**: Risks identified through assessments from the <a href="#entity-map" class="entity-link">MAP</a> and <a href="#entity-measure" class="entity-link">MEASURE</a> functions are prioritized and managed.  
   - **1.1**: Determine if the <a href="#entity-ai-system" class="entity-link">AI system</a> meets its goals and if it should continue development or deployment.  
   - **1.2**: Prioritize treatment of documented <a href="#entity-ai-risks" class="entity-link">AI risks</a> based on their impact, likelihood, and available resources.  
   - **1.3**: Develop, plan, and document responses for high-priority <a href="#entity-ai-risks" class="entity-link">AI risks</a> identified by the <a href="#entity-map-function" class="entity-link">MAP function</a>. Options include mitigating, transferring, avoiding, or accepting the risks.  
   - **1.4**: Document any <a href="#entity-negative-residual-risks" class="entity-link">negative residual risks</a> that remain for both downstream users of <a href="#entity-ai-systems" class="entity-link">AI systems</a> and <a href="#entity-end-users" class="entity-link">end users</a>.  

**<a href="#entity-manage" class="entity-link">MANAGE</a> 2: Maximizing AI Benefits**  
2. **Planning and Implementing Strategies**: Strategies are created to maximize the benefits of AI while minimizing negative impacts, informed by input from relevant <a href="#entity-stakeholders" class="entity-link">stakeholders</a>.  
   - **2.1**: Consider the resources needed to <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a>, along with alternative non-AI methods, to reduce potential impacts.  
   - **2.2**: Implement mechanisms to maintain the value of deployed <a href="#entity-ai-systems" class="entity-link">AI systems</a>.  
   - **2.3**: Follow procedures to respond to and recover from any previously unknown risks once they are identified.  
   - **2.4**: Establish mechanisms and assign responsibilities to deactivate <a href="#entity-ai-systems" class="entity-link">AI systems</a> that perform poorly or yield unintended results.
    </div>
    <div class="chunk-original" id="chunk-50-original">
      categories for the MANAGE function.
MANAGE 1: AI
risks based on
assessments and
other analytical
output from the
MAP and MEASURE
functions are
prioritized,
responded to, and
managed.
MANAGE 1.1: A determination is made as to whether the AI
system achieves its intended purposes and stated objectives and
whether its development or deployment should proceed.
MANAGE 1.2: Treatment of documented AI risks is prioritized
based on impact, likelihood, and available resources or methods.
MANAGE 1.3: Responses to the AI risks deemed high priority, as
identified by the MAP function, are developed, planned, and doc-
umented. Risk response options can include mitigating, transfer-
ring, avoiding, or accepting.
MANAGE 1.4: Negative residual risks (defined as the sum of all
unmitigated risks) to both downstream acquirers of AI systems
and end users are documented.
MANAGE 2:
Strategies to
maximize AI
benefits and
minimize negative
impacts are planned,
prepared,
implemented,
documented, and
informed by input
from relevant AI
actors.
MANAGE 2.1: Resources required to manage AI risks are taken
into account – along with viable non-AI alternative systems, ap-
proaches, or methods – to reduce the magnitude or likelihood of
potential impacts.
MANAGE 2.2: Mechanisms are in place and applied to sustain
the value of deployed AI systems.
MANAGE 2.3: Procedures are followed to respond to and recover
from a previously unknown risk when it is identified.
MANAGE 2.4: Mechanisms are in place and applied, and respon-
sibilities are assigned and understood, to supersede, disengage, or
deactivate AI systems that demonstrate performance or outcomes
inconsistent with intended use.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-51')">
      Show original text
    </div>
    <div class="card" id="chunk-51-rephrase">
      AGE 2.4: There are established procedures to deactivate or disengage <a href="#entity-ai-systems" class="entity-link">AI systems</a> that do not perform as intended, and everyone understands their responsibilities in this process. 

<a href="#entity-manage-3" class="entity-link">MANAGE 3</a>: The risks and benefits of AI from third-party sources are actively managed. 

<a href="#entity-manage-3.1" class="entity-link">MANAGE 3.1</a>: The risks and benefits from third-party AI resources are regularly monitored, and appropriate risk controls are documented. 

<a href="#entity-manage-3.2" class="entity-link">MANAGE 3.2</a>: <a href="#entity-pre-trained-models" class="entity-link">Pre-trained models</a> used in development are included in the regular monitoring and maintenance of the <a href="#entity-ai-system" class="entity-link">AI system</a>. 

<a href="#entity-manage-4" class="entity-link">MANAGE 4</a>: Plans for addressing risks, including response, recovery, and communication strategies for identified <a href="#entity-ai-risks" class="entity-link">AI risks</a>, are documented and regularly reviewed. 

<a href="#entity-manage-4.1" class="entity-link">MANAGE 4.1</a>: Monitoring plans for <a href="#entity-ai-systems" class="entity-link">AI systems</a> after deployment are in place, which include ways to gather and assess feedback from users and other <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, as well as procedures for appeals, overrides, decommissioning, incident response, recovery, and change management. 

<a href="#entity-manage-4.2" class="entity-link">MANAGE 4.2</a>: Activities aimed at continuous improvement are incorporated into <a href="#entity-ai-system" class="entity-link">AI system</a> updates and involve regular communication with <a href="#entity-stakeholders" class="entity-link">stakeholders</a>. 

<a href="#entity-manage-4.3" class="entity-link">MANAGE 4.3</a>: Incidents and errors are reported to relevant <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, including <a href="#entity-affected-communities" class="entity-link">affected communities</a>. There are established processes for tracking, responding to, and recovering from these incidents, which are documented.
    </div>
    <div class="chunk-original" id="chunk-51-original">
      AGE 2.4: Mechanisms are in place and applied, and respon-
sibilities are assigned and understood, to supersede, disengage, or
deactivate AI systems that demonstrate performance or outcomes
inconsistent with intended use.
MANAGE 3: AI
risks and benefits
from third-party
entities are
managed.
MANAGE 3.1: AI risks and benefits from third-party resources
are regularly monitored, and risk controls are applied and
documented.
MANAGE 3.2: Pre-trained models which are used for develop-
ment are monitored as part of AI system regular monitoring and
maintenance.
Categories
Subcategories
Continued on next page
Page 32


NIST AI 100-1
AI RMF 1.0
Table 4: Categories and subcategories for the MANAGE function. (Continued)
MANAGE 4: Risk
treatments,
including response
and recovery, and
communication
plans for the
identified and
measured AI risks
are documented and
monitored regularly.
MANAGE 4.1: Post-deployment AI system monitoring plans
are implemented, including mechanisms for capturing and eval-
uating input from users and other relevant AI actors, appeal
and override, decommissioning, incident response, recovery, and
change management.
MANAGE 4.2: Measurable activities for continual improvements
are integrated into AI system updates and include regular engage-
ment with interested parties, including relevant AI actors.
MANAGE 4.3: Incidents and errors are communicated to relevant
AI actors, including affected communities. Processes for track-
ing, responding to, and recovering from incidents and errors are
followed and documented.
Categories
Subcategories
6.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-52')">
      Show original text
    </div>
    <div class="card" id="chunk-52-rephrase">
      4.3: Incidents and errors are reported to the relevant AI <a href="#entity-stakeholders" class="entity-link">stakeholders</a>, including the communities affected. There are established processes for tracking, responding to, and recovering from these incidents and errors, and these processes are documented.

Categories and Subcategories:

6. <a href="#entity-ai-rmf-profiles" class="entity-link">AI RMF Profiles</a>
<a href="#entity-ai-rmf-profiles" class="entity-link">AI RMF profiles</a> are specific applications of the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (RMF) functions, categories, and subcategories tailored to particular settings or applications. These profiles are based on the user&#x27;s requirements, <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>, and available resources. For example, there could be an <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> profile for hiring or for fair housing. These profiles help <a href="#entity-organizations" class="entity-link">organizations</a> understand how to <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a> at different stages of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> or within specific sectors, technologies, or applications. They guide <a href="#entity-organizations" class="entity-link">organizations</a> in aligning their <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> with their goals, legal requirements, best practices, and <a href="#entity-risk-management" class="entity-link">risk management</a> priorities.

<a href="#entity-ai-rmf-temporal-profiles" class="entity-link">AI RMF temporal profiles</a> describe either the current state or the desired state of <a href="#entity-ai-risk-management-activities" class="entity-link">AI risk management activities</a> in a specific sector, industry, organization, or application. A <a href="#entity-current-profile" class="entity-link">Current Profile</a> shows how AI is currently managed and the associated risks based on current outcomes. A <a href="#entity-target-profile" class="entity-link">Target Profile</a> outlines the outcomes needed to achieve the desired <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> goals. By comparing the Current and Target Profiles, <a href="#entity-organizations" class="entity-link">organizations</a> can identify gaps that need to be addressed to meet their <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> objectives. Action plans can then be created to close these gaps, with prioritization based on the user&#x27;s needs and <a href="#entity-risk-management" class="entity-link">risk management</a> processes. This risk-based approach allows users to compare their methods with others and assess the resources required (like staffing and funding) to achieve their <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> goals effectively and efficiently.
    </div>
    <div class="chunk-original" id="chunk-52-original">
       4.3: Incidents and errors are communicated to relevant
AI actors, including affected communities. Processes for track-
ing, responding to, and recovering from incidents and errors are
followed and documented.
Categories
Subcategories
6.
AI RMF Profiles
AI RMF use-case profiles are implementations of the AI RMF functions, categories, and
subcategories for a specific setting or application based on the requirements, risk tolerance,
and resources of the Framework user: for example, an AI RMF hiring profile or an AI
RMF fair housing profile. Profiles may illustrate and offer insights into how risk can be
managed at various stages of the AI lifecycle or in specific sector, technology, or end-use
applications. AI RMF profiles assist organizations in deciding how they might best manage
AI risk that is well-aligned with their goals, considers legal/regulatory requirements and
best practices, and reflects risk management priorities.
AI RMF temporal profiles are descriptions of either the current state or the desired, target
state of specific AI risk management activities within a given sector, industry, organization,
or application context. An AI RMF Current Profile indicates how AI is currently being
managed and the related risks in terms of current outcomes. A Target Profile indicates the
outcomes needed to achieve the desired or target AI risk management goals.
Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk
management objectives. Action plans can be developed to address these gaps to fulfill
outcomes in a given category or subcategory. Prioritization of gap mitigation is driven by
the user’s needs and risk management processes. This risk-based approach also enables
Framework users to compare their approaches with other approaches and to gauge the
resources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-
effective, prioritized manner.
Page 33


NIST AI 100-1
AI RMF 1.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-53')">
      Show original text
    </div>
    <div class="card" id="chunk-53-rephrase">
      This document outlines approaches and resources needed, such as staffing and funding, to effectively <a href="#entity-manage" class="entity-link">manage</a> <a href="#entity-ai-risks" class="entity-link">AI risks</a> in a prioritized way.

The <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a> <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) 1.0 includes cross-sectoral profiles that address risks associated with AI models or applications used in various sectors. These profiles also provide guidance on how to <a href="#entity-govern" class="entity-link">govern</a>, <a href="#entity-map" class="entity-link">map</a>, <a href="#entity-measure" class="entity-link">measure</a>, and <a href="#entity-manage" class="entity-link">manage</a> risks for common activities across sectors, such as using large language models, cloud services, or acquisitions. The framework allows flexibility in implementation by not prescribing specific profile templates.

Appendix A describes the tasks of <a href="#entity-ai-actors" class="entity-link">AI actors</a> involved in the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. During the Application Context and Data and Input phases, AI Design tasks are carried out. AI Design actors are responsible for creating the concept and objectives of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, planning, designing, and collecting and processing data to ensure the <a href="#entity-ai-system" class="entity-link">AI system</a> is lawful and suitable for its purpose. Their tasks include defining and documenting the system&#x27;s concept, assumptions, context, and requirements, as well as gathering and cleaning data and documenting the dataset&#x27;s metadata and characteristics. <a href="#entity-ai-actors" class="entity-link">AI actors</a> in this category include data scientists, <a href="#entity-domain-experts" class="entity-link">domain experts</a>, socio-cultural analysts, diversity and inclusion experts, community members, <a href="#entity-human-factors" class="entity-link">human factors</a> experts, governance experts, data engineers, data providers, system funders, product managers, <a href="#entity-third-party-entities" class="entity-link">third-party entities</a>, evaluators, and legal and <a href="#entity-privacy" class="entity-link">privacy</a> governance experts. <a href="#entity-ai-development" class="entity-link">AI Development</a> tasks occur during the AI Model phase of the lifecycle.
    </div>
    <div class="chunk-original" id="chunk-53-original">
       approaches and to gauge the
resources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-
effective, prioritized manner.
Page 33


NIST AI 100-1
AI RMF 1.0
AI RMF cross-sectoral profiles cover risks of models or applications that can be used across
use cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure,
and manage risks for activities or business processes common across sectors such as the
use of large language models, cloud-based services or acquisition.
This Framework does not prescribe profile templates, allowing for flexibility in implemen-
tation.
Page 34


NIST AI 100-1
AI RMF 1.0
Appendix A:
Descriptions of AI Actor Tasks from Figures 2 and 3
AI Design tasks are performed during the Application Context and Data and Input phases
of the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI
systems and are responsible for the planning, design, and data collection and processing
tasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include ar-
ticulating and documenting the system’s concept and objectives, underlying assumptions,
context, and requirements; gathering and cleaning data; and documenting the metadata
and characteristics of the dataset. AI actors in this category include data scientists, do-
main experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion,
and accessibility, members of impacted communities, human factors experts (e.g., UX/UI
design), governance experts, data engineers, data providers, system funders, product man-
agers, third-party entities, evaluators, and legal and privacy governance.
AI Development tasks are performed during the AI Model phase of the lifecycle in Figure
2.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-54')">
      Show original text
    </div>
    <div class="card" id="chunk-54-rephrase">
      The <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a> involves several key roles and tasks. Governance experts, data engineers, data providers, system funders, product managers, <a href="#entity-third-party-entities" class="entity-link">third-party entities</a>, evaluators, and legal and <a href="#entity-privacy" class="entity-link">privacy</a> governance professionals all play important roles. 

During the AI Model phase, <a href="#entity-ai-development" class="entity-link">AI Development</a> tasks are carried out. This phase includes building and interpreting AI models, which involves creating, selecting, calibrating, training, and testing algorithms. Key players in this phase are machine learning experts, data scientists, developers, <a href="#entity-third-party-entities" class="entity-link">third-party entities</a>, legal and <a href="#entity-privacy" class="entity-link">privacy</a> experts, and those knowledgeable about socio-cultural factors related to the deployment setting.

In the Task and Output phase, <a href="#entity-ai-deployment" class="entity-link">AI Deployment</a> tasks take place. Here, actors make decisions about how the <a href="#entity-ai-system" class="entity-link">AI system</a> will be used to ensure it is effectively deployed. Tasks include piloting the system, checking compatibility with existing systems, ensuring compliance with regulations, managing organizational changes, and evaluating user experiences. Participants in this phase include system integrators, software developers, <a href="#entity-end-users" class="entity-link">end users</a>, operators, evaluators, and <a href="#entity-domain-experts" class="entity-link">domain experts</a> focused on <a href="#entity-human-factors" class="entity-link">human factors</a> and governance.

The Application Context/Operate and Monitor phase involves <a href="#entity-operation-and-monitoring" class="entity-link">Operation and Monitoring</a> tasks. In this phase, actors operate the <a href="#entity-ai-system" class="entity-link">AI system</a> and regularly assess its output and impacts. This group includes system operators, <a href="#entity-domain-experts" class="entity-link">domain experts</a>, AI designers, users who interpret AI outputs, product developers, evaluators, auditors, <a href="#entity-compliance-experts" class="entity-link">compliance experts</a>, <a href="#entity-organizational-management" class="entity-link">organizational management</a>, and researchers.

Finally, Test, Evaluation, Verification, and Validation (<a href="#entity-tevv" class="entity-link">TEVV</a>) tasks are conducted throughout the entire <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>.
    </div>
    <div class="chunk-original" id="chunk-54-original">
      ), governance experts, data engineers, data providers, system funders, product man-
agers, third-party entities, evaluators, and legal and privacy governance.
AI Development tasks are performed during the AI Model phase of the lifecycle in Figure
2. AI Development actors provide the initial infrastructure of AI systems and are responsi-
ble for model building and interpretation tasks, which involve the creation, selection, cali-
bration, training, and/or testing of models or algorithms. AI actors in this category include
machine learning experts, data scientists, developers, third-party entities, legal and privacy
governance experts, and experts in the socio-cultural and contextual factors associated with
the deployment setting.
AI Deployment tasks are performed during the Task and Output phase of the lifecycle in
Figure 2. AI Deployment actors are responsible for contextual decisions relating to how
the AI system is used to assure deployment of the system into production. Related tasks
include piloting the system, checking compatibility with legacy systems, ensuring regu-
latory compliance, managing organizational change, and evaluating user experience. AI
actors in this category include system integrators, software developers, end users, oper-
ators and practitioners, evaluators, and domain experts with expertise in human factors,
socio-cultural analysis, and governance.
Operation and Monitoring tasks are performed in the Application Context/Operate and
Monitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are
responsible for operating the AI system and working with others to regularly assess system
output and impacts. AI actors in this category include system operators, domain experts, AI
designers, users who interpret or incorporate the output of AI systems, product developers,
evaluators and auditors, compliance experts, organizational management, and members of
the research community.
Test, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout
the AI lifecycle.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-55')">
      Show original text
    </div>
    <div class="card" id="chunk-55-rephrase">
      <a href="#entity-ai-systems" class="entity-link">AI systems</a> involve various roles, including product developers, evaluators, auditors, <a href="#entity-compliance-experts" class="entity-link">compliance experts</a>, organizational managers, and researchers. Throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, tasks related to Testing, Evaluation, Verification, and Validation (<a href="#entity-tevv" class="entity-link">TEVV</a>) are conducted by AI professionals. These tasks help assess the <a href="#entity-ai-system" class="entity-link">AI system</a> or its components and address any issues that arise. Ideally, those who verify and validate the <a href="#entity-ai-system" class="entity-link">AI system</a> should be separate from those who conduct tests and evaluations. 

<a href="#entity-tevv" class="entity-link">TEVV</a> tasks can begin as early as the design phase, where tests are planned based on design requirements. 
- In the design phase, <a href="#entity-tevv" class="entity-link">TEVV</a> tasks focus on validating assumptions about system design, data collection, and measurements relevant to the intended use. 
- During development, tasks include validating and assessing the AI model. 
- In the deployment phase, tasks involve validating the system, integrating it into production, testing, and ensuring compliance with legal and ethical standards. 
- For operations, ongoing monitoring is essential, which includes regular updates, testing, managing reported errors, detecting new issues, and establishing processes for addressing problems. 

<a href="#entity-human-factors" class="entity-link">Human Factors</a> tasks are integrated throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. These tasks emphasize human-centered design, ensuring that <a href="#entity-end-users" class="entity-link">end users</a> and <a href="#entity-stakeholders" class="entity-link">stakeholders</a> are actively involved, incorporating relevant norms and values into system design, and continuously evaluating and improving user experiences.
    </div>
    <div class="chunk-original" id="chunk-55-original">
       output of AI systems, product developers,
evaluators and auditors, compliance experts, organizational management, and members of
the research community.
Test, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout
the AI lifecycle. They are carried out by AI actors who examine the AI system or its
components, or detect and remediate problems. Ideally, AI actors carrying out verification
Page 35


NIST AI 100-1
AI RMF 1.0
and validation tasks are distinct from those who perform test and evaluation actions. Tasks
can be incorporated into a phase as early as design, where tests are planned in accordance
with the design requirement.
• TEVV tasks for design, planning, and data may center on internal and external vali-
dation of assumptions for system design, data collection, and measurements relative
to the intended context of deployment or application.
• TEVV tasks for development (i.e., model building) include model validation and
assessment.
• TEVV tasks for deployment include system validation and integration in production,
with testing, and recalibration for systems and process integration, user experience,
and compliance with existing legal, regulatory, and ethical specifications.
• TEVV tasks for operations involve ongoing monitoring for periodic updates, testing,
and subject matter expert (SME) recalibration of models, the tracking of incidents
or errors reported and their management, the detection of emergent properties and
related impacts, and processes for redress and response.
Human Factors tasks and activities are found throughout the dimensions of the AI life-
cycle. They include human-centered design practices and methodologies, promoting the
active involvement of end users and other interested parties and relevant AI actors, incor-
porating context-specific norms and values in system design, evaluating and adapting end
user experiences, and broad integration of humans and human dynamics in all phases of the
AI lifecycle.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-56')">
      Show original text
    </div>
    <div class="card" id="chunk-56-rephrase">
      <a href="#entity-end-users" class="entity-link">End users</a>, interested parties, and relevant AI professionals should incorporate specific norms and values into the design of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. This includes evaluating and adapting user experiences and ensuring that human dynamics are considered throughout the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>. <a href="#entity-human-factors" class="entity-link">Human factors</a> experts bring diverse skills to understand how AI will be used, promote diversity, engage in consultations, design and assess user experiences, conduct human-centered evaluations, and inform impact assessments.

<a href="#entity-domain-experts" class="entity-link">Domain Experts</a> are professionals or scholars from various fields who provide knowledge about the specific industry or context where an <a href="#entity-ai-system" class="entity-link">AI system</a> is applied. Their insights are crucial for guiding the design and development of <a href="#entity-ai-systems" class="entity-link">AI systems</a> and interpreting results for teams involved in testing and assessing AI impacts.

<a href="#entity-ai-impact-assessment" class="entity-link">AI Impact Assessment</a> involves evaluating the <a href="#entity-accountability" class="entity-link">accountability</a> of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, addressing harmful biases, and examining their effects on safety, liability, and <a href="#entity-security" class="entity-link">security</a>. Professionals in this area offer expertise in technical, <a href="#entity-human-factors" class="entity-link">human factors</a>, socio-cultural, and legal aspects.

<a href="#entity-procurement-tasks" class="entity-link">Procurement tasks</a> are handled by AI professionals with the authority to acquire AI models, products, or services from external developers or vendors.

Governance and Oversight responsibilities fall to AI leaders with management and legal authority within the organization. Key figures in AI governance include <a href="#entity-organizational-management" class="entity-link">organizational management</a>, <a href="#entity-senior-leadership" class="entity-link">senior leadership</a>, and the Board of Directors, who focus on the overall impact and sustainability of the organization.
    </div>
    <div class="chunk-original" id="chunk-56-original">
       of end users and other interested parties and relevant AI actors, incor-
porating context-specific norms and values in system design, evaluating and adapting end
user experiences, and broad integration of humans and human dynamics in all phases of the
AI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives
to understand context of use, inform interdisciplinary and demographic diversity, engage
in consultative processes, design and evaluate user experience, perform human-centered
evaluation and testing, and inform impact assessments.
Domain Expert tasks involve input from multidisciplinary practitioners or scholars who
provide knowledge or expertise in – and about – an industry sector, economic sector, con-
text, or application area where an AI system is being used. AI actors who are domain
experts can provide essential guidance for AI system design and development, and inter-
pret outputs in support of work performed by TEVV and AI impact assessment teams.
AI Impact Assessment tasks include assessing and evaluating requirements for AI system
accountability, combating harmful bias, examining impacts of AI systems, product safety,
liability, and security, among others. AI actors such as impact assessors and evaluators
provide technical, human factor, socio-cultural, and legal expertise.
Procurement tasks are conducted by AI actors with financial, legal, or policy management
authority for acquisition of AI models, products, or services from a third-party developer,
vendor, or contractor.
Governance and Oversight tasks are assumed by AI actors with management, fiduciary,
and legal authority and responsibility for the organization in which an AI system is de-
Page 36


NIST AI 100-1
AI RMF 1.0
signed, developed, and/or deployed. Key AI actors responsible for AI governance include
organizational management, senior leadership, and the Board of Directors. These actors
are parties that are concerned with the impact and sustainability of the organization as a
whole.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-57')">
      Show original text
    </div>
    <div class="card" id="chunk-57-rephrase">
      Key players in AI governance include <a href="#entity-organizational-management" class="entity-link">organizational management</a>, <a href="#entity-senior-leadership" class="entity-link">senior leadership</a>, and the Board of Directors. They focus on the overall impact and sustainability of the organization. 

Additional <a href="#entity-ai-actors" class="entity-link">AI actors</a> include <a href="#entity-third-party-entities" class="entity-link">third-party entities</a> such as providers, developers, vendors, and evaluators of data, algorithms, models, and systems. These external parties are involved in the design and development of <a href="#entity-ai-technologies" class="entity-link">AI technologies</a> for other <a href="#entity-organizations" class="entity-link">organizations</a> or their clients. The technologies they provide can be complex, and their risk tolerances may differ from those of the <a href="#entity-organizations" class="entity-link">organizations</a> using them. 

<a href="#entity-end-users" class="entity-link">End users</a> are individuals or groups who interact with <a href="#entity-ai-systems" class="entity-link">AI systems</a> for specific purposes, ranging from AI experts to first-time users. 

Affected individuals and communities are those who are directly or indirectly impacted by <a href="#entity-ai-systems" class="entity-link">AI systems</a> or decisions made based on AI outputs, even if they do not use the systems themselves. 

<a href="#entity-other-ai-actors" class="entity-link">Other AI actors</a> may offer guidance on managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>, including trade associations, standards <a href="#entity-organizations" class="entity-link">organizations</a>, advocacy groups, researchers, environmental <a href="#entity-organizations" class="entity-link">organizations</a>, and <a href="#entity-civil-society" class="entity-link">civil society</a> groups. 

The <a href="#entity-general-public" class="entity-link">general public</a> is likely to experience both positive and negative effects of <a href="#entity-ai-technologies" class="entity-link">AI technologies</a> and can influence the actions of <a href="#entity-ai-actors" class="entity-link">AI actors</a>. This group includes individuals, communities, and consumers related to the context in which <a href="#entity-ai-systems" class="entity-link">AI systems</a> are developed or used.
    </div>
    <div class="chunk-original" id="chunk-57-original">
      , developed, and/or deployed. Key AI actors responsible for AI governance include
organizational management, senior leadership, and the Board of Directors. These actors
are parties that are concerned with the impact and sustainability of the organization as a
whole.
Additional AI Actors
Third-party entities include providers, developers, vendors, and evaluators of data, al-
gorithms, models, and/or systems and related services for another organization or the or-
ganization’s customers or clients. Third-party entities are responsible for AI design and
development tasks, in whole or in part. By definition, they are external to the design, devel-
opment, or deployment team of the organization that acquires its technologies or services.
The technologies acquired from third-party entities may be complex or opaque, and risk
tolerances may not align with the deploying or operating organization.
End users of an AI system are the individuals or groups that use the system for specific
purposes. These individuals or groups interact with an AI system in a specific context. End
users can range in competency from AI experts to first-time technology end users.
Affected individuals/communities encompass all individuals, groups, communities, or
organizations directly or indirectly affected by AI systems or decisions based on the output
of AI systems. These individuals do not necessarily interact with the deployed system or
application.
Other AI actors may provide formal or quasi-formal norms or guidance for specifying
and managing AI risks. They can include trade associations, standards developing or-
ganizations, advocacy groups, researchers, environmental groups, and civil society
organizations.
The general public is most likely to directly experience positive and negative impacts of
AI technologies. They may provide the motivation for actions taken by the AI actors. This
group can include individuals, communities, and consumers associated with the context in
which an AI system is developed or deployed.
Page 37


NIST AI 100-1
AI RMF 1.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-58')">
      Show original text
    </div>
    <div class="card" id="chunk-58-rephrase">
      <a href="#entity-ai-actors" class="entity-link">AI actors</a> include individuals, communities, and consumers involved in the development or use of <a href="#entity-ai-systems" class="entity-link">AI systems</a>. According to the <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a> document, <a href="#entity-ai-risks" class="entity-link">AI risks</a> differ from <a href="#entity-traditional-software" class="entity-link">traditional software</a> risks in several ways. While both can have significant impacts on <a href="#entity-organizations" class="entity-link">organizations</a> and society, <a href="#entity-ai-systems" class="entity-link">AI systems</a> introduce unique risks that current <a href="#entity-risk-management" class="entity-link">risk management</a> frameworks do not fully address. Some features of AI, like <a href="#entity-pre-trained-models" class="entity-link">pre-trained models</a> and <a href="#entity-transfer-learning" class="entity-link">transfer learning</a>, can improve research and <a href="#entity-accuracy" class="entity-link">accuracy</a>, but they also come with risks. Understanding the context in which AI operates helps identify these risks and <a href="#entity-manage" class="entity-link">manage</a> them effectively.

New or heightened risks specific to AI compared to <a href="#entity-traditional-software" class="entity-link">traditional software</a> include:
- The data used to create <a href="#entity-ai-systems" class="entity-link">AI systems</a> may not accurately represent the intended context, and there may be issues with <a href="#entity-bias" class="entity-link">bias</a> and data quality that affect trustworthiness.
- <a href="#entity-ai-systems" class="entity-link">AI systems</a> rely heavily on data for training, which can be complex and voluminous.
- Changes made during the training process can significantly impact how the <a href="#entity-ai-system" class="entity-link">AI system</a> performs.
- Training <a href="#entity-datasets" class="entity-link">datasets</a> may lose their relevance or become outdated compared to the context in which the AI is deployed.
- The scale and complexity of <a href="#entity-ai-systems" class="entity-link">AI systems</a> can be much greater, with some containing billions or trillions of decision points, compared to <a href="#entity-traditional-software" class="entity-link">traditional software</a>.
    </div>
    <div class="chunk-original" id="chunk-58-original">
       taken by the AI actors. This
group can include individuals, communities, and consumers associated with the context in
which an AI system is developed or deployed.
Page 37


NIST AI 100-1
AI RMF 1.0
Appendix B:
How AI Risks Differ from Traditional Software Risks
As with traditional software, risks from AI-based technology can be bigger than an en-
terprise, span organizations, and lead to societal impacts. AI systems also bring a set of
risks that are not comprehensively addressed by current risk frameworks and approaches.
Some AI system features that present risks also can be beneficial. For example, pre-trained
models and transfer learning can advance research and increase accuracy and resilience
when compared to other models and approaches. Identifying contextual factors in the MAP
function will assist AI actors in determining the level of risk and potential management
efforts.
Compared to traditional software, AI-specific risks that are new or increased include the
following:
• The data used for building an AI system may not be a true or appropriate representa-
tion of the context or intended use of the AI system, and the ground truth may either
not exist or not be available. Additionally, harmful bias and other data quality issues
can affect AI system trustworthiness, which could lead to negative impacts.
• AI system dependency and reliance on data for training tasks, combined with in-
creased volume and complexity typically associated with such data.
• Intentional or unintentional changes during training may fundamentally alter AI sys-
tem performance.
• Datasets used to train AI systems may become detached from their original and in-
tended context or may become stale or outdated relative to deployment context.
• AI system scale and complexity (many systems contain billions or even trillions of
decision points) housed within more traditional software applications.

    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-59')">
      Show original text
    </div>
    <div class="card" id="chunk-59-rephrase">
      <a href="#entity-ai-systems" class="entity-link">AI systems</a> can lose their original purpose or become outdated when used in different contexts. These systems can be very complex, often containing billions or trillions of decision points within <a href="#entity-traditional-software" class="entity-link">traditional software</a>. While <a href="#entity-pre-trained-models" class="entity-link">pre-trained models</a> can enhance research and performance, they may also introduce statistical uncertainty and issues with <a href="#entity-bias" class="entity-link">bias</a>, scientific validity, and reproducibility. It is challenging to predict failures in large-scale <a href="#entity-pre-trained-models" class="entity-link">pre-trained models</a> due to their emergent properties. Additionally, <a href="#entity-ai-systems" class="entity-link">AI systems</a> pose <a href="#entity-privacy" class="entity-link">privacy</a> risks because they can aggregate data more effectively. They may also need more frequent maintenance due to changes in data, models, or concepts. There are concerns about the lack of <a href="#entity-transparency" class="entity-link">transparency</a> and reproducibility in <a href="#entity-ai-systems" class="entity-link">AI systems</a>, as well as underdeveloped software testing standards. Regular testing of AI software is difficult because these systems do not follow the same controls as <a href="#entity-traditional-software" class="entity-link">traditional software</a> development. The costs of developing <a href="#entity-ai-systems" class="entity-link">AI systems</a> can also have environmental impacts, and it is hard to predict or identify side effects beyond statistical measures. <a href="#entity-privacy" class="entity-link">Privacy</a> and <a href="#entity-cybersecurity" class="entity-link">cybersecurity</a> risks must be managed throughout the design, development, deployment, evaluation, and use of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, and these risks should be integrated into broader <a href="#entity-enterprise-risk-management" class="entity-link">enterprise risk management</a> strategies.
    </div>
    <div class="chunk-original" id="chunk-59-original">
       may become detached from their original and in-
tended context or may become stale or outdated relative to deployment context.
• AI system scale and complexity (many systems contain billions or even trillions of
decision points) housed within more traditional software applications.
• Use of pre-trained models that can advance research and improve performance can
also increase levels of statistical uncertainty and cause issues with bias management,
scientific validity, and reproducibility.
• Higher degree of difficulty in predicting failure modes for emergent properties of
large-scale pre-trained models.
• Privacy risk due to enhanced data aggregation capability for AI systems.
• AI systems may require more frequent maintenance and triggers for conducting cor-
rective maintenance due to data, model, or concept drift.
• Increased opacity and concerns about reproducibility.
• Underdeveloped software testing standards and inability to document AI-based prac-
tices to the standard expected of traditionally engineered software for all but the
simplest of cases.
• Difficulty in performing regular AI-based software testing, or determining what to
test, since AI systems are not subject to the same controls as traditional code devel-
opment.
Page 38


NIST AI 100-1
AI RMF 1.0
• Computational costs for developing AI systems and their impact on the environment
and planet.
• Inability to predict or detect the side effects of AI-based systems beyond statistical
measures.
Privacy and cybersecurity risk management considerations and approaches are applicable
in the design, development, deployment, evaluation, and use of AI systems. Privacy and
cybersecurity risks are also considered as part of broader enterprise risk management con-
siderations, which may incorporate AI risks.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-60')">
      Show original text
    </div>
    <div class="card" id="chunk-60-rephrase">
      This text discusses the design, development, deployment, evaluation, and use of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, focusing on <a href="#entity-privacy" class="entity-link">privacy</a> and <a href="#entity-cybersecurity" class="entity-link">cybersecurity</a> risks as part of overall <a href="#entity-enterprise-risk-management" class="entity-link">enterprise risk management</a>. <a href="#entity-organizations" class="entity-link">Organizations</a> can enhance the trustworthiness of AI by using existing standards and guidelines to minimize <a href="#entity-security" class="entity-link">security</a> and <a href="#entity-privacy" class="entity-link">privacy</a> risks. Key frameworks include the <a href="#entity-nist-cybersecurity-framework" class="entity-link">NIST Cybersecurity Framework</a>, <a href="#entity-nist-privacy-framework" class="entity-link">NIST Privacy Framework</a>, <a href="#entity-nist-risk-management-framework" class="entity-link">NIST Risk Management Framework</a>, and <a href="#entity-secure-software-development-framework" class="entity-link">Secure Software Development Framework</a>. These frameworks share similarities with the <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) and are outcome-based, focusing on core functions and categories. However, they differ in their specific areas of focus, and <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> must address additional risks. Current guidance does not fully cover many <a href="#entity-ai-system" class="entity-link">AI system</a> risks, such as: managing <a href="#entity-harmful-bias" class="entity-link">harmful bias</a> in AI, addressing risks from <a href="#entity-generative-ai" class="entity-link">generative AI</a>, tackling <a href="#entity-security" class="entity-link">security</a> issues like evasion and model extraction, understanding the complex vulnerabilities of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, and considering risks from <a href="#entity-third-party-ai-technologies" class="entity-link">third-party AI technologies</a> and off-label uses where <a href="#entity-ai-systems" class="entity-link">AI systems</a> are used outside an organization’s <a href="#entity-security" class="entity-link">security</a> controls.
    </div>
    <div class="chunk-original" id="chunk-60-original">
       and approaches are applicable
in the design, development, deployment, evaluation, and use of AI systems. Privacy and
cybersecurity risks are also considered as part of broader enterprise risk management con-
siderations, which may incorporate AI risks. As part of the effort to address AI trustworthi-
ness characteristics such as “Secure and Resilient” and “Privacy-Enhanced,” organizations
may consider leveraging available standards and guidance that provide broad guidance to
organizations to reduce security and privacy risks, such as, but not limited to, the NIST Cy-
bersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Frame-
work, and the Secure Software Development Framework. These frameworks have some
features in common with the AI RMF. Like most risk management approaches, they are
outcome-based rather than prescriptive and are often structured around a Core set of func-
tions, categories, and subcategories. While there are significant differences between these
frameworks based on the domain addressed – and because AI risk management calls for
addressing many other types of risks – frameworks like those mentioned above may inform
security and privacy considerations in the MAP, MEASURE, and MANAGE functions of the
AI RMF.
At the same time, guidance available before publication of this AI RMF does not compre-
hensively address many AI system risks. For example, existing frameworks and guidance
are unable to:
• adequately manage the problem of harmful bias in AI systems;
• confront the challenging risks related to generative AI;
• comprehensively address security concerns related to evasion, model extraction, mem-
bership inference, availability, or other machine learning attacks;
• account for the complex attack surface of AI systems or other security abuses enabled
by AI systems; and
• consider risks associated with third-party AI technologies, transfer learning, and off-
label use where AI systems may be trained for decision-making outside an organiza-
tion’s security controls
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-61')">
      Show original text
    </div>
    <div class="card" id="chunk-61-rephrase">
      <a href="#entity-ai-systems" class="entity-link">AI systems</a> can pose <a href="#entity-security" class="entity-link">security</a> risks, especially when using third-party technologies or when they are trained for purposes outside an organization&#x27;s <a href="#entity-security" class="entity-link">security</a> measures. It&#x27;s important to monitor advancements in both AI and <a href="#entity-traditional-software" class="entity-link">traditional software</a> to ensure they are used responsibly and effectively. 

<a href="#entity-organizations" class="entity-link">Organizations</a> that create or use <a href="#entity-ai-systems" class="entity-link">AI systems</a> should improve their <a href="#entity-risk-management" class="entity-link">risk management</a> by understanding how humans interact with these systems. The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) helps clarify the roles and responsibilities of people involved in using or managing AI. 

<a href="#entity-ai-systems" class="entity-link">AI systems</a> often convert complex human behaviors and decisions into measurable data, but this can lead to a loss of important context, making it harder to assess their impact on individuals and society. Key areas for further research include: 
1. Clearly defining human roles in decision-making and oversight of <a href="#entity-ai-systems" class="entity-link">AI systems</a>, which can range from fully automated to fully manual processes. Some <a href="#entity-ai-systems" class="entity-link">AI systems</a> can make decisions on their own, while others may need human input or oversight. For example, AI used for video compression may not need human supervision, while other systems do.
    </div>
    <div class="chunk-original" id="chunk-61-original">
       AI systems or other security abuses enabled
by AI systems; and
• consider risks associated with third-party AI technologies, transfer learning, and off-
label use where AI systems may be trained for decision-making outside an organiza-
tion’s security controls or trained in one domain and then “fine-tuned” for another.
Both AI and traditional software technologies and systems are subject to rapid innovation.
Technology advances should be monitored and deployed to take advantage of those devel-
opments and work towards a future of AI that is both trustworthy and responsible.
Page 39


NIST AI 100-1
AI RMF 1.0
Appendix C:
AI Risk Management and Human-AI Interaction
Organizations that design, develop, or deploy AI systems for use in operational settings
may enhance their AI risk management by understanding current limitations of human-
AI interaction. The AI RMF provides opportunities to clearly define and differentiate the
various human roles and responsibilities when using, interacting with, or managing AI
systems.
Many of the data-driven approaches that AI systems rely on attempt to convert or represent
individual and social observational and decision-making practices into measurable quanti-
ties. Representing complex human phenomena with mathematical models can come at the
cost of removing necessary context. This loss of context may in turn make it difficult to
understand individual and societal impacts that are key to AI risk management efforts.
Issues that merit further consideration and research include:
1. Human roles and responsibilities in decision making and overseeing AI systems
need to be clearly defined and differentiated. Human-AI configurations can span
from fully autonomous to fully manual. AI systems can autonomously make deci-
sions, defer decision making to a human expert, or be used by a human decision
maker as an additional opinion. Some AI systems may not require human oversight,
such as models used to improve video compression. Other systems may specifically
require human oversight.
2.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-62')">
      Show original text
    </div>
    <div class="card" id="chunk-62-rephrase">
      <a href="#entity-ai-systems" class="entity-link">AI systems</a> can either make decisions on their own or provide additional insights for human decision makers. Some AI, like those used for video compression, may not need human oversight, while others do. The design and use of <a href="#entity-ai-systems" class="entity-link">AI systems</a> can be influenced by human biases, which can come from individual or group perspectives. These biases can be introduced at various stages of the <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>, including design and modeling, and may not always be harmful. However, they can be worsened by a lack of <a href="#entity-transparency" class="entity-link">transparency</a> in <a href="#entity-ai-systems" class="entity-link">AI systems</a>. Organizational biases can affect team structures and decision-making processes, potentially leading to negative outcomes for users and policymakers. The results of human-AI interactions can vary; in some cases, AI can amplify human biases, resulting in more biased decisions than either the AI or human would make alone. However, if these variations are managed well, they can lead to better teamwork and improved performance. Understanding how humans interpret AI outputs is complex, as individuals have different preferences and skills. The <a href="#entity-govern-function" class="entity-link">GOVERN function</a> helps <a href="#entity-organizations" class="entity-link">organizations</a> define the roles and responsibilities of humans in AI teams and those overseeing AI performance.
    </div>
    <div class="chunk-original" id="chunk-62-original">
       decision making to a human expert, or be used by a human decision
maker as an additional opinion. Some AI systems may not require human oversight,
such as models used to improve video compression. Other systems may specifically
require human oversight.
2. Decisions that go into the design, development, deployment, evaluation, and use
of AI systems reflect systemic and human cognitive biases. AI actors bring their
cognitive biases, both individual and group, into the process. Biases can stem from
end-user decision-making tasks and be introduced across the AI lifecycle via human
assumptions, expectations, and decisions during design and modeling tasks. These
biases, which are not necessarily always harmful, may be exacerbated by AI system
opacity and the resulting lack of transparency. Systemic biases at the organizational
level can influence how teams are structured and who controls the decision-making
processes throughout the AI lifecycle. These biases can also influence downstream
decisions by end users, decision makers, and policy makers and may lead to negative
impacts.
3. Human-AI interaction results vary. Under certain conditions – for example, in
perceptual-based judgment tasks – the AI part of the human-AI interaction can am-
plify human biases, leading to more biased decisions than the AI or human alone.
When these variations are judiciously taken into account in organizing human-AI
teams, however, they can result in complementarity and improved overall perfor-
mance.
Page 40


NIST AI 100-1
AI RMF 1.0
4. Presenting AI system information to humans is complex. Humans perceive and
derive meaning from AI system output and explanations in different ways, reflecting
different individual preferences, traits, and skills.
The GOVERN function provides organizations with the opportunity to clarify and define
the roles and responsibilities for the humans in the Human-AI team configurations and
those who are overseeing the AI system performance.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-63')">
      Show original text
    </div>
    <div class="card" id="chunk-63-rephrase">
      The text discusses two important functions, <a href="#entity-govern" class="entity-link">GOVERN</a> and <a href="#entity-map" class="entity-link">MAP</a>, that help <a href="#entity-organizations" class="entity-link">organizations</a> <a href="#entity-manage" class="entity-link">manage</a> Human-AI teams effectively. The <a href="#entity-govern-function" class="entity-link">GOVERN function</a> helps clarify roles and responsibilities for people involved in overseeing <a href="#entity-ai-systems" class="entity-link">AI systems</a>, making decision-making processes clearer to reduce biases. The <a href="#entity-map-function" class="entity-link">MAP function</a> focuses on defining and documenting how operators can effectively use <a href="#entity-ai-systems" class="entity-link">AI systems</a> and ensure their reliability. By implementing the <a href="#entity-map-function" class="entity-link">MAP function</a>, <a href="#entity-organizations" class="entity-link">organizations</a> can enhance their ability to analyze situations, identify limitations, and assess the real-world impacts of <a href="#entity-ai-systems" class="entity-link">AI systems</a> throughout their lifecycle. Both functions emphasize the need for diverse teams and feedback from <a href="#entity-affected-communities" class="entity-link">affected communities</a> to ensure <a href="#entity-ai-systems" class="entity-link">AI systems</a> align with user needs and societal values. Additionally, ongoing research is necessary to understand how humans can effectively challenge AI outputs and to gather data on how often and why humans override AI decisions.
    </div>
    <div class="chunk-original" id="chunk-63-original">
       reflecting
different individual preferences, traits, and skills.
The GOVERN function provides organizations with the opportunity to clarify and define
the roles and responsibilities for the humans in the Human-AI team configurations and
those who are overseeing the AI system performance. The GOVERN function also creates
mechanisms for organizations to make their decision-making processes more explicit, to
help counter systemic biases.
The MAP function suggests opportunities to define and document processes for operator
and practitioner proficiency with AI system performance and trustworthiness concepts, and
to define relevant technical standards and certifications. Implementing MAP function cat-
egories and subcategories may help organizations improve their internal competency for
analyzing context, identifying procedural and system limitations, exploring and examining
impacts of AI-based systems in the real world, and evaluating decision-making processes
throughout the AI lifecycle.
The GOVERN and MAP functions describe the importance of interdisciplinarity and demo-
graphically diverse teams and utilizing feedback from potentially impacted individuals and
communities. AI actors called out in the AI RMF who perform human factors tasks and
activities can assist technical teams by anchoring in design and development practices to
user intentions and representatives of the broader AI community, and societal values. These
actors further help to incorporate context-specific norms and values in system design and
evaluate end user experiences – in conjunction with AI systems.
AI risk management approaches for human-AI configurations will be augmented by on-
going research and evaluation. For example, the degree to which humans are empowered
and incentivized to challenge AI system output requires further studies. Data about the fre-
quency and rationale with which humans overrule AI system output in deployed systems
may be useful to collect and analyze.
Page 41


NIST AI 100-1
AI RMF 1.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-64')">
      Show original text
    </div>
    <div class="card" id="chunk-64-rephrase">
      Further research could be beneficial in understanding how often and why people override the outputs of <a href="#entity-ai-systems" class="entity-link">AI systems</a> in real-world applications. 

<a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>
<a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>
Appendix D:
Attributes of the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>
When the development of the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> began, <a href="#entity-nist" class="entity-link">NIST</a> identified several important characteristics that have guided its creation and remain unchanged. Here are those key attributes:

1. The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> aims to be risk-based, efficient in resource use, supportive of innovation, and voluntary.
2. It is developed through a consensus-driven process that is open and transparent, allowing all <a href="#entity-stakeholders" class="entity-link">stakeholders</a> to contribute.
3. The language used in the <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> is clear and simple, making it accessible to a wide audience, including senior executives, government officials, and non-experts, while still being detailed enough for professionals in the field. This ensures effective communication of <a href="#entity-ai-risks" class="entity-link">AI risks</a> within <a href="#entity-organizations" class="entity-link">organizations</a> and to the public.
4. The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> provides a common framework for understanding and managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>, including terminology, definitions, metrics, and classifications.
5. It is designed to be user-friendly and compatible with other <a href="#entity-risk-management" class="entity-link">risk management</a> practices, making it easy to integrate into an organization’s overall risk strategy.
6. The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> is applicable across various sectors and technology areas, making it relevant for any <a href="#entity-ai-technology" class="entity-link">AI technology</a> and specific use cases.
7. It focuses on outcomes and does not prescribe specific methods.
    </div>
    <div class="chunk-original" id="chunk-64-original">
       further studies. Data about the fre-
quency and rationale with which humans overrule AI system output in deployed systems
may be useful to collect and analyze.
Page 41


NIST AI 100-1
AI RMF 1.0
Appendix D:
Attributes of the AI RMF
NIST described several key attributes of the AI RMF when work on the Framework first
began. These attributes have remained intact and were used to guide the AI RMF’s devel-
opment. They are provided here as a reference.
The AI RMF strives to:
1. Be risk-based, resource-efficient, pro-innovation, and voluntary.
2. Be consensus-driven and developed and regularly updated through an open, trans-
parent process. All stakeholders should have the opportunity to contribute to the AI
RMF’s development.
3. Use clear and plain language that is understandable by a broad audience, including
senior executives, government officials, non-governmental organization leadership,
and those who are not AI professionals – while still of sufficient technical depth to
be useful to practitioners. The AI RMF should allow for communication of AI risks
across an organization, between organizations, with customers, and to the public at
large.
4. Provide common language and understanding to manage AI risks. The AI RMF
should offer taxonomy, terminology, definitions, metrics, and characterizations for
AI risk.
5. Be easily usable and fit well with other aspects of risk management. Use of the
Framework should be intuitive and readily adaptable as part of an organization’s
broader risk management strategy and processes. It should be consistent or aligned
with other approaches to managing AI risks.
6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI
RMF should be universally applicable to any AI technology and to context-specific
use cases.
7. Be outcome-focused and non-prescriptive.
    </div>
    
    <div class="toggle" onclick="toggleChunk('chunk-65')">
      Show original text
    </div>
    <div class="card" id="chunk-65-rephrase">
      6. The <a href="#entity-ai-risk-management" class="entity-link">AI Risk Management</a> Framework (<a href="#entity-ai-rmf" class="entity-link">AI RMF</a>) should be applicable to various perspectives, sectors, and types of technology, making it suitable for any <a href="#entity-ai-technology" class="entity-link">AI technology</a> and specific use cases. 7. It should focus on outcomes rather than strict rules, offering a variety of outcomes and methods instead of a one-size-fits-all approach. 8. The framework should utilize and promote awareness of existing standards, guidelines, best practices, and tools for managing <a href="#entity-ai-risks" class="entity-link">AI risks</a>, while also highlighting the need for better resources. 9. It should be neutral regarding laws and regulations, allowing <a href="#entity-organizations" class="entity-link">organizations</a> to comply with relevant domestic and international legal requirements. 10. The <a href="#entity-ai-rmf" class="entity-link">AI RMF</a> should be a dynamic document that can be updated as technology evolves, our understanding of <a href="#entity-ai-trustworthiness" class="entity-link">AI trustworthiness</a> improves, and as <a href="#entity-stakeholders" class="entity-link">stakeholders</a> gain insights from implementing <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a> and this framework.
    </div>
    <div class="chunk-original" id="chunk-65-original">
       risks.
6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI
RMF should be universally applicable to any AI technology and to context-specific
use cases.
7. Be outcome-focused and non-prescriptive. The Framework should provide a catalog
of outcomes and approaches rather than prescribe one-size-fits-all requirements.
8. Take advantage of and foster greater awareness of existing standards, guidelines, best
practices, methodologies, and tools for managing AI risks – as well as illustrate the
need for additional, improved resources.
9. Be law- and regulation-agnostic.
The Framework should support organizations’
abilities to operate under applicable domestic and international legal or regulatory
regimes.
10. Be a living document. The AI RMF should be readily updated as technology, under-
standing, and approaches to AI trustworthiness and uses of AI change and as stake-
holders learn from implementing AI risk management generally and this framework
in particular.
Page 42


This publication is available free of charge from:
https://doi.org/10.6028/NIST.AI.100-1

    </div>
     
    <h2 id="entities">Entities</h2>
    
    <div class="entity-card" id="entity-accountability">
      <h3>Accountability</h3>
      
      <div class="desc">A characteristic of trustworthy AI that presupposes transparency.</div>
       
      <ul>
        
        <li>→ <a href="#entity-transparency" class="entity-link">Transparency</a>: Accountability in AI systems is supported by transparency.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-26-rephrase">Chunk 26</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-accountability">
      <h3>accountability</h3>
      
      <div class="desc">Accountability in the context of AI systems involves the responsibility of developers and deployers to ensure that the systems operate fairly and ethically.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems require accountability measures to ensure ethical and fair operation.</li>
        
        <li>← <a href="#entity-risk-management" class="entity-link">risk management</a>: Risk management practices help foster accountability in the deployment of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-27-rephrase">Chunk 27</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-accuracy">
      <h3>accuracy</h3>
      
      <div class="desc">The closeness of results of observations, computations, or estimates to the true values.</div>
       
      <ul>
        
        <li>← <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>: Defines accuracy as the closeness of results to true values.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Contributes to the validity and trustworthiness of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-23-rephrase">Chunk 23</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-affected-communities">
      <h3>affected communities</h3>
      
      <div class="desc">Groups of people who may be impacted by the deployment of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: Affected communities are supported in reporting problems related to AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-48-rephrase">Chunk 48</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-affected-individuals/communities">
      <h3>Affected individuals/communities</h3>
      
      <div class="desc">Individuals, groups, communities, or organizations directly or indirectly affected by AI systems or decisions based on their outputs.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Affected individuals and communities are impacted by the decisions made based on AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-57-rephrase">Chunk 57</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-actor-tasks">
      <h3>AI Actor Tasks</h3>
      
      <div class="desc">Tasks performed by various actors involved in the AI lifecycle, including testing, evaluation, verification, and validation.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: AI Actor Tasks are included in the discussions of the AI RMF Core.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-actors">
      <h3>AI actors</h3>
      
      <div class="desc">AI actors encompass individuals and organizations involved in the AI lifecycle, including those who build, use, verify, and validate AI models.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-lifecycle-stages" class="entity-link">AI lifecycle stages</a>: AI actors are involved in various tasks throughout the AI lifecycle stages.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports AI actors by providing frameworks for managing risks and enhancing the trustworthiness of AI systems.</li>
        
        <li>← <a href="#entity-oecd" class="entity-link">OECD</a>: OECD defines AI actors as those involved in the AI system lifecycle.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI actors manage the risks associated with AI systems throughout their lifecycle, ensuring responsibilities are clearly defined.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: AI actors are involved in various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-people-&-planet-dimension" class="entity-link">People &amp; Planet dimension</a>: The People &amp; Planet dimension includes AI actors who inform the primary audience regarding societal impacts.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 includes diverse AI actors necessary for effective risk management.</li>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core supports the engagement with AI actors for diverse perspectives.</li>
        
        <li>← <a href="#entity-govern-5" class="entity-link">GOVERN 5</a>: GOVERN 5 supports robust engagement with relevant AI actors.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: AI actors operate under the framework of the AI lifecycle.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function includes practices for engaging with relevant AI actors.</li>
        
        <li>→ <a href="#entity-manage-function" class="entity-link">MANAGE function</a>: The planning and implementation of the MANAGE function are informed by input from relevant AI actors.</li>
        
        <li>← <a href="#entity-ai-development" class="entity-link">AI Development</a>: AI Development includes various AI actors such as machine learning experts and data scientists.</li>
        
        <li>← <a href="#entity-ai-deployment" class="entity-link">AI Deployment</a>: AI Deployment includes AI actors like system integrators and end users who are crucial for effective deployment.</li>
        
        <li>← <a href="#entity-operation-and-monitoring" class="entity-link">Operation and Monitoring</a>: Operation and Monitoring includes AI actors such as system operators and compliance experts responsible for system assessment.</li>
        
        <li>← <a href="#entity-tevv" class="entity-link">TEVV</a>: TEVV tasks involve AI actors who ensure the reliability of AI systems throughout their lifecycle.</li>
        
        <li>→ <a href="#entity-test,-evaluation,-verification,-and-validation-(tevv)" class="entity-link">Test, Evaluation, Verification, and Validation (TEVV)</a>: AI actors support the TEVV framework by performing necessary tasks throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-human-factors-professionals" class="entity-link">Human factors professionals</a>: Human factors professionals provide support to AI actors by informing user experience design and evaluation.</li>
        
        <li>← <a href="#entity-ai-impact-assessment" class="entity-link">AI Impact Assessment</a>: AI actors such as impact assessors and evaluators are included in AI impact assessment tasks.</li>
        
        <li>← <a href="#entity-procurement-tasks" class="entity-link">Procurement tasks</a>: Procurement tasks are managed by AI actors with financial, legal, or policy authority.</li>
        
        <li>← <a href="#entity-governance-and-oversight-tasks" class="entity-link">Governance and Oversight tasks</a>: Governance and oversight tasks operate under the authority of AI actors responsible for organizational management.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: NIST AI RMF 1.0 is a document published by NIST that serves as a guideline for AI actors.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework is applicable to AI actors involved in the development and deployment of AI systems.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function supports AI actors in assessing risks associated with AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-3-rephrase">Chunk 3</a> • <a href="#chunk-6-rephrase">Chunk 6</a> • <a href="#chunk-17-rephrase">Chunk 17</a> • <a href="#chunk-19-rephrase">Chunk 19</a> • <a href="#chunk-54-rephrase">Chunk 54</a> • <a href="#chunk-57-rephrase">Chunk 57</a> • <a href="#chunk-58-rephrase">Chunk 58</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-actors">
      <h3>AI actors</h3>
      
      <div class="desc">AI actors are stakeholders involved in the AI lifecycle, each with distinct responsibilities and varying levels of risk awareness.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-lifecycle-stages" class="entity-link">AI lifecycle stages</a>: AI actors are involved in various tasks throughout the AI lifecycle stages.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports AI actors by providing frameworks for managing risks and enhancing the trustworthiness of AI systems.</li>
        
        <li>← <a href="#entity-oecd" class="entity-link">OECD</a>: OECD defines AI actors as those involved in the AI system lifecycle.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI actors manage the risks associated with AI systems throughout their lifecycle, ensuring responsibilities are clearly defined.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: AI actors are involved in various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-people-&-planet-dimension" class="entity-link">People &amp; Planet dimension</a>: The People &amp; Planet dimension includes AI actors who inform the primary audience regarding societal impacts.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 includes diverse AI actors necessary for effective risk management.</li>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core supports the engagement with AI actors for diverse perspectives.</li>
        
        <li>← <a href="#entity-govern-5" class="entity-link">GOVERN 5</a>: GOVERN 5 supports robust engagement with relevant AI actors.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: AI actors operate under the framework of the AI lifecycle.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function includes practices for engaging with relevant AI actors.</li>
        
        <li>→ <a href="#entity-manage-function" class="entity-link">MANAGE function</a>: The planning and implementation of the MANAGE function are informed by input from relevant AI actors.</li>
        
        <li>← <a href="#entity-ai-development" class="entity-link">AI Development</a>: AI Development includes various AI actors such as machine learning experts and data scientists.</li>
        
        <li>← <a href="#entity-ai-deployment" class="entity-link">AI Deployment</a>: AI Deployment includes AI actors like system integrators and end users who are crucial for effective deployment.</li>
        
        <li>← <a href="#entity-operation-and-monitoring" class="entity-link">Operation and Monitoring</a>: Operation and Monitoring includes AI actors such as system operators and compliance experts responsible for system assessment.</li>
        
        <li>← <a href="#entity-tevv" class="entity-link">TEVV</a>: TEVV tasks involve AI actors who ensure the reliability of AI systems throughout their lifecycle.</li>
        
        <li>→ <a href="#entity-test,-evaluation,-verification,-and-validation-(tevv)" class="entity-link">Test, Evaluation, Verification, and Validation (TEVV)</a>: AI actors support the TEVV framework by performing necessary tasks throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-human-factors-professionals" class="entity-link">Human factors professionals</a>: Human factors professionals provide support to AI actors by informing user experience design and evaluation.</li>
        
        <li>← <a href="#entity-ai-impact-assessment" class="entity-link">AI Impact Assessment</a>: AI actors such as impact assessors and evaluators are included in AI impact assessment tasks.</li>
        
        <li>← <a href="#entity-procurement-tasks" class="entity-link">Procurement tasks</a>: Procurement tasks are managed by AI actors with financial, legal, or policy authority.</li>
        
        <li>← <a href="#entity-governance-and-oversight-tasks" class="entity-link">Governance and Oversight tasks</a>: Governance and oversight tasks operate under the authority of AI actors responsible for organizational management.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: NIST AI RMF 1.0 is a document published by NIST that serves as a guideline for AI actors.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework is applicable to AI actors involved in the development and deployment of AI systems.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function supports AI actors in assessing risks associated with AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-15-rephrase">Chunk 15</a> • <a href="#chunk-18-rephrase">Chunk 18</a> • <a href="#chunk-32-rephrase">Chunk 32</a> • <a href="#chunk-50-rephrase">Chunk 50</a> • <a href="#chunk-56-rephrase">Chunk 56</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-actors">
      <h3>AI actors</h3>
      
      <div class="desc">AI actors are individuals or entities responsible for various aspects of the AI lifecycle, ensuring the integrity and performance of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-lifecycle-stages" class="entity-link">AI lifecycle stages</a>: AI actors are involved in various tasks throughout the AI lifecycle stages.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports AI actors by providing frameworks for managing risks and enhancing the trustworthiness of AI systems.</li>
        
        <li>← <a href="#entity-oecd" class="entity-link">OECD</a>: OECD defines AI actors as those involved in the AI system lifecycle.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI actors manage the risks associated with AI systems throughout their lifecycle, ensuring responsibilities are clearly defined.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: AI actors are involved in various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-people-&-planet-dimension" class="entity-link">People &amp; Planet dimension</a>: The People &amp; Planet dimension includes AI actors who inform the primary audience regarding societal impacts.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 includes diverse AI actors necessary for effective risk management.</li>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core supports the engagement with AI actors for diverse perspectives.</li>
        
        <li>← <a href="#entity-govern-5" class="entity-link">GOVERN 5</a>: GOVERN 5 supports robust engagement with relevant AI actors.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: AI actors operate under the framework of the AI lifecycle.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function includes practices for engaging with relevant AI actors.</li>
        
        <li>→ <a href="#entity-manage-function" class="entity-link">MANAGE function</a>: The planning and implementation of the MANAGE function are informed by input from relevant AI actors.</li>
        
        <li>← <a href="#entity-ai-development" class="entity-link">AI Development</a>: AI Development includes various AI actors such as machine learning experts and data scientists.</li>
        
        <li>← <a href="#entity-ai-deployment" class="entity-link">AI Deployment</a>: AI Deployment includes AI actors like system integrators and end users who are crucial for effective deployment.</li>
        
        <li>← <a href="#entity-operation-and-monitoring" class="entity-link">Operation and Monitoring</a>: Operation and Monitoring includes AI actors such as system operators and compliance experts responsible for system assessment.</li>
        
        <li>← <a href="#entity-tevv" class="entity-link">TEVV</a>: TEVV tasks involve AI actors who ensure the reliability of AI systems throughout their lifecycle.</li>
        
        <li>→ <a href="#entity-test,-evaluation,-verification,-and-validation-(tevv)" class="entity-link">Test, Evaluation, Verification, and Validation (TEVV)</a>: AI actors support the TEVV framework by performing necessary tasks throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-human-factors-professionals" class="entity-link">Human factors professionals</a>: Human factors professionals provide support to AI actors by informing user experience design and evaluation.</li>
        
        <li>← <a href="#entity-ai-impact-assessment" class="entity-link">AI Impact Assessment</a>: AI actors such as impact assessors and evaluators are included in AI impact assessment tasks.</li>
        
        <li>← <a href="#entity-procurement-tasks" class="entity-link">Procurement tasks</a>: Procurement tasks are managed by AI actors with financial, legal, or policy authority.</li>
        
        <li>← <a href="#entity-governance-and-oversight-tasks" class="entity-link">Governance and Oversight tasks</a>: Governance and oversight tasks operate under the authority of AI actors responsible for organizational management.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: NIST AI RMF 1.0 is a document published by NIST that serves as a guideline for AI actors.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework is applicable to AI actors involved in the development and deployment of AI systems.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function supports AI actors in assessing risks associated with AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a> • <a href="#chunk-55-rephrase">Chunk 55</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-community">
      <h3>AI community</h3>
      
      <div class="desc">A collective term for organizations and individuals involved in the development and management of artificial intelligence technologies.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework supports the AI community by providing guidelines for risk management.</li>
        
        <li>← <a href="#entity-nist" class="entity-link">NIST</a>: NIST collaborates with the AI community to evaluate the effectiveness of the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-31-rephrase">Chunk 31</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-deployer">
      <h3>AI deployer</h3>
      
      <div class="desc">An AI deployer is responsible for implementing and operating AI models in real-world contexts, facing distinct risks compared to developers.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-developer" class="entity-link">AI developer</a>: AI developers contribute to the deployment of AI systems by providing pre-trained models, which are then utilized by AI deployers.</li>
        
        <li>→ <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: AI deployers assess trustworthiness characteristics based on the operational context of the AI system.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-12-rephrase">Chunk 12</a> • <a href="#chunk-22-rephrase">Chunk 22</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-deployment">
      <h3>AI Deployment</h3>
      
      <div class="desc">AI Deployment involves tasks performed during the Task and Output phase, ensuring the AI system is effectively used in production.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI Deployment includes AI actors like system integrators and end users who are crucial for effective deployment.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-54-rephrase">Chunk 54</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-designer">
      <h3>AI designer</h3>
      
      <div class="desc">An AI designer is an individual responsible for creating the design and specifications of an AI system.</div>
       
      <ul>
        
        <li>→ <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: AI designers may have a different perception of trustworthiness characteristics compared to other AI actors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-22-rephrase">Chunk 22</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-developer">
      <h3>AI developer</h3>
      
      <div class="desc">An AI developer is an individual or organization that creates AI software and implements systems based on specific designs, considering associated risks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-deployer" class="entity-link">AI deployer</a>: AI developers contribute to the deployment of AI systems by providing pre-trained models, which are then utilized by AI deployers.</li>
        
        <li>→ <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: AI developers may interpret trustworthiness characteristics differently based on their role in the AI lifecycle.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-12-rephrase">Chunk 12</a> • <a href="#chunk-22-rephrase">Chunk 22</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-development">
      <h3>AI Development</h3>
      
      <div class="desc">AI Development encompasses tasks performed during the AI Model phase of the lifecycle, focusing on model building and interpretation.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI Development includes various AI actors such as machine learning experts and data scientists.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-54-rephrase">Chunk 54</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-impact-assessment">
      <h3>AI Impact Assessment</h3>
      
      <div class="desc">Tasks that involve assessing and evaluating requirements for AI system accountability and examining the impacts of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-domain-expert" class="entity-link">Domain Expert</a>: Domain experts contribute essential guidance for AI impact assessment tasks.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors such as impact assessors and evaluators are included in AI impact assessment tasks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-56-rephrase">Chunk 56</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-lifecycle">
      <h3>AI lifecycle</h3>
      
      <div class="desc">The AI lifecycle encompasses the stages through which an AI system progresses, from development to deployment and management.</div>
       
      <ul>
        
        <li>→ <a href="#entity-trustworthy-ai-system" class="entity-link">trustworthy AI system</a>: The AI lifecycle supports the development of trustworthy AI systems by outlining the necessary stages and responsibilities involved.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors are involved in various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-tevv-processes" class="entity-link">TEVV processes</a>: TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.</li>
        
        <li>← <a href="#entity-tevv" class="entity-link">TEVV</a>: TEVV tasks are integrated throughout the AI lifecycle to manage risks effectively.</li>
        
        <li>→ <a href="#entity-contextually-sensitive-evaluations" class="entity-link">contextually sensitive evaluations</a>: Enhancing contextual awareness in the AI lifecycle supports contextually sensitive evaluations.</li>
        
        <li>← <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: Trustworthiness characteristics influence the evaluation and management of AI systems throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF framework is applicable to various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function illustrates the interdependencies within the AI lifecycle.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors operate under the framework of the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The AI lifecycle includes various stages where AI risks need to be assessed.</li>
        
        <li>← <a href="#entity-human-factors" class="entity-link">Human Factors</a>: Human Factors are integrated throughout the AI lifecycle to enhance user experience and system design.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-33-rephrase">Chunk 33</a> • <a href="#chunk-38-rephrase">Chunk 38</a> • <a href="#chunk-48-rephrase">Chunk 48</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-lifecycle">
      <h3>AI lifecycle</h3>
      
      <div class="desc">The AI lifecycle encompasses the stages of development, deployment, and evaluation of AI systems, guiding their progression from design to post-deployment.</div>
       
      <ul>
        
        <li>→ <a href="#entity-trustworthy-ai-system" class="entity-link">trustworthy AI system</a>: The AI lifecycle supports the development of trustworthy AI systems by outlining the necessary stages and responsibilities involved.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors are involved in various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-tevv-processes" class="entity-link">TEVV processes</a>: TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.</li>
        
        <li>← <a href="#entity-tevv" class="entity-link">TEVV</a>: TEVV tasks are integrated throughout the AI lifecycle to manage risks effectively.</li>
        
        <li>→ <a href="#entity-contextually-sensitive-evaluations" class="entity-link">contextually sensitive evaluations</a>: Enhancing contextual awareness in the AI lifecycle supports contextually sensitive evaluations.</li>
        
        <li>← <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: Trustworthiness characteristics influence the evaluation and management of AI systems throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF framework is applicable to various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function illustrates the interdependencies within the AI lifecycle.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors operate under the framework of the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The AI lifecycle includes various stages where AI risks need to be assessed.</li>
        
        <li>← <a href="#entity-human-factors" class="entity-link">Human Factors</a>: Human Factors are integrated throughout the AI lifecycle to enhance user experience and system design.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-12-rephrase">Chunk 12</a> • <a href="#chunk-22-rephrase">Chunk 22</a> • <a href="#chunk-26-rephrase">Chunk 26</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-lifecycle">
      <h3>AI lifecycle</h3>
      
      <div class="desc">The AI lifecycle encompasses all stages of AI system development and use, including design, development, deployment, evaluation, and management of AI risks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-trustworthy-ai-system" class="entity-link">trustworthy AI system</a>: The AI lifecycle supports the development of trustworthy AI systems by outlining the necessary stages and responsibilities involved.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors are involved in various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-tevv-processes" class="entity-link">TEVV processes</a>: TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.</li>
        
        <li>← <a href="#entity-tevv" class="entity-link">TEVV</a>: TEVV tasks are integrated throughout the AI lifecycle to manage risks effectively.</li>
        
        <li>→ <a href="#entity-contextually-sensitive-evaluations" class="entity-link">contextually sensitive evaluations</a>: Enhancing contextual awareness in the AI lifecycle supports contextually sensitive evaluations.</li>
        
        <li>← <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: Trustworthiness characteristics influence the evaluation and management of AI systems throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF framework is applicable to various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function illustrates the interdependencies within the AI lifecycle.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors operate under the framework of the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The AI lifecycle includes various stages where AI risks need to be assessed.</li>
        
        <li>← <a href="#entity-human-factors" class="entity-link">Human Factors</a>: Human Factors are integrated throughout the AI lifecycle to enhance user experience and system design.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-17-rephrase">Chunk 17</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-lifecycle">
      <h3>AI lifecycle</h3>
      
      <div class="desc">The stages through which an AI system progresses, from development to deployment, where contextual awareness can be enhanced.</div>
       
      <ul>
        
        <li>→ <a href="#entity-trustworthy-ai-system" class="entity-link">trustworthy AI system</a>: The AI lifecycle supports the development of trustworthy AI systems by outlining the necessary stages and responsibilities involved.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors are involved in various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-tevv-processes" class="entity-link">TEVV processes</a>: TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.</li>
        
        <li>← <a href="#entity-tevv" class="entity-link">TEVV</a>: TEVV tasks are integrated throughout the AI lifecycle to manage risks effectively.</li>
        
        <li>→ <a href="#entity-contextually-sensitive-evaluations" class="entity-link">contextually sensitive evaluations</a>: Enhancing contextual awareness in the AI lifecycle supports contextually sensitive evaluations.</li>
        
        <li>← <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: Trustworthiness characteristics influence the evaluation and management of AI systems throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF framework is applicable to various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function illustrates the interdependencies within the AI lifecycle.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors operate under the framework of the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The AI lifecycle includes various stages where AI risks need to be assessed.</li>
        
        <li>← <a href="#entity-human-factors" class="entity-link">Human Factors</a>: Human Factors are integrated throughout the AI lifecycle to enhance user experience and system design.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-models-and-algorithms">
      <h3>AI models and algorithms</h3>
      
      <div class="desc">The computational methods and structures that underpin AI systems and affect their trustworthiness.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-trustworthiness-characteristics" class="entity-link">AI trustworthiness characteristics</a>: The selection of AI models and algorithms affects the trustworthiness characteristics of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-20-rephrase">Chunk 20</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risk-management">
      <h3>AI risk management</h3>
      
      <div class="desc">AI risk management is a key component of responsible AI development, aimed at mitigating and managing risks associated with AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI risk management supports the responsible development and use of AI systems.</li>
        
        <li>→ <a href="#entity-responsible-ai" class="entity-link">Responsible AI</a>: AI risk management drives responsible uses and practices in AI development.</li>
        
        <li>← <a href="#entity-national-artificial-intelligence-initiative-act-of-2020" class="entity-link">National Artificial Intelligence Initiative Act of 2020</a>: The act directs the development of AI initiatives, including risk management practices.</li>
        
        <li>← <a href="#entity-environmental-groups" class="entity-link">environmental groups</a>: Environmental groups contribute to AI risk management by providing context and understanding of potential impacts.</li>
        
        <li>← <a href="#entity-civil-society-organizations" class="entity-link">civil society organizations</a>: Civil society organizations contribute to AI risk management by offering norms and guidance.</li>
        
        <li>← <a href="#entity-end-users" class="entity-link">end users</a>: End users contribute to AI risk management by sharing their experiences and insights.</li>
        
        <li>→ <a href="#entity-safety-risks" class="entity-link">safety risks</a>: AI risk management practices evaluate and prioritize safety risks to minimize potential harms.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-4-rephrase">Chunk 4</a> • <a href="#chunk-5-rephrase">Chunk 5</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risk-management">
      <h3>AI risk management</h3>
      
      <div class="desc">A practice focused on minimizing potential negative impacts of AI systems, including the need for human intervention in certain cases.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI risk management supports the responsible development and use of AI systems.</li>
        
        <li>→ <a href="#entity-responsible-ai" class="entity-link">Responsible AI</a>: AI risk management drives responsible uses and practices in AI development.</li>
        
        <li>← <a href="#entity-national-artificial-intelligence-initiative-act-of-2020" class="entity-link">National Artificial Intelligence Initiative Act of 2020</a>: The act directs the development of AI initiatives, including risk management practices.</li>
        
        <li>← <a href="#entity-environmental-groups" class="entity-link">environmental groups</a>: Environmental groups contribute to AI risk management by providing context and understanding of potential impacts.</li>
        
        <li>← <a href="#entity-civil-society-organizations" class="entity-link">civil society organizations</a>: Civil society organizations contribute to AI risk management by offering norms and guidance.</li>
        
        <li>← <a href="#entity-end-users" class="entity-link">end users</a>: End users contribute to AI risk management by sharing their experiences and insights.</li>
        
        <li>→ <a href="#entity-safety-risks" class="entity-link">safety risks</a>: AI risk management practices evaluate and prioritize safety risks to minimize potential harms.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-24-rephrase">Chunk 24</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risk-management-activities">
      <h3>AI risk management activities</h3>
      
      <div class="desc">Functions that organize activities to govern, map, measure, and manage risks associated with AI technologies.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: AI risk management activities are designed to govern and manage risks associated with AI technologies.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-3-rephrase">Chunk 3</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risk-management-framework-roadmap">
      <h3>AI Risk Management Framework Roadmap</h3>
      
      <div class="desc">A roadmap that outlines priority research and guidance to enhance the AI RMF.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 supports the development of the AI Risk Management Framework Roadmap.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-8-rephrase">Chunk 8</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risk-management-training">
      <h3>AI risk management training</h3>
      
      <div class="desc">Training provided to personnel and partners to equip them with the skills needed to manage AI risks effectively.</div>
       
      <ul>
        
        <li>→ <a href="#entity-organizational-risk-priorities" class="entity-link">organizational risk priorities</a>: AI risk management training contributes to achieving the organization&#x27;s risk priorities.</li>
        
        <li>← <a href="#entity-executive-leadership" class="entity-link">executive leadership</a>: Executive leadership manages the implementation of AI risk management training within the organization.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-36-rephrase">Chunk 36</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risks">
      <h3>AI Risks</h3>
      
      <div class="desc">AI risks refer to potential negative impacts and uncertainties associated with the deployment and use of AI systems, differing from traditional software risks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI Risks are addressed within the AI RMF 1.0 framework.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a> • <a href="#chunk-3-rephrase">Chunk 3</a> • <a href="#chunk-16-rephrase">Chunk 16</a> • <a href="#chunk-35-rephrase">Chunk 35</a> • <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-44-rephrase">Chunk 44</a> • <a href="#chunk-45-rephrase">Chunk 45</a> • <a href="#chunk-48-rephrase">Chunk 48</a> • <a href="#chunk-50-rephrase">Chunk 50</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risks">
      <h3>AI risks</h3>
      
      <div class="desc">Risks associated with the deployment and operation of AI systems, which are prioritized and managed through the MANAGE function.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risk-management-activities" class="entity-link">AI risk management activities</a>: AI risk management activities are designed to govern and manage risks associated with AI technologies.</li>
        
        <li>→ <a href="#entity-individuals-and-communities" class="entity-link">individuals and communities</a>: AI risks can negatively impact individuals, groups, organizations, and communities.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 supports the management of AI risks by providing a structured framework for integration into enterprise risk management.</li>
        
        <li>→ <a href="#entity-cybersecurity" class="entity-link">cybersecurity</a>: AI risks are aligned with cybersecurity risks, as both involve critical concerns regarding data protection and system integrity.</li>
        
        <li>→ <a href="#entity-privacy" class="entity-link">privacy</a>: AI risks are aligned with privacy concerns, particularly regarding the use of data in AI systems.</li>
        
        <li>← <a href="#entity-small-to-medium-sized-organizations" class="entity-link">small to medium-sized organizations</a>: Small to medium-sized organizations face unique challenges in managing AI risks due to their limited resources.</li>
        
        <li>← <a href="#entity-large-organizations" class="entity-link">large organizations</a>: Large organizations typically have more resources to manage AI risks effectively compared to smaller organizations.</li>
        
        <li>→ <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: AI risks are managed through the processes and practices established in the GOVERN function.</li>
        
        <li>← <a href="#entity-govern-5.1" class="entity-link">GOVERN 5.1</a>: GOVERN 5.1 addresses AI risks by integrating feedback from external sources.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function identifies AI risks to help organizations understand potential negative impacts.</li>
        
        <li>← <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The GOVERN function manages AI risks through the implementation of policies and procedures.</li>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function evaluates AI risks through various metrics and methodologies as part of an overall risk management strategy.</li>
        
        <li>← <a href="#entity-manage-function" class="entity-link">MANAGE function</a>: The MANAGE function supports organizations in their efforts to mitigate AI risks.</li>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function supports the identification and analysis of AI risks.</li>
        
        <li>← <a href="#entity-metrics-and-measurement-methodologies" class="entity-link">metrics and measurement methodologies</a>: Metrics and measurement methodologies support the identification and assessment of AI risks.</li>
        
        <li>→ <a href="#entity-domain-experts" class="entity-link">domain experts</a>: Domain experts provide insights that inform the identification and tracking of AI risks.</li>
        
        <li>→ <a href="#entity-end-users" class="entity-link">end users</a>: End users provide feedback on AI risks based on their interactions with the systems.</li>
        
        <li>→ <a href="#entity-affected-communities" class="entity-link">affected communities</a>: Affected communities are supported in reporting problems related to AI risks.</li>
        
        <li>← <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: The AI lifecycle includes various stages where AI risks need to be assessed.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF 1.0 document describes practices related to managing AI risks.</li>
        
        <li>← <a href="#entity-manage-function" class="entity-link">MANAGE function</a>: The MANAGE function is responsible for managing AI risks based on assessments.</li>
        
        <li>→ <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function identifies high-priority AI risks that need to be addressed.</li>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function provides analytical outputs that inform the assessment of AI risks.</li>
        
        <li>→ <a href="#entity-non-ai-alternative-systems" class="entity-link">non-AI alternative systems</a>: Resources required to manage AI risks are considered alongside viable non-AI alternative systems.</li>
        
        <li>← <a href="#entity-other-ai-actors" class="entity-link">Other AI actors</a>: Other AI actors provide guidance for managing AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-49-rephrase">Chunk 49</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-risks-and-trustworthiness">
      <h3>AI Risks and Trustworthiness</h3>
      
      <div class="desc">This category within the AI RMF Playbook addresses various aspects of AI risks and the criteria necessary for AI systems to be considered trustworthy.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>: The category of AI Risks and Trustworthiness supports the overall framework outlined in the AI RMF Playbook.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI Risks and Trustworthiness aligns with the principles outlined in the AI RMF 1.0.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-1-rephrase">Chunk 1</a> • <a href="#chunk-19-rephrase">Chunk 19</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf">
      <h3>AI RMF</h3>
      
      <div class="desc">The AI RMF is designed to help organizations manage risks associated with AI systems, promoting trustworthy development and use across various contexts.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The AI RMF describes the nature and functionality of AI systems.</li>
        
        <li>← <a href="#entity-oecd-recommendation-on-ai:2019" class="entity-link">OECD Recommendation on AI:2019</a>: The OECD Recommendation on AI:2019 aligns with the principles outlined in the AI RMF.</li>
        
        <li>← <a href="#entity-iso/iec-22989:2022" class="entity-link">ISO/IEC 22989:2022</a>: ISO/IEC 22989:2022 provides standards that align with the AI RMF.</li>
        
        <li>→ <a href="#entity-national-artificial-intelligence-initiative-act-of-2020" class="entity-link">National Artificial Intelligence Initiative Act of 2020</a>: The AI RMF is developed as a resource directed by the National Artificial Intelligence Initiative Act.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The AI RMF supports AI actors by providing frameworks for managing risks and enhancing the trustworthiness of AI systems.</li>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: NIST aligns the AI RMF with applicable international standards and guidelines.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The AI RMF supports the management of risks associated with AI systems to enhance their trustworthiness.</li>
        
        <li>→ <a href="#entity-risk-management" class="entity-link">risk management</a>: The AI RMF develops approaches to enhance risk management specifically for AI technologies.</li>
        
        <li>→ <a href="#entity-tevv" class="entity-link">TEVV</a>: The AI RMF supports the integration of TEVV tasks throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-oecd-framework-for-the-classification-of-ai-systems" class="entity-link">OECD Framework for the Classification of AI systems</a>: The OECD framework is aligned with the principles outlined in the AI RMF.</li>
        
        <li>← <a href="#entity-nist-cybersecurity-framework" class="entity-link">NIST Cybersecurity Framework</a>: The NIST Cybersecurity Framework aligns with the AI RMF in addressing security and privacy considerations.</li>
        
        <li>← <a href="#entity-nist-privacy-framework" class="entity-link">NIST Privacy Framework</a>: The NIST Privacy Framework aligns with the AI RMF to enhance privacy protections in AI systems.</li>
        
        <li>← <a href="#entity-nist-risk-management-framework" class="entity-link">NIST Risk Management Framework</a>: The NIST Risk Management Framework provides a structured approach that complements the AI RMF.</li>
        
        <li>← <a href="#entity-secure-software-development-framework" class="entity-link">Secure Software Development Framework</a>: The Secure Software Development Framework shares principles that can inform the AI RMF.</li>
        
        <li>→ <a href="#entity-map,-measure,-and-manage-functions" class="entity-link">MAP, MEASURE, and MANAGE functions</a>: The AI RMF includes the MAP, MEASURE, and MANAGE functions to address AI system risks.</li>
        
        <li>← <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The GOVERN function supports the AI RMF by clarifying roles and responsibilities in AI team configurations.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function supports the AI RMF by suggesting documentation processes for AI system performance and trustworthiness.</li>
        
        <li>→ <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: The AI RMF is published by NIST as part of their guidelines on AI risk management.</li>
        
        <li>→ <a href="#entity-organizations" class="entity-link">organizations</a>: The AI RMF supports organizations&#x27; abilities to operate under applicable legal or regulatory regimes.</li>
        
        <li>→ <a href="#entity-standards,-guidelines,-best-practices,-methodologies,-and-tools" class="entity-link">standards, guidelines, best practices, methodologies, and tools</a>: The AI RMF fosters greater awareness of existing resources for managing AI risks.</li>
        
        <li>→ <a href="#entity-stakeholders" class="entity-link">stakeholders</a>: The AI RMF provides a framework for stakeholders to learn from implementing AI risk management.</li>
        
        <li>→ <a href="#entity-ai-technology" class="entity-link">AI technology</a>: The AI RMF is applicable to any AI technology and context-specific use cases.</li>
        
        <li>← <a href="#entity-nist.ai.100-1" class="entity-link">NIST.AI.100-1</a>: The document NIST.AI.100-1 is a publication related to the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-4-rephrase">Chunk 4</a> • <a href="#chunk-6-rephrase">Chunk 6</a> • <a href="#chunk-9-rephrase">Chunk 9</a> • <a href="#chunk-17-rephrase">Chunk 17</a> • <a href="#chunk-18-rephrase">Chunk 18</a> • <a href="#chunk-60-rephrase">Chunk 60</a> • <a href="#chunk-63-rephrase">Chunk 63</a> • <a href="#chunk-65-rephrase">Chunk 65</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-1">
      <h3>AI RMF 1</h3>
      
      <div class="desc">The first version of the AI Risk Management Framework developed by NIST.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 is a document published by NIST related to the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-57-rephrase">Chunk 57</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-1.0">
      <h3>AI RMF 1.0</h3>
      
      <div class="desc">AI RMF 1.0 is a framework developed by NIST for managing risks associated with AI systems, emphasizing governance, fairness, and bias management.</div>
       
      <ul>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: The AI RMF 1.0 document is published by NIST.</li>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI Risks</a>: AI Risks are addressed within the AI RMF 1.0 framework.</li>
        
        <li>← <a href="#entity-oecd-framework-for-the-classification-of-ai-systems" class="entity-link">OECD Framework for the Classification of AI systems</a>: The OECD Framework influences the design and understanding of AI systems within the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: NIST develops the AI RMF 1.0 framework to guide AI risk management.</li>
        
        <li>→ <a href="#entity-ai-risk-management-framework-roadmap" class="entity-link">AI Risk Management Framework Roadmap</a>: The AI RMF 1.0 supports the development of the AI Risk Management Framework Roadmap.</li>
        
        <li>→ <a href="#entity-iso-31000:2018" class="entity-link">ISO 31000:2018</a>: The principles of ISO 31000:2018 are infused throughout the AI RMF 1.0 framework.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 outlines and publishes the AI RMF 1.0 framework for managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The AI RMF 1.0 framework supports the responsible management of risks associated with AI systems through structured guidelines.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 includes the AI RMF 1.0 framework as part of its guidelines for AI risk management.</li>
        
        <li>→ <a href="#entity-risk-tolerance" class="entity-link">Risk Tolerance</a>: The AI RMF supports the prioritization of risk tolerance in AI systems.</li>
        
        <li>← <a href="#entity-risk-management-framework" class="entity-link">risk management framework</a>: The risk management framework is developed to augment existing practices like the AI RMF.</li>
        
        <li>→ <a href="#entity-risk-management-culture" class="entity-link">risk management culture</a>: The AI RMF 1.0 supports the development of a risk management culture within organizations by providing guidelines for risk assessment.</li>
        
        <li>→ <a href="#entity-residual-risk" class="entity-link">residual risk</a>: AI RMF 1.0 includes the concept of residual risk, emphasizing its importance in understanding the impacts on end users.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The AI RMF 1.0 supports the management of AI risks by providing a structured framework for integration into enterprise risk management.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 provides guidelines and standards that inform the implementation of the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The AI RMF 1.0 includes diverse AI actors necessary for effective risk management.</li>
        
        <li>← <a href="#entity-ai-risks-and-trustworthiness" class="entity-link">AI Risks and Trustworthiness</a>: AI Risks and Trustworthiness aligns with the principles outlined in the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a>: AI RMF 1.0 provides guidance for achieving trustworthy AI.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: Includes the AI Risk Management Framework 1.0 as part of its content.</li>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: AI RMF 1.0 supports the development of AI safety risk management approaches.</li>
        
        <li>→ <a href="#entity-fairness-in-ai" class="entity-link">Fairness in AI</a>: AI RMF 1.0 is aligned with the principles of fairness in AI, emphasizing the need to manage bias.</li>
        
        <li>→ <a href="#entity-systemic-bias" class="entity-link">systemic bias</a>: AI RMF 1.0 identifies systemic bias as a major category of AI bias.</li>
        
        <li>→ <a href="#entity-computational-and-statistical-bias" class="entity-link">computational and statistical bias</a>: AI RMF 1.0 identifies computational and statistical bias as a major category of AI bias.</li>
        
        <li>→ <a href="#entity-human-cognitive-bias" class="entity-link">human-cognitive bias</a>: AI RMF 1.0 identifies human-cognitive bias as a major category of AI bias.</li>
        
        <li>← <a href="#entity-nist-special-publication-1270" class="entity-link">NIST Special Publication 1270</a>: NIST Special Publication 1270 aligns with the AI RMF 1.0 framework for managing AI risks.</li>
        
        <li>← <a href="#entity-nist" class="entity-link">NIST</a>: NIST develops the AI RMF 1.0 framework to assist organizations in managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-community" class="entity-link">AI community</a>: The AI RMF 1.0 framework supports the AI community by providing guidelines for risk management.</li>
        
        <li>→ <a href="#entity-tevv-practices" class="entity-link">TEVV practices</a>: The AI RMF 1.0 framework includes TEVV practices for evaluating AI systems.</li>
        
        <li>← <a href="#entity-govern" class="entity-link">GOVERN</a>: The GOVERN function supports the implementation of AI RMF 1.0 by integrating governance into AI risk management practices.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 aligns with the principles and framework of AI RMF 1.0 for effective risk management.</li>
        
        <li>→ <a href="#entity-map-function" class="entity-link">MAP function</a>: The AI RMF 1.0 includes the MAP function, which is essential for establishing context and categorizing AI systems.</li>
        
        <li>→ <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: The AI RMF 1.0 framework is published as part of the NIST AI 100-1 document.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The AI RMF 1.0 framework is applicable to AI actors involved in the development and deployment of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a> • <a href="#chunk-15-rephrase">Chunk 15</a> • <a href="#chunk-28-rephrase">Chunk 28</a> • <a href="#chunk-29-rephrase">Chunk 29</a> • <a href="#chunk-34-rephrase">Chunk 34</a> • <a href="#chunk-46-rephrase">Chunk 46</a> • <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-1.0">
      <h3>AI RMF 1.0</h3>
      
      <div class="desc">The AI RMF 1.0 provides structured guidelines for managing risks associated with AI systems, emphasizing trustworthiness and effective risk management practices.</div>
       
      <ul>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: The AI RMF 1.0 document is published by NIST.</li>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI Risks</a>: AI Risks are addressed within the AI RMF 1.0 framework.</li>
        
        <li>← <a href="#entity-oecd-framework-for-the-classification-of-ai-systems" class="entity-link">OECD Framework for the Classification of AI systems</a>: The OECD Framework influences the design and understanding of AI systems within the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: NIST develops the AI RMF 1.0 framework to guide AI risk management.</li>
        
        <li>→ <a href="#entity-ai-risk-management-framework-roadmap" class="entity-link">AI Risk Management Framework Roadmap</a>: The AI RMF 1.0 supports the development of the AI Risk Management Framework Roadmap.</li>
        
        <li>→ <a href="#entity-iso-31000:2018" class="entity-link">ISO 31000:2018</a>: The principles of ISO 31000:2018 are infused throughout the AI RMF 1.0 framework.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 outlines and publishes the AI RMF 1.0 framework for managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The AI RMF 1.0 framework supports the responsible management of risks associated with AI systems through structured guidelines.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 includes the AI RMF 1.0 framework as part of its guidelines for AI risk management.</li>
        
        <li>→ <a href="#entity-risk-tolerance" class="entity-link">Risk Tolerance</a>: The AI RMF supports the prioritization of risk tolerance in AI systems.</li>
        
        <li>← <a href="#entity-risk-management-framework" class="entity-link">risk management framework</a>: The risk management framework is developed to augment existing practices like the AI RMF.</li>
        
        <li>→ <a href="#entity-risk-management-culture" class="entity-link">risk management culture</a>: The AI RMF 1.0 supports the development of a risk management culture within organizations by providing guidelines for risk assessment.</li>
        
        <li>→ <a href="#entity-residual-risk" class="entity-link">residual risk</a>: AI RMF 1.0 includes the concept of residual risk, emphasizing its importance in understanding the impacts on end users.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The AI RMF 1.0 supports the management of AI risks by providing a structured framework for integration into enterprise risk management.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 provides guidelines and standards that inform the implementation of the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The AI RMF 1.0 includes diverse AI actors necessary for effective risk management.</li>
        
        <li>← <a href="#entity-ai-risks-and-trustworthiness" class="entity-link">AI Risks and Trustworthiness</a>: AI Risks and Trustworthiness aligns with the principles outlined in the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a>: AI RMF 1.0 provides guidance for achieving trustworthy AI.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: Includes the AI Risk Management Framework 1.0 as part of its content.</li>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: AI RMF 1.0 supports the development of AI safety risk management approaches.</li>
        
        <li>→ <a href="#entity-fairness-in-ai" class="entity-link">Fairness in AI</a>: AI RMF 1.0 is aligned with the principles of fairness in AI, emphasizing the need to manage bias.</li>
        
        <li>→ <a href="#entity-systemic-bias" class="entity-link">systemic bias</a>: AI RMF 1.0 identifies systemic bias as a major category of AI bias.</li>
        
        <li>→ <a href="#entity-computational-and-statistical-bias" class="entity-link">computational and statistical bias</a>: AI RMF 1.0 identifies computational and statistical bias as a major category of AI bias.</li>
        
        <li>→ <a href="#entity-human-cognitive-bias" class="entity-link">human-cognitive bias</a>: AI RMF 1.0 identifies human-cognitive bias as a major category of AI bias.</li>
        
        <li>← <a href="#entity-nist-special-publication-1270" class="entity-link">NIST Special Publication 1270</a>: NIST Special Publication 1270 aligns with the AI RMF 1.0 framework for managing AI risks.</li>
        
        <li>← <a href="#entity-nist" class="entity-link">NIST</a>: NIST develops the AI RMF 1.0 framework to assist organizations in managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-community" class="entity-link">AI community</a>: The AI RMF 1.0 framework supports the AI community by providing guidelines for risk management.</li>
        
        <li>→ <a href="#entity-tevv-practices" class="entity-link">TEVV practices</a>: The AI RMF 1.0 framework includes TEVV practices for evaluating AI systems.</li>
        
        <li>← <a href="#entity-govern" class="entity-link">GOVERN</a>: The GOVERN function supports the implementation of AI RMF 1.0 by integrating governance into AI risk management practices.</li>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 aligns with the principles and framework of AI RMF 1.0 for effective risk management.</li>
        
        <li>→ <a href="#entity-map-function" class="entity-link">MAP function</a>: The AI RMF 1.0 includes the MAP function, which is essential for establishing context and categorizing AI systems.</li>
        
        <li>→ <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: The AI RMF 1.0 framework is published as part of the NIST AI 100-1 document.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The AI RMF 1.0 framework is applicable to AI actors involved in the development and deployment of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-8-rephrase">Chunk 8</a> • <a href="#chunk-10-rephrase">Chunk 10</a> • <a href="#chunk-11-rephrase">Chunk 11</a> • <a href="#chunk-13-rephrase">Chunk 13</a> • <a href="#chunk-14-rephrase">Chunk 14</a> • <a href="#chunk-16-rephrase">Chunk 16</a> • <a href="#chunk-19-rephrase">Chunk 19</a> • <a href="#chunk-20-rephrase">Chunk 20</a> • <a href="#chunk-23-rephrase">Chunk 23</a> • <a href="#chunk-24-rephrase">Chunk 24</a> • <a href="#chunk-25-rephrase">Chunk 25</a> • <a href="#chunk-30-rephrase">Chunk 30</a> • <a href="#chunk-31-rephrase">Chunk 31</a> • <a href="#chunk-32-rephrase">Chunk 32</a> • <a href="#chunk-36-rephrase">Chunk 36</a> • <a href="#chunk-39-rephrase">Chunk 39</a> • <a href="#chunk-41-rephrase">Chunk 41</a> • <a href="#chunk-42-rephrase">Chunk 42</a> • <a href="#chunk-43-rephrase">Chunk 43</a> • <a href="#chunk-44-rephrase">Chunk 44</a> • <a href="#chunk-58-rephrase">Chunk 58</a> • <a href="#chunk-59-rephrase">Chunk 59</a> • <a href="#chunk-61-rephrase">Chunk 61</a> • <a href="#chunk-62-rephrase">Chunk 62</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-core">
      <h3>AI RMF Core</h3>
      
      <div class="desc">The core components of the AI Risk Management Framework, which include functions such as Govern, Map, Measure, and Manage.</div>
       
      <ul>
        
        <li>→ <a href="#entity-govern" class="entity-link">Govern</a>: The AI RMF Core includes the GOVERN function as one of its components.</li>
        
        <li>→ <a href="#entity-map" class="entity-link">Map</a>: The AI RMF Core includes the MAP function as one of its components.</li>
        
        <li>→ <a href="#entity-measure" class="entity-link">Measure</a>: The AI RMF Core includes the MEASURE function as one of its components.</li>
        
        <li>→ <a href="#entity-manage" class="entity-link">Manage</a>: The AI RMF Core includes the MANAGE function as one of its components.</li>
        
        <li>← <a href="#entity-ai-actor-tasks" class="entity-link">AI Actor Tasks</a>: AI Actor Tasks are included in the discussions of the AI RMF Core.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The AI RMF Core supports the engagement with AI actors for diverse perspectives.</li>
        
        <li>← <a href="#entity-diverse-team" class="entity-link">diverse team</a>: A diverse team contributes to the AI RMF Core by providing varied perspectives.</li>
        
        <li>→ <a href="#entity-downstream-risks" class="entity-link">downstream risks</a>: The AI RMF Core identifies downstream risks associated with AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-core">
      <h3>AI RMF Core</h3>
      
      <div class="desc">The core component of the AI RMF that includes actions and outcomes to manage AI risks and develop trustworthy AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-govern" class="entity-link">Govern</a>: The AI RMF Core includes the GOVERN function as one of its components.</li>
        
        <li>→ <a href="#entity-map" class="entity-link">Map</a>: The AI RMF Core includes the MAP function as one of its components.</li>
        
        <li>→ <a href="#entity-measure" class="entity-link">Measure</a>: The AI RMF Core includes the MEASURE function as one of its components.</li>
        
        <li>→ <a href="#entity-manage" class="entity-link">Manage</a>: The AI RMF Core includes the MANAGE function as one of its components.</li>
        
        <li>← <a href="#entity-ai-actor-tasks" class="entity-link">AI Actor Tasks</a>: AI Actor Tasks are included in the discussions of the AI RMF Core.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The AI RMF Core supports the engagement with AI actors for diverse perspectives.</li>
        
        <li>← <a href="#entity-diverse-team" class="entity-link">diverse team</a>: A diverse team contributes to the AI RMF Core by providing varied perspectives.</li>
        
        <li>→ <a href="#entity-downstream-risks" class="entity-link">downstream risks</a>: The AI RMF Core identifies downstream risks associated with AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-32-rephrase">Chunk 32</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-playbook">
      <h3>AI RMF Playbook</h3>
      
      <div class="desc">A playbook developed by NIST that outlines the framework for managing risks associated with artificial intelligence.</div>
       
      <ul>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: The AI RMF Playbook is published by NIST.</li>
        
        <li>→ <a href="#entity-version-control-table" class="entity-link">Version Control Table</a>: The AI RMF Playbook includes a Version Control Table to track changes.</li>
        
        <li>→ <a href="#entity-foundational-information" class="entity-link">Foundational Information</a>: The AI RMF Playbook is subdivided into sections, including Foundational Information.</li>
        
        <li>→ <a href="#entity-core-and-profiles" class="entity-link">Core and Profiles</a>: The AI RMF Playbook is subdivided into sections, including Core and Profiles.</li>
        
        <li>← <a href="#entity-ai-risks-and-trustworthiness" class="entity-link">AI Risks and Trustworthiness</a>: The category of AI Risks and Trustworthiness supports the overall framework outlined in the AI RMF Playbook.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The AI RMF 1.0 includes additional resources provided in the AI RMF Playbook.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-1-rephrase">Chunk 1</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-playbook">
      <h3>AI RMF Playbook</h3>
      
      <div class="desc">A supplementary resource related to the AI RMF, providing additional guidance and tools for implementing the framework.</div>
       
      <ul>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: The AI RMF Playbook is published by NIST.</li>
        
        <li>→ <a href="#entity-version-control-table" class="entity-link">Version Control Table</a>: The AI RMF Playbook includes a Version Control Table to track changes.</li>
        
        <li>→ <a href="#entity-foundational-information" class="entity-link">Foundational Information</a>: The AI RMF Playbook is subdivided into sections, including Foundational Information.</li>
        
        <li>→ <a href="#entity-core-and-profiles" class="entity-link">Core and Profiles</a>: The AI RMF Playbook is subdivided into sections, including Core and Profiles.</li>
        
        <li>← <a href="#entity-ai-risks-and-trustworthiness" class="entity-link">AI Risks and Trustworthiness</a>: The category of AI Risks and Trustworthiness supports the overall framework outlined in the AI RMF Playbook.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The AI RMF 1.0 includes additional resources provided in the AI RMF Playbook.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-7-rephrase">Chunk 7</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-profiles">
      <h3>AI RMF Profiles</h3>
      
      <div class="desc">AI RMF use-case profiles are implementations of the AI RMF functions, categories, and subcategories for specific settings or applications based on requirements, risk tolerance, and resources.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-temporal-profiles" class="entity-link">AI RMF temporal profiles</a>: AI RMF Profiles include AI RMF temporal profiles as part of their framework.</li>
        
        <li>→ <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: AI RMF Profiles are associated with the NIST AI 100-1 document.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-52-rephrase">Chunk 52</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-rmf-temporal-profiles">
      <h3>AI RMF temporal profiles</h3>
      
      <div class="desc">Descriptions of either the current state or the desired, target state of specific AI risk management activities within a given sector, industry, organization, or application context.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-profiles" class="entity-link">AI RMF Profiles</a>: AI RMF Profiles include AI RMF temporal profiles as part of their framework.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-52-rephrase">Chunk 52</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-safety-risk-management-approaches">
      <h3>AI safety risk management approaches</h3>
      
      <div class="desc">Approaches that focus on managing risks associated with AI systems to ensure safety and reliability.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI RMF 1.0 supports the development of AI safety risk management approaches.</li>
        
        <li>← <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>: ISO/IEC TS 5723:2022 provides guidelines that align with AI safety risk management approaches.</li>
        
        <li>← <a href="#entity-nist-cybersecurity-framework" class="entity-link">NIST Cybersecurity Framework</a>: The NIST Cybersecurity Framework is applicable to AI safety risk management approaches.</li>
        
        <li>← <a href="#entity-nist-risk-management-framework" class="entity-link">NIST Risk Management Framework</a>: The NIST Risk Management Framework is applicable to AI safety risk management approaches.</li>
        
        <li>← <a href="#entity-security-and-resilience" class="entity-link">security and resilience</a>: Security and resilience are included as key considerations in AI safety risk management approaches.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-25-rephrase">Chunk 25</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-system">
      <h3>AI system</h3>
      
      <div class="desc">An AI system is a technological framework that employs artificial intelligence to perform tasks typically requiring human intelligence, ensuring safety, security, and responsible governance.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-metrics" class="entity-link">risk metrics</a>: The AI system&#x27;s risk metrics may not align with those used by organizations deploying the system.</li>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function is applicable to AI systems for ensuring context understanding and facilitating categorization and assessment.</li>
        
        <li>→ <a href="#entity-safety-risks" class="entity-link">safety risks</a>: The AI system is evaluated for safety risks as identified in the MAP function.</li>
        
        <li>→ <a href="#entity-privacy-risk" class="entity-link">privacy risk</a>: The privacy risk of the AI system is examined and documented.</li>
        
        <li>→ <a href="#entity-fairness-and-bias" class="entity-link">fairness and bias</a>: Fairness and bias associated with the AI system are evaluated and results are documented.</li>
        
        <li>→ <a href="#entity-environmental-impact" class="entity-link">environmental impact</a>: The environmental impact of AI model training and management activities is assessed and documented.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-11-rephrase">Chunk 11</a> • <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-41-rephrase">Chunk 41</a> • <a href="#chunk-42-rephrase">Chunk 42</a> • <a href="#chunk-47-rephrase">Chunk 47</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-system-categorization">
      <h3>AI system categorization</h3>
      
      <div class="desc">The classification of AI systems based on their capabilities and contexts.</div>
       
      <ul>
        
        <li>→ <a href="#entity-organizational-risk-tolerance" class="entity-link">organizational risk tolerance</a>: The categorization of AI systems is aligned with the organization&#x27;s risk tolerance.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-43-rephrase">Chunk 43</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-system-lifecycle">
      <h3>AI system lifecycle</h3>
      
      <div class="desc">The stages through which an AI system progresses, from conception to deployment and beyond.</div>
       
      <ul>
        
        <li>← <a href="#entity-plan-and-design-function" class="entity-link">Plan and Design function</a>: The Plan and Design function supports the various stages of the AI system lifecycle.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-19-rephrase">Chunk 19</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-systems">
      <h3>AI systems</h3>
      
      <div class="desc">AI systems are technological solutions that utilize artificial intelligence to perform tasks and make decisions, often requiring varying levels of human oversight.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF describes the nature and functionality of AI systems.</li>
        
        <li>→ <a href="#entity-societal-dynamics" class="entity-link">societal dynamics</a>: AI systems are influenced by societal dynamics and human behavior.</li>
        
        <li>← <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: AI risk management supports the responsible development and use of AI systems.</li>
        
        <li>→ <a href="#entity-inequitable-outcomes" class="entity-link">inequitable outcomes</a>: AI systems can create opportunities for inequitable outcomes if not properly managed.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports the management of risks associated with AI systems to enhance their trustworthiness.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework supports the responsible management of risks associated with AI systems through structured guidelines.</li>
        
        <li>→ <a href="#entity-risk-measurement-challenges" class="entity-link">risk measurement challenges</a>: The complexities of AI systems illustrate the challenges in defining and measuring associated risks.</li>
        
        <li>→ <a href="#entity-organizations" class="entity-link">organizations</a>: AI systems are applicable to various organizations that may have different risk tolerances.</li>
        
        <li>→ <a href="#entity-risk-management-culture" class="entity-link">risk management culture</a>: AI systems are applicable to the risk management culture as they require specific risk assessments and management strategies.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors manage the risks associated with AI systems throughout their lifecycle, ensuring responsibilities are clearly defined.</li>
        
        <li>← <a href="#entity-end-users" class="entity-link">end users</a>: End users are affected by the risks associated with AI systems, particularly in terms of potential negative impacts.</li>
        
        <li>← <a href="#entity-accuracy" class="entity-link">accuracy</a>: Contributes to the validity and trustworthiness of AI systems.</li>
        
        <li>← <a href="#entity-robustness" class="entity-link">robustness</a>: Contributes to appropriate system functionality in a broad set of conditions.</li>
        
        <li>→ <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>: AI systems should comply with the safety requirements outlined in ISO/IEC TS 5723:2022 to avoid endangering human life and health.</li>
        
        <li>→ <a href="#entity-transparency" class="entity-link">transparency</a>: AI systems benefit from transparency to enhance accountability and trustworthiness, particularly in relation to bias.</li>
        
        <li>→ <a href="#entity-accountability" class="entity-link">accountability</a>: AI systems require accountability measures to ensure ethical and fair operation.</li>
        
        <li>← <a href="#entity-explainability" class="entity-link">explainability</a>: Explainability contributes to a better understanding of AI systems&#x27; operations and outputs.</li>
        
        <li>← <a href="#entity-interpretability" class="entity-link">interpretability</a>: Interpretability helps users understand the meaning of AI systems&#x27; outputs in their functional context.</li>
        
        <li>→ <a href="#entity-fairness" class="entity-link">fairness</a>: AI systems are closely associated with the concept of fairness in relation to bias.</li>
        
        <li>← <a href="#entity-govern" class="entity-link">GOVERN</a>: The GOVERN function is applicable to AI systems, ensuring that risk management practices are implemented throughout their lifecycle.</li>
        
        <li>← <a href="#entity-impacts-to-individuals,-groups,-communities,-organizations,-and-society" class="entity-link">impacts to individuals, groups, communities, organizations, and society</a>: The impacts of AI systems are evaluated to understand their effects on various stakeholders.</li>
        
        <li>→ <a href="#entity-trustworthy-characteristics" class="entity-link">trustworthy characteristics</a>: AI systems are evaluated for trustworthy characteristics.</li>
        
        <li>→ <a href="#entity-safety-risks" class="entity-link">safety risks</a>: AI systems are evaluated regularly for safety risks.</li>
        
        <li>→ <a href="#entity-negative-residual-risks" class="entity-link">negative residual risks</a>: AI systems can contribute to negative residual risks for downstream acquirers and end users.</li>
        
        <li>← <a href="#entity-test,-evaluation,-verification,-and-validation-(tevv)" class="entity-link">Test, Evaluation, Verification, and Validation (TEVV)</a>: The TEVV framework applies to AI systems to ensure their reliability and compliance.</li>
        
        <li>← <a href="#entity-compliance-experts" class="entity-link">compliance experts</a>: Compliance experts evaluate AI systems to ensure they meet legal and ethical standards.</li>
        
        <li>← <a href="#entity-organizational-management" class="entity-link">organizational management</a>: Organizational management oversees the implementation and operation of AI systems within organizations.</li>
        
        <li>← <a href="#entity-affected-individuals/communities" class="entity-link">Affected individuals/communities</a>: Affected individuals and communities are impacted by the decisions made based on AI systems.</li>
        
        <li>← <a href="#entity-datasets" class="entity-link">datasets</a>: Datasets are used to train AI systems, but may become outdated or detached from their original context.</li>
        
        <li>← <a href="#entity-privacy-and-cybersecurity-risk-management" class="entity-link">Privacy and cybersecurity risk management</a>: Privacy and cybersecurity risk management considerations are applicable to the design and use of AI systems.</li>
        
        <li>← <a href="#entity-enterprise-risk-management" class="entity-link">enterprise risk management</a>: Enterprise risk management includes considerations for managing risks associated with AI systems.</li>
        
        <li>→ <a href="#entity-harmful-bias" class="entity-link">harmful bias</a>: AI systems can exhibit harmful bias, which is a significant risk that needs to be managed.</li>
        
        <li>→ <a href="#entity-generative-ai" class="entity-link">generative AI</a>: Generative AI is a subset of AI systems that poses unique risks.</li>
        
        <li>→ <a href="#entity-machine-learning-attacks" class="entity-link">machine learning attacks</a>: AI systems are vulnerable to various machine learning attacks that can compromise security.</li>
        
        <li>→ <a href="#entity-third-party-ai-technologies" class="entity-link">third-party AI technologies</a>: AI systems may face risks from third-party AI technologies that are outside an organization&#x27;s control.</li>
        
        <li>← <a href="#entity-third-party-ai-technologies" class="entity-link">third-party AI technologies</a>: Third-party AI technologies may introduce risks when integrated into an organization&#x27;s AI systems.</li>
        
        <li>← <a href="#entity-transfer-learning" class="entity-link">transfer learning</a>: Transfer learning is a methodology that can be applied to enhance the capabilities of AI systems.</li>
        
        <li>← <a href="#entity-human-roles-and-responsibilities" class="entity-link">human roles and responsibilities</a>: Human roles and responsibilities are evaluated to ensure effective oversight of AI systems.</li>
        
        <li>← <a href="#entity-video-compression-models" class="entity-link">video compression models</a>: Video compression models function as a specific application of AI systems that may not require human oversight.</li>
        
        <li>→ <a href="#entity-human-decision-maker" class="entity-link">human decision maker</a>: AI systems can support human decision makers by providing additional opinions or insights.</li>
        
        <li>← <a href="#entity-cognitive-biases" class="entity-link">cognitive biases</a>: Cognitive biases can influence the design and decision-making processes within AI systems.</li>
        
        <li>← <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The GOVERN function provides clarity on roles and responsibilities for humans overseeing AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-4-rephrase">Chunk 4</a> • <a href="#chunk-23-rephrase">Chunk 23</a> • <a href="#chunk-24-rephrase">Chunk 24</a> • <a href="#chunk-27-rephrase">Chunk 27</a> • <a href="#chunk-55-rephrase">Chunk 55</a> • <a href="#chunk-60-rephrase">Chunk 60</a> • <a href="#chunk-62-rephrase">Chunk 62</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-systems">
      <h3>AI systems</h3>
      
      <div class="desc">AI systems are technological solutions that utilize artificial intelligence to perform tasks typically requiring human intelligence, impacting individuals and society in various ways.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF describes the nature and functionality of AI systems.</li>
        
        <li>→ <a href="#entity-societal-dynamics" class="entity-link">societal dynamics</a>: AI systems are influenced by societal dynamics and human behavior.</li>
        
        <li>← <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: AI risk management supports the responsible development and use of AI systems.</li>
        
        <li>→ <a href="#entity-inequitable-outcomes" class="entity-link">inequitable outcomes</a>: AI systems can create opportunities for inequitable outcomes if not properly managed.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports the management of risks associated with AI systems to enhance their trustworthiness.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework supports the responsible management of risks associated with AI systems through structured guidelines.</li>
        
        <li>→ <a href="#entity-risk-measurement-challenges" class="entity-link">risk measurement challenges</a>: The complexities of AI systems illustrate the challenges in defining and measuring associated risks.</li>
        
        <li>→ <a href="#entity-organizations" class="entity-link">organizations</a>: AI systems are applicable to various organizations that may have different risk tolerances.</li>
        
        <li>→ <a href="#entity-risk-management-culture" class="entity-link">risk management culture</a>: AI systems are applicable to the risk management culture as they require specific risk assessments and management strategies.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors manage the risks associated with AI systems throughout their lifecycle, ensuring responsibilities are clearly defined.</li>
        
        <li>← <a href="#entity-end-users" class="entity-link">end users</a>: End users are affected by the risks associated with AI systems, particularly in terms of potential negative impacts.</li>
        
        <li>← <a href="#entity-accuracy" class="entity-link">accuracy</a>: Contributes to the validity and trustworthiness of AI systems.</li>
        
        <li>← <a href="#entity-robustness" class="entity-link">robustness</a>: Contributes to appropriate system functionality in a broad set of conditions.</li>
        
        <li>→ <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>: AI systems should comply with the safety requirements outlined in ISO/IEC TS 5723:2022 to avoid endangering human life and health.</li>
        
        <li>→ <a href="#entity-transparency" class="entity-link">transparency</a>: AI systems benefit from transparency to enhance accountability and trustworthiness, particularly in relation to bias.</li>
        
        <li>→ <a href="#entity-accountability" class="entity-link">accountability</a>: AI systems require accountability measures to ensure ethical and fair operation.</li>
        
        <li>← <a href="#entity-explainability" class="entity-link">explainability</a>: Explainability contributes to a better understanding of AI systems&#x27; operations and outputs.</li>
        
        <li>← <a href="#entity-interpretability" class="entity-link">interpretability</a>: Interpretability helps users understand the meaning of AI systems&#x27; outputs in their functional context.</li>
        
        <li>→ <a href="#entity-fairness" class="entity-link">fairness</a>: AI systems are closely associated with the concept of fairness in relation to bias.</li>
        
        <li>← <a href="#entity-govern" class="entity-link">GOVERN</a>: The GOVERN function is applicable to AI systems, ensuring that risk management practices are implemented throughout their lifecycle.</li>
        
        <li>← <a href="#entity-impacts-to-individuals,-groups,-communities,-organizations,-and-society" class="entity-link">impacts to individuals, groups, communities, organizations, and society</a>: The impacts of AI systems are evaluated to understand their effects on various stakeholders.</li>
        
        <li>→ <a href="#entity-trustworthy-characteristics" class="entity-link">trustworthy characteristics</a>: AI systems are evaluated for trustworthy characteristics.</li>
        
        <li>→ <a href="#entity-safety-risks" class="entity-link">safety risks</a>: AI systems are evaluated regularly for safety risks.</li>
        
        <li>→ <a href="#entity-negative-residual-risks" class="entity-link">negative residual risks</a>: AI systems can contribute to negative residual risks for downstream acquirers and end users.</li>
        
        <li>← <a href="#entity-test,-evaluation,-verification,-and-validation-(tevv)" class="entity-link">Test, Evaluation, Verification, and Validation (TEVV)</a>: The TEVV framework applies to AI systems to ensure their reliability and compliance.</li>
        
        <li>← <a href="#entity-compliance-experts" class="entity-link">compliance experts</a>: Compliance experts evaluate AI systems to ensure they meet legal and ethical standards.</li>
        
        <li>← <a href="#entity-organizational-management" class="entity-link">organizational management</a>: Organizational management oversees the implementation and operation of AI systems within organizations.</li>
        
        <li>← <a href="#entity-affected-individuals/communities" class="entity-link">Affected individuals/communities</a>: Affected individuals and communities are impacted by the decisions made based on AI systems.</li>
        
        <li>← <a href="#entity-datasets" class="entity-link">datasets</a>: Datasets are used to train AI systems, but may become outdated or detached from their original context.</li>
        
        <li>← <a href="#entity-privacy-and-cybersecurity-risk-management" class="entity-link">Privacy and cybersecurity risk management</a>: Privacy and cybersecurity risk management considerations are applicable to the design and use of AI systems.</li>
        
        <li>← <a href="#entity-enterprise-risk-management" class="entity-link">enterprise risk management</a>: Enterprise risk management includes considerations for managing risks associated with AI systems.</li>
        
        <li>→ <a href="#entity-harmful-bias" class="entity-link">harmful bias</a>: AI systems can exhibit harmful bias, which is a significant risk that needs to be managed.</li>
        
        <li>→ <a href="#entity-generative-ai" class="entity-link">generative AI</a>: Generative AI is a subset of AI systems that poses unique risks.</li>
        
        <li>→ <a href="#entity-machine-learning-attacks" class="entity-link">machine learning attacks</a>: AI systems are vulnerable to various machine learning attacks that can compromise security.</li>
        
        <li>→ <a href="#entity-third-party-ai-technologies" class="entity-link">third-party AI technologies</a>: AI systems may face risks from third-party AI technologies that are outside an organization&#x27;s control.</li>
        
        <li>← <a href="#entity-third-party-ai-technologies" class="entity-link">third-party AI technologies</a>: Third-party AI technologies may introduce risks when integrated into an organization&#x27;s AI systems.</li>
        
        <li>← <a href="#entity-transfer-learning" class="entity-link">transfer learning</a>: Transfer learning is a methodology that can be applied to enhance the capabilities of AI systems.</li>
        
        <li>← <a href="#entity-human-roles-and-responsibilities" class="entity-link">human roles and responsibilities</a>: Human roles and responsibilities are evaluated to ensure effective oversight of AI systems.</li>
        
        <li>← <a href="#entity-video-compression-models" class="entity-link">video compression models</a>: Video compression models function as a specific application of AI systems that may not require human oversight.</li>
        
        <li>→ <a href="#entity-human-decision-maker" class="entity-link">human decision maker</a>: AI systems can support human decision makers by providing additional opinions or insights.</li>
        
        <li>← <a href="#entity-cognitive-biases" class="entity-link">cognitive biases</a>: Cognitive biases can influence the design and decision-making processes within AI systems.</li>
        
        <li>← <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The GOVERN function provides clarity on roles and responsibilities for humans overseeing AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-9-rephrase">Chunk 9</a> • <a href="#chunk-10-rephrase">Chunk 10</a> • <a href="#chunk-13-rephrase">Chunk 13</a> • <a href="#chunk-14-rephrase">Chunk 14</a> • <a href="#chunk-15-rephrase">Chunk 15</a> • <a href="#chunk-30-rephrase">Chunk 30</a> • <a href="#chunk-34-rephrase">Chunk 34</a> • <a href="#chunk-46-rephrase">Chunk 46</a> • <a href="#chunk-50-rephrase">Chunk 50</a> • <a href="#chunk-59-rephrase">Chunk 59</a> • <a href="#chunk-61-rephrase">Chunk 61</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-technologies">
      <h3>AI technologies</h3>
      
      <div class="desc">Technologies that have the potential to transform various sectors of society, including commerce, health, transportation, and cybersecurity.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: NIST AI RMF 1.0 provides a framework for managing risks related to AI technologies.</li>
        
        <li>→ <a href="#entity-society" class="entity-link">society</a>: AI technologies have the potential to transform various aspects of society.</li>
        
        <li>← <a href="#entity-general-public" class="entity-link">General public</a>: The general public experiences both positive and negative impacts of AI technologies.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-3-rephrase">Chunk 3</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-technology">
      <h3>AI technology</h3>
      
      <div class="desc">AI technology includes tools and systems that utilize artificial intelligence to perform tasks typically requiring human intelligence.</div>
       
      <ul>
        
        <li>→ <a href="#entity-negative-ai-risks" class="entity-link">negative AI risks</a>: The deployment of AI technology can create opportunities for negative AI risks if not managed properly.</li>
        
        <li>→ <a href="#entity-third-party-software-and-data" class="entity-link">third-party software and data</a>: AI technology includes the use of third-party software and data.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF is applicable to any AI technology and context-specific use cases.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-22-rephrase">Chunk 22</a> • <a href="#chunk-43-rephrase">Chunk 43</a> • <a href="#chunk-65-rephrase">Chunk 65</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-trustworthiness">
      <h3>AI trustworthiness</h3>
      
      <div class="desc">A social concept that encompasses various characteristics of AI systems, emphasizing that trustworthiness is influenced by tradeoffs among these characteristics.</div>
       
      <ul>
        
        <li>← <a href="#entity-organizations" class="entity-link">organizations</a>: Organizations manage the characteristics of AI trustworthiness while facing tradeoffs.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-trustworthiness-characteristics">
      <h3>AI trustworthiness characteristics</h3>
      
      <div class="desc">Characteristics that define the trustworthiness of AI systems, including accountability and transparency.</div>
       
      <ul>
        
        <li>← <a href="#entity-trustworthy-ai" class="entity-link">trustworthy AI</a>: Trustworthy AI encompasses various characteristics that define its trustworthiness.</li>
        
        <li>→ <a href="#entity-social-and-organizational-behavior" class="entity-link">social and organizational behavior</a>: AI trustworthiness characteristics are influenced by social and organizational behavior.</li>
        
        <li>→ <a href="#entity-datasets" class="entity-link">datasets</a>: Datasets used by AI systems are relevant to the evaluation of AI trustworthiness characteristics.</li>
        
        <li>→ <a href="#entity-ai-models-and-algorithms" class="entity-link">AI models and algorithms</a>: The selection of AI models and algorithms affects the trustworthiness characteristics of AI systems.</li>
        
        <li>← <a href="#entity-human-judgment" class="entity-link">human judgment</a>: Human judgment is essential in determining the metrics related to AI trustworthiness characteristics.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-20-rephrase">Chunk 20</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-ai-based-technology">
      <h3>AI-based technology</h3>
      
      <div class="desc">Technologies that utilize artificial intelligence to perform tasks that typically require human intelligence.</div>
       
      <ul>
        
        <li>→ <a href="#entity-traditional-software" class="entity-link">traditional software</a>: AI-based technology presents new risks compared to traditional software.</li>
        
        <li>← <a href="#entity-data-quality-issues" class="entity-link">data quality issues</a>: Data quality issues can negatively impact the trustworthiness of AI-based technology.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-58-rephrase">Chunk 58</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-artificial-intelligence-risk-management-framework-(ai-rmf-1.0)">
      <h3>Artificial Intelligence Risk Management Framework (AI RMF 1.0)</h3>
      
      <div class="desc">A framework intended to manage risks associated with artificial intelligence, designed to be a living document that will be regularly updated.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 includes the Artificial Intelligence Risk Management Framework (AI RMF 1.0).</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-0-rephrase">Chunk 0</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-bias">
      <h3>Bias</h3>
      
      <div class="desc">A broader concept than demographic balance and data representativeness, which can affect fairness in AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-fairness-in-ai" class="entity-link">Fairness in AI</a>: Fairness in AI includes the management of harmful bias and discrimination.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-29-rephrase">Chunk 29</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-civil-society">
      <h3>civil society</h3>
      
      <div class="desc">A sector that contributes to the development and debate of methods for informing harm/cost-benefit tradeoffs in AI.</div>
       
      <ul>
        
        <li>→ <a href="#entity-methods" class="entity-link">methods</a>: Civil society contributes to the development of methods for harm/cost-benefit tradeoffs in AI.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-13-rephrase">Chunk 13</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-civil-society-organizations">
      <h3>civil society organizations</h3>
      
      <div class="desc">Organizations that represent the interests of the public and can contribute to AI risk management.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: Civil society organizations contribute to AI risk management by offering norms and guidance.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-19-rephrase">Chunk 19</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-cognitive-biases">
      <h3>cognitive biases</h3>
      
      <div class="desc">Cognitive biases are systematic patterns of deviation from norm or rationality in judgment, which can affect decision-making processes in AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Cognitive biases can influence the design and decision-making processes within AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-62-rephrase">Chunk 62</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-compliance-experts">
      <h3>compliance experts</h3>
      
      <div class="desc">Compliance experts are professionals who ensure AI systems adhere to legal, regulatory, and ethical standards during their operation and monitoring.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Compliance experts evaluate AI systems to ensure they meet legal and ethical standards.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-54-rephrase">Chunk 54</a> • <a href="#chunk-55-rephrase">Chunk 55</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-computational-and-statistical-bias">
      <h3>computational and statistical bias</h3>
      
      <div class="desc">Biases that arise from systematic errors in AI datasets and algorithmic processes, often due to non-representative samples.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI RMF 1.0 identifies computational and statistical bias as a major category of AI bias.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-30-rephrase">Chunk 30</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-contextually-sensitive-evaluations">
      <h3>contextually sensitive evaluations</h3>
      
      <div class="desc">Evaluations that take into account the specific context in which an AI system operates, aiming to identify benefits and risks.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: Enhancing contextual awareness in the AI lifecycle supports contextually sensitive evaluations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-core-and-profiles">
      <h3>Core and Profiles</h3>
      
      <div class="desc">A section in the AI RMF Playbook that outlines the core components and profiles related to AI risk management.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>: The AI RMF Playbook is subdivided into sections, including Core and Profiles.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-1-rephrase">Chunk 1</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-current-profile">
      <h3>Current Profile</h3>
      
      <div class="desc">Indicates how AI is currently being managed and the related risks in terms of current outcomes.</div>
       
      <ul>
        
        <li>→ <a href="#entity-target-profile" class="entity-link">Target Profile</a>: Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-52-rephrase">Chunk 52</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-cybersecurity">
      <h3>cybersecurity</h3>
      
      <div class="desc">Cybersecurity encompasses the practices and technologies used to protect systems, networks, and data from cyber threats.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: AI risks are aligned with cybersecurity risks, as both involve critical concerns regarding data protection and system integrity.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-16-rephrase">Chunk 16</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-data-quality-issues">
      <h3>data quality issues</h3>
      
      <div class="desc">Problems related to the accuracy, completeness, and reliability of data used in AI systems, which can affect their trustworthiness.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-based-technology" class="entity-link">AI-based technology</a>: Data quality issues can negatively impact the trustworthiness of AI-based technology.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-58-rephrase">Chunk 58</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-data-sparsity">
      <h3>data sparsity</h3>
      
      <div class="desc">A situation where there is insufficient data, which can lead to tradeoffs in AI system performance.</div>
       
      <ul>
        
        <li>→ <a href="#entity-predictive-accuracy" class="entity-link">predictive accuracy</a>: Data sparsity can negatively affect predictive accuracy when using privacy-enhancing techniques.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-datasets">
      <h3>datasets</h3>
      
      <div class="desc">Datasets are collections of data used to train AI systems, impacting their trustworthiness and relevance.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-trustworthiness-characteristics" class="entity-link">AI trustworthiness characteristics</a>: Datasets used by AI systems are relevant to the evaluation of AI trustworthiness characteristics.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Datasets are used to train AI systems, but may become outdated or detached from their original context.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-20-rephrase">Chunk 20</a> • <a href="#chunk-58-rephrase">Chunk 58</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-diverse-team">
      <h3>diverse team</h3>
      
      <div class="desc">A team composed of individuals with varied backgrounds and perspectives contributing to AI development.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: A diverse team contributes to the AI RMF Core by providing varied perspectives.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-32-rephrase">Chunk 32</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-domain-expert">
      <h3>Domain Expert</h3>
      
      <div class="desc">Practitioners or scholars who provide knowledge or expertise in a specific industry or application area where an AI system is used.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-impact-assessment" class="entity-link">AI Impact Assessment</a>: Domain experts contribute essential guidance for AI impact assessment tasks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-56-rephrase">Chunk 56</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-domain-experts">
      <h3>domain experts</h3>
      
      <div class="desc">Individuals with specialized knowledge in a particular area relevant to AI risk assessment.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: Domain experts provide insights that inform the identification and tracking of AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-48-rephrase">Chunk 48</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-downstream-risks">
      <h3>downstream risks</h3>
      
      <div class="desc">Risks that arise as a consequence of AI system deployment and operation.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core identifies downstream risks associated with AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-32-rephrase">Chunk 32</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-emergent-risks">
      <h3>emergent risks</h3>
      
      <div class="desc">Newly arising risks that may not have been previously identified or considered in risk management efforts.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-management-efforts" class="entity-link">risk management efforts</a>: Identifying and tracking emergent risks enhances organizations&#x27; risk management efforts.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-11-rephrase">Chunk 11</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-end-users">
      <h3>end users</h3>
      
      <div class="desc">End users are individuals or communities that interact with AI systems, providing insights and feedback on their impacts and associated risks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: End users are affected by the risks associated with AI systems, particularly in terms of potential negative impacts.</li>
        
        <li>→ <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: End users contribute to AI risk management by sharing their experiences and insights.</li>
        
        <li>← <a href="#entity-internal-team" class="entity-link">internal team</a>: The internal team interacts with end users to understand their needs and impacts of AI systems.</li>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: End users provide feedback on AI risks based on their interactions with the systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-15-rephrase">Chunk 15</a> • <a href="#chunk-19-rephrase">Chunk 19</a> • <a href="#chunk-39-rephrase">Chunk 39</a> • <a href="#chunk-48-rephrase">Chunk 48</a> • <a href="#chunk-54-rephrase">Chunk 54</a> • <a href="#chunk-57-rephrase">Chunk 57</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-enterprise-risk-management">
      <h3>enterprise risk management</h3>
      
      <div class="desc">A comprehensive approach to identifying, assessing, and managing risks across an organization, including those related to AI.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Enterprise risk management includes considerations for managing risks associated with AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-59-rephrase">Chunk 59</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-environmental-groups">
      <h3>environmental groups</h3>
      
      <div class="desc">Organizations focused on environmental issues that can provide context and understanding of AI impacts.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: Environmental groups contribute to AI risk management by providing context and understanding of potential impacts.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-19-rephrase">Chunk 19</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-environmental-impact">
      <h3>environmental impact</h3>
      
      <div class="desc">The effects of AI model training and management activities on the environment, which are assessed and documented.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-system" class="entity-link">AI system</a>: The environmental impact of AI model training and management activities is assessed and documented.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-47-rephrase">Chunk 47</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-executive-leadership">
      <h3>executive leadership</h3>
      
      <div class="desc">The individuals in leadership positions within the organization responsible for decision-making regarding AI risks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risk-management-training" class="entity-link">AI risk management training</a>: Executive leadership manages the implementation of AI risk management training within the organization.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-36-rephrase">Chunk 36</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-explainability">
      <h3>explainability</h3>
      
      <div class="desc">Explainability refers to the representation of the mechanisms underlying the operation of AI systems, helping users understand how decisions are made.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Explainability contributes to a better understanding of AI systems&#x27; operations and outputs.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-27-rephrase">Chunk 27</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-explainable-ai">
      <h3>Explainable AI</h3>
      
      <div class="desc">A framework that focuses on making AI systems understandable to users by providing insights into their functionality and trustworthiness.</div>
       
      <ul>
        
        <li>→ <a href="#entity-interpretability" class="entity-link">Interpretability</a>: Explainable AI frameworks support the development of interpretability in AI systems.</li>
        
        <li>→ <a href="#entity-transparency" class="entity-link">Transparency</a>: Explainable AI frameworks promote transparency in AI systems by clarifying decision-making processes.</li>
        
        <li>← <a href="#entity-privacy" class="entity-link">Privacy</a>: Privacy norms align with the principles of explainable AI by emphasizing user autonomy and control over personal data.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-28-rephrase">Chunk 28</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-external-collaborators">
      <h3>external collaborators</h3>
      
      <div class="desc">Individuals or groups outside the internal team that engage with the team to provide perspectives on AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-internal-team" class="entity-link">internal team</a>: The internal team engages with external collaborators to incorporate diverse perspectives in AI development.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-39-rephrase">Chunk 39</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-fairness">
      <h3>fairness</h3>
      
      <div class="desc">A principle that aims to ensure equitable treatment and outcomes in the context of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are closely associated with the concept of fairness in relation to bias.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-30-rephrase">Chunk 30</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-fairness-and-bias">
      <h3>fairness and bias</h3>
      
      <div class="desc">Concerns regarding the equitable treatment of individuals by AI systems, which are evaluated and documented.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-system" class="entity-link">AI system</a>: Fairness and bias associated with the AI system are evaluated and results are documented.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-47-rephrase">Chunk 47</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-fairness-in-ai">
      <h3>Fairness in AI</h3>
      
      <div class="desc">A concept that includes concerns for equality and equity, addressing issues such as harmful bias and discrimination in AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-bias" class="entity-link">Bias</a>: Fairness in AI includes the management of harmful bias and discrimination.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI RMF 1.0 is aligned with the principles of fairness in AI, emphasizing the need to manage bias.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-29-rephrase">Chunk 29</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-foundational-information">
      <h3>Foundational Information</h3>
      
      <div class="desc">A section in the AI RMF Playbook that provides essential background information on AI risk management.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>: The AI RMF Playbook is subdivided into sections, including Foundational Information.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-1-rephrase">Chunk 1</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-framework-users">
      <h3>Framework users</h3>
      
      <div class="desc">Framework users are individuals or organizations that utilize the NIST AI RMF to assess and manage risks associated with AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-map-function" class="entity-link">MAP function</a>: Framework users utilize the MAP function to inform decisions about AI system design and deployment.</li>
        
        <li>→ <a href="#entity-manage" class="entity-link">MANAGE</a>: Framework users apply the MANAGE function to effectively manage AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-49-rephrase">Chunk 49</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-general-public">
      <h3>General public</h3>
      
      <div class="desc">Individuals, communities, and consumers who directly experience the impacts of AI technologies and may motivate actions taken by AI actors.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-technologies" class="entity-link">AI technologies</a>: The general public experiences both positive and negative impacts of AI technologies.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-57-rephrase">Chunk 57</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-generative-ai">
      <h3>generative AI</h3>
      
      <div class="desc">A type of AI that can generate new content or data based on learned patterns.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Generative AI is a subset of AI systems that poses unique risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-gina-m.-raimondo">
      <h3>Gina M. Raimondo</h3>
      
      <div class="desc">The Secretary of the U.S. Department of Commerce.</div>
       
      <ul>
        
        <li>→ <a href="#entity-u.s.-department-of-commerce" class="entity-link">U.S. Department of Commerce</a>: Gina M. Raimondo is the Secretary directing the U.S. Department of Commerce.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-0-rephrase">Chunk 0</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern">
      <h3>Govern</h3>
      
      <div class="desc">The GOVERN function of the AI RMF focuses on establishing governance structures for AI systems and cultivating a culture of risk management within organizations.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core includes the GOVERN function as one of its components.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a> • <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-32-rephrase">Chunk 32</a> • <a href="#chunk-33-rephrase">Chunk 33</a> • <a href="#chunk-34-rephrase">Chunk 34</a> • <a href="#chunk-49-rephrase">Chunk 49</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern-5">
      <h3>GOVERN 5</h3>
      
      <div class="desc">A guideline that emphasizes the importance of robust engagement with relevant AI actors.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: GOVERN 5 supports robust engagement with relevant AI actors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern-5.1">
      <h3>GOVERN 5.1</h3>
      
      <div class="desc">A guideline detailing the need for organizational policies to collect and integrate feedback from external sources regarding AI risks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: GOVERN 5.1 addresses AI risks by integrating feedback from external sources.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern-5.2">
      <h3>GOVERN 5.2</h3>
      
      <div class="desc">A guideline that establishes mechanisms for incorporating feedback from relevant AI actors into system design.</div>
       
      <ul>
        
        <li>→ <a href="#entity-system-design" class="entity-link">system design</a>: GOVERN 5.2 contributes to system design by incorporating feedback from relevant AI actors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern-6">
      <h3>GOVERN 6</h3>
      
      <div class="desc">A guideline that addresses AI risks and benefits arising from third-party software and data.</div>
       
      <ul>
        
        <li>→ <a href="#entity-third-party-software" class="entity-link">third-party software</a>: GOVERN 6 addresses AI risks and benefits arising from third-party software.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern-6.1">
      <h3>GOVERN 6.1</h3>
      
      <div class="desc">A guideline focused on policies addressing AI risks associated with third-party entities.</div>
       
      <ul>
        
        <li>→ <a href="#entity-third-party-entities" class="entity-link">third-party entities</a>: GOVERN 6.1 addresses risks associated with third-party entities.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern-6.2">
      <h3>GOVERN 6.2</h3>
      
      <div class="desc">A guideline that outlines contingency processes for handling failures in high-risk third-party data or AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-high-risk-systems" class="entity-link">high-risk systems</a>: GOVERN 6.2 provides contingency processes for handling failures in high-risk systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-govern-function">
      <h3>GOVERN function</h3>
      
      <div class="desc">The GOVERN function provides a framework for organizations to establish policies and processes for managing AI risks, clarifying roles and responsibilities within Human-AI team configurations.</div>
       
      <ul>
        
        <li>→ <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>: The GOVERN function is supported by the practices described in the NIST AI RMF Playbook.</li>
        
        <li>← <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>: The risk tolerance of an organization is applicable to the activities outlined in the GOVERN function.</li>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: AI risks are managed through the processes and practices established in the GOVERN function.</li>
        
        <li>→ <a href="#entity-organizational-risk-priorities" class="entity-link">organizational risk priorities</a>: The GOVERN function supports the establishment of organizational risk priorities.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The GOVERN function manages AI risks through the implementation of policies and procedures.</li>
        
        <li>→ <a href="#entity-human-oversight-processes" class="entity-link">human oversight processes</a>: Human oversight processes operate under the governance established by the GOVERN function.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The GOVERN function provides clarity on roles and responsibilities for humans overseeing AI systems.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The GOVERN function supports the AI RMF by clarifying roles and responsibilities in AI team configurations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-35-rephrase">Chunk 35</a> • <a href="#chunk-36-rephrase">Chunk 36</a> • <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-43-rephrase">Chunk 43</a> • <a href="#chunk-62-rephrase">Chunk 62</a> • <a href="#chunk-63-rephrase">Chunk 63</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-governance-and-oversight-tasks">
      <h3>Governance and Oversight tasks</h3>
      
      <div class="desc">Tasks assumed by AI actors with management and legal authority responsible for the organization in which an AI system is designed and deployed.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: Governance and oversight tasks operate under the authority of AI actors responsible for organizational management.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-56-rephrase">Chunk 56</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-governing-authorities">
      <h3>governing authorities</h3>
      
      <div class="desc">Governing authorities are entities that establish policies and frameworks to guide an organization&#39;s mission, goals, and risk tolerance.</div>
       
      <ul>
        
        <li>→ <a href="#entity-organizational-culture" class="entity-link">organizational culture</a>: Governing authorities direct the organizational culture by establishing overarching policies and values.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-34-rephrase">Chunk 34</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-harmful-bias">
      <h3>harmful bias</h3>
      
      <div class="desc">A risk associated with AI systems that leads to unfair or discriminatory outcomes.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems can exhibit harmful bias, which is a significant risk that needs to be managed.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-baseline">
      <h3>human baseline</h3>
      
      <div class="desc">Human baseline refers to the metrics used for comparison when assessing AI systems that augment or replace human activities, particularly in decision-making.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-management" class="entity-link">risk management</a>: The human baseline provides a framework for comparing AI systems against human performance in decision-making tasks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-12-rephrase">Chunk 12</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-decision-maker">
      <h3>human decision maker</h3>
      
      <div class="desc">A human decision maker is an individual who makes decisions, potentially using AI systems as a supplementary opinion.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems can support human decision makers by providing additional opinions or insights.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-62-rephrase">Chunk 62</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-factors">
      <h3>Human Factors</h3>
      
      <div class="desc">Tasks and activities that focus on human-centered design and the integration of human dynamics in the AI lifecycle.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: Human Factors are integrated throughout the AI lifecycle to enhance user experience and system design.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-55-rephrase">Chunk 55</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-factors-professionals">
      <h3>Human factors professionals</h3>
      
      <div class="desc">Professionals who provide multidisciplinary skills and perspectives to understand the context of use and engage in consultative processes related to AI system design.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: Human factors professionals provide support to AI actors by informing user experience design and evaluation.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-56-rephrase">Chunk 56</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-judgment">
      <h3>human judgment</h3>
      
      <div class="desc">The decision-making process employed by humans to determine metrics related to AI trustworthiness.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-trustworthiness-characteristics" class="entity-link">AI trustworthiness characteristics</a>: Human judgment is essential in determining the metrics related to AI trustworthiness characteristics.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-20-rephrase">Chunk 20</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-roles-and-responsibilities">
      <h3>human roles and responsibilities</h3>
      
      <div class="desc">The defined roles and responsibilities of humans in the context of decision-making and oversight of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Human roles and responsibilities are evaluated to ensure effective oversight of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-61-rephrase">Chunk 61</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-subject-protection">
      <h3>human subject protection</h3>
      
      <div class="desc">Requirements that ensure the safety and rights of human subjects involved in evaluations.</div>
       
      <ul>
        
        <li>← <a href="#entity-evaluations-involving-human-subjects" class="entity-link">evaluations involving human subjects</a>: Evaluations involving human subjects must meet human subject protection requirements.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-46-rephrase">Chunk 46</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-human-cognitive-bias">
      <h3>human-cognitive bias</h3>
      
      <div class="desc">Biases related to individual or group perceptions of AI system information, influencing decision-making processes.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI RMF 1.0 identifies human-cognitive bias as a major category of AI bias.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-30-rephrase">Chunk 30</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-impacts-on-a-population">
      <h3>impacts on a population</h3>
      
      <div class="desc">The effects or consequences that AI systems may have on different groups within a population.</div>
       
      <ul>
        
        <li>→ <a href="#entity-methodologies" class="entity-link">methodologies</a>: Methodologies for measuring impacts on a population are applicable to understanding AI harms.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-11-rephrase">Chunk 11</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-impacts-to-individuals,-groups,-communities,-organizations,-and-society">
      <h3>impacts to individuals, groups, communities, organizations, and society</h3>
      
      <div class="desc">The effects that AI systems may have on various stakeholders and societal structures.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The impacts of AI systems are evaluated to understand their effects on various stakeholders.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-43-rephrase">Chunk 43</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-individuals,-communities,-and-society">
      <h3>individuals, communities, and society</h3>
      
      <div class="desc">Stakeholders that can experience the impacts of AI systems, both positive and negative.</div>
       
      <ul>
        
        <li>← <a href="#entity-risk-management" class="entity-link">risk management</a>: Risk management provides frameworks to understand and mitigate impacts on individuals, communities, and society.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-9-rephrase">Chunk 9</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-inequitable-outcomes">
      <h3>inequitable outcomes</h3>
      
      <div class="desc">Undesirable results that can be amplified or perpetuated by AI systems if not properly controlled.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems can create opportunities for inequitable outcomes if not properly managed.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-4-rephrase">Chunk 4</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-inscrutability">
      <h3>inscrutability</h3>
      
      <div class="desc">Inscrutability refers to the challenges in understanding and measuring risks associated with AI systems due to their opaque nature and lack of transparency.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-management" class="entity-link">risk management</a>: Inscrutability identifies challenges in risk management for AI systems due to their opaque nature and limited explainability.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-12-rephrase">Chunk 12</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-interdisciplinary-ai-actors">
      <h3>interdisciplinary AI actors</h3>
      
      <div class="desc">Individuals with diverse competencies and skills involved in the development and deployment of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-organizational-risk-tolerances" class="entity-link">organizational risk tolerances</a>: Interdisciplinary AI actors evaluate and document the organizational risk tolerances related to AI technologies.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-41-rephrase">Chunk 41</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-internal-risk-controls">
      <h3>internal risk controls</h3>
      
      <div class="desc">Measures and protocols established to manage and mitigate risks within AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-system-components" class="entity-link">AI system components</a>: Internal risk controls manage risks associated with components of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-43-rephrase">Chunk 43</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-internal-team">
      <h3>internal team</h3>
      
      <div class="desc">A diverse team within an organization that contributes to the development and deployment of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-external-collaborators" class="entity-link">external collaborators</a>: The internal team engages with external collaborators to incorporate diverse perspectives in AI development.</li>
        
        <li>→ <a href="#entity-end-users" class="entity-link">end users</a>: The internal team interacts with end users to understand their needs and impacts of AI systems.</li>
        
        <li>→ <a href="#entity-potentially-impacted-communities" class="entity-link">potentially impacted communities</a>: The internal team may engage with potentially impacted communities based on the risk level of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-39-rephrase">Chunk 39</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-interpretability">
      <h3>interpretability</h3>
      
      <div class="desc">The degree to which an AI system&#39;s decisions can be understood by humans, which may conflict with other characteristics like privacy.</div>
       
      <ul>
        
        <li>→ <a href="#entity-privacy" class="entity-link">privacy</a>: There is a tradeoff between optimizing for interpretability and achieving privacy in AI systems.</li>
        
        <li>← <a href="#entity-predictive-accuracy" class="entity-link">predictive accuracy</a>: A tradeoff exists between predictive accuracy and interpretability in certain scenarios.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Interpretability helps users understand the meaning of AI systems&#x27; outputs in their functional context.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-interpretability">
      <h3>interpretability</h3>
      
      <div class="desc">Interpretability refers to the meaning of the outputs generated by AI systems in the context of their designed functional purposes.</div>
       
      <ul>
        
        <li>→ <a href="#entity-privacy" class="entity-link">privacy</a>: There is a tradeoff between optimizing for interpretability and achieving privacy in AI systems.</li>
        
        <li>← <a href="#entity-predictive-accuracy" class="entity-link">predictive accuracy</a>: A tradeoff exists between predictive accuracy and interpretability in certain scenarios.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Interpretability helps users understand the meaning of AI systems&#x27; outputs in their functional context.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-27-rephrase">Chunk 27</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-interpretability">
      <h3>Interpretability</h3>
      
      <div class="desc">A characteristic of AI systems that allows users to understand the meaning or context of decisions made by the system.</div>
       
      <ul>
        
        <li>← <a href="#entity-explainable-ai" class="entity-link">Explainable AI</a>: Explainable AI frameworks support the development of interpretability in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-28-rephrase">Chunk 28</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-iso-26000:2010">
      <h3>ISO 26000:2010</h3>
      
      <div class="desc">An international standard that outlines social responsibility and the impacts of organizational decisions on society and the environment.</div>
       
      <ul>
        
        <li>← <a href="#entity-responsible-ai" class="entity-link">Responsible AI</a>: Responsible AI practices align with the principles of social responsibility outlined in ISO 26000:2010.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-5-rephrase">Chunk 5</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-iso-31000:2018">
      <h3>ISO 31000:2018</h3>
      
      <div class="desc">ISO 31000:2018 is an international standard that provides guidelines and principles for managing risks within organizations.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The principles of ISO 31000:2018 are infused throughout the AI RMF 1.0 framework.</li>
        
        <li>← <a href="#entity-risk-management" class="entity-link">risk management</a>: Risk management processes are aligned with the guidelines provided by ISO 31000:2018.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-8-rephrase">Chunk 8</a> • <a href="#chunk-9-rephrase">Chunk 9</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-iso-9000:2015">
      <h3>ISO 9000:2015</h3>
      
      <div class="desc">ISO 9000:2015 is an international standard that specifies requirements for a quality management system.</div>
       
      <ul>
        
        <li>→ <a href="#entity-validation" class="entity-link">validation</a>: ISO 9000:2015 provides guidelines for validating the requirements for specific intended uses of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-22-rephrase">Chunk 22</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-iso-guide-73">
      <h3>ISO GUIDE 73</h3>
      
      <div class="desc">ISO GUIDE 73 provides definitions and guidelines for risk management, including concepts like risk tolerance and residual risk.</div>
       
      <ul>
        
        <li>← <a href="#entity-risk-tolerance" class="entity-link">Risk Tolerance</a>: Risk tolerance is influenced by guidelines established in ISO GUIDE 73.</li>
        
        <li>→ <a href="#entity-residual-risk" class="entity-link">residual risk</a>: ISO GUIDE 73 defines residual risk and its implications for risk management practices.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-13-rephrase">Chunk 13</a> • <a href="#chunk-15-rephrase">Chunk 15</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-iso/iec-22989:2022">
      <h3>ISO/IEC 22989:2022</h3>
      
      <div class="desc">An international standard that addresses the characteristics and requirements of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: ISO/IEC 22989:2022 provides standards that align with the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-4-rephrase">Chunk 4</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-iso/iec-tr-24368:2022">
      <h3>ISO/IEC TR 24368:2022</h3>
      
      <div class="desc">A standard that defines professional responsibility in the context of AI, ensuring that professionals recognize their influence on society and the future of AI.</div>
       
      <ul>
        
        <li>← <a href="#entity-responsible-ai" class="entity-link">Responsible AI</a>: Responsible AI is in accord with the professional responsibility defined by ISO/IEC TR 24368:2022.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-5-rephrase">Chunk 5</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-iso/iec-ts-5723:2022">
      <h3>ISO/IEC TS 5723:2022</h3>
      
      <div class="desc">ISO/IEC TS 5723:2022 defines reliability, accuracy, and safety requirements for AI systems, ensuring they do not endanger human life or the environment.</div>
       
      <ul>
        
        <li>→ <a href="#entity-reliability" class="entity-link">reliability</a>: ISO/IEC TS 5723:2022 defines reliability as the ability of a system or product to perform as required without failure.</li>
        
        <li>→ <a href="#entity-accuracy" class="entity-link">accuracy</a>: Defines accuracy as the closeness of results to true values.</li>
        
        <li>→ <a href="#entity-robustness" class="entity-link">robustness</a>: Defines robustness as the ability to maintain performance under various circumstances.</li>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems should comply with the safety requirements outlined in ISO/IEC TS 5723:2022 to avoid endangering human life and health.</li>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: ISO/IEC TS 5723:2022 provides guidelines that align with AI safety risk management approaches.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-22-rephrase">Chunk 22</a> • <a href="#chunk-23-rephrase">Chunk 23</a> • <a href="#chunk-24-rephrase">Chunk 24</a> • <a href="#chunk-25-rephrase">Chunk 25</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-large-organizations">
      <h3>large organizations</h3>
      
      <div class="desc">Large organizations typically have more resources and capabilities to manage AI risks effectively compared to smaller counterparts.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: Large organizations typically have more resources to manage AI risks effectively compared to smaller organizations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-16-rephrase">Chunk 16</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-laurie-e.-locascio">
      <h3>Laurie E. Locascio</h3>
      
      <div class="desc">The Director of the National Institute of Standards and Technology and Under Secretary of Commerce for Standards and Technology.</div>
       
      <ul>
        
        <li>→ <a href="#entity-national-institute-of-standards-and-technology" class="entity-link">National Institute of Standards and Technology</a>: Laurie E. Locascio is the Director of the National Institute of Standards and Technology.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-0-rephrase">Chunk 0</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-machine-learning-attacks">
      <h3>machine learning attacks</h3>
      
      <div class="desc">Security threats that exploit vulnerabilities in machine learning models.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are vulnerable to various machine learning attacks that can compromise security.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage">
      <h3>Manage</h3>
      
      <div class="desc">The MANAGE function in the AI RMF deals with the ongoing management of identified risks throughout the AI system lifecycle, allocating resources as defined by governance structures.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core includes the MANAGE function as one of its components.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a> • <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-32-rephrase">Chunk 32</a> • <a href="#chunk-33-rephrase">Chunk 33</a> • <a href="#chunk-49-rephrase">Chunk 49</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-3">
      <h3>MANAGE 3</h3>
      
      <div class="desc">A function focused on managing AI risks and benefits from third-party entities.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 includes the MANAGE 3 function for managing AI risks and benefits.</li>
        
        <li>→ <a href="#entity-manage-3.1" class="entity-link">MANAGE 3.1</a>: MANAGE 3 is subdivided into MANAGE 3.1 for monitoring AI risks and benefits.</li>
        
        <li>→ <a href="#entity-manage-3.2" class="entity-link">MANAGE 3.2</a>: MANAGE 3 is subdivided into MANAGE 3.2 for monitoring pre-trained models.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-3.1">
      <h3>MANAGE 3.1</h3>
      
      <div class="desc">A subcategory that involves regular monitoring of AI risks and benefits from third-party resources.</div>
       
      <ul>
        
        <li>← <a href="#entity-manage-3" class="entity-link">MANAGE 3</a>: MANAGE 3 is subdivided into MANAGE 3.1 for monitoring AI risks and benefits.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-3.2">
      <h3>MANAGE 3.2</h3>
      
      <div class="desc">A subcategory that focuses on monitoring pre-trained models used in AI system development.</div>
       
      <ul>
        
        <li>← <a href="#entity-manage-3" class="entity-link">MANAGE 3</a>: MANAGE 3 is subdivided into MANAGE 3.2 for monitoring pre-trained models.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-4">
      <h3>MANAGE 4</h3>
      
      <div class="desc">A function that documents and monitors risk treatments, including response and recovery plans for identified AI risks.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 includes the MANAGE 4 function for documenting and monitoring risk treatments.</li>
        
        <li>→ <a href="#entity-manage-4.1" class="entity-link">MANAGE 4.1</a>: MANAGE 4 is subdivided into MANAGE 4.1 for post-deployment monitoring plans.</li>
        
        <li>→ <a href="#entity-manage-4.2" class="entity-link">MANAGE 4.2</a>: MANAGE 4 is subdivided into MANAGE 4.2 for continual improvements in AI system updates.</li>
        
        <li>→ <a href="#entity-manage-4.3" class="entity-link">MANAGE 4.3</a>: MANAGE 4 is subdivided into MANAGE 4.3 for communication regarding incidents and errors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-4.1">
      <h3>MANAGE 4.1</h3>
      
      <div class="desc">A subcategory that implements post-deployment monitoring plans for AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-manage-4" class="entity-link">MANAGE 4</a>: MANAGE 4 is subdivided into MANAGE 4.1 for post-deployment monitoring plans.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-4.2">
      <h3>MANAGE 4.2</h3>
      
      <div class="desc">A subcategory that integrates measurable activities for continual improvements into AI system updates.</div>
       
      <ul>
        
        <li>← <a href="#entity-manage-4" class="entity-link">MANAGE 4</a>: MANAGE 4 is subdivided into MANAGE 4.2 for continual improvements in AI system updates.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-4.3">
      <h3>MANAGE 4.3</h3>
      
      <div class="desc">A subcategory that focuses on communication processes regarding incidents and errors to relevant AI actors.</div>
       
      <ul>
        
        <li>← <a href="#entity-manage-4" class="entity-link">MANAGE 4</a>: MANAGE 4 is subdivided into MANAGE 4.3 for communication regarding incidents and errors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-51-rephrase">Chunk 51</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-manage-function">
      <h3>MANAGE function</h3>
      
      <div class="desc">The MANAGE function focuses on addressing and managing identified AI risks based on assessments from the MAP and MEASURE functions.</div>
       
      <ul>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function provides foundational outcomes that inform the MANAGE function.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The MANAGE function supports organizations in their efforts to mitigate AI risks.</li>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function informs the MANAGE function regarding AI risks.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The MANAGE function is responsible for managing AI risks based on assessments.</li>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The planning and implementation of the MANAGE function are informed by input from relevant AI actors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-39-rephrase">Chunk 39</a> • <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-44-rephrase">Chunk 44</a> • <a href="#chunk-50-rephrase">Chunk 50</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-map">
      <h3>Map</h3>
      
      <div class="desc">The MAP function in the AI RMF involves identifying and assessing AI risks and their impacts, serving as a starting point for risk management processes.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core includes the MAP function as one of its components.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a> • <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-32-rephrase">Chunk 32</a> • <a href="#chunk-33-rephrase">Chunk 33</a> • <a href="#chunk-49-rephrase">Chunk 49</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-map-function">
      <h3>MAP function</h3>
      
      <div class="desc">The MAP function establishes the context for AI systems, identifying risks and informing decision-making related to model management and safety.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: The MAP function illustrates the interdependencies within the AI lifecycle.</li>
        
        <li>→ <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The outcomes of the MAP function support the MEASURE function in assessing AI risks.</li>
        
        <li>→ <a href="#entity-manage-function" class="entity-link">MANAGE function</a>: The MAP function provides foundational outcomes that inform the MANAGE function.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>: The NIST AI RMF Playbook includes the MAP function as part of its guidelines for managing AI risks.</li>
        
        <li>← <a href="#entity-framework-users" class="entity-link">Framework users</a>: Framework users utilize the MAP function to inform decisions about AI system design and deployment.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The MAP function identifies AI risks to help organizations understand potential negative impacts.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 includes the MAP function, which is essential for establishing context and categorizing AI systems.</li>
        
        <li>→ <a href="#entity-ai-system" class="entity-link">AI system</a>: The MAP function is applicable to AI systems for ensuring context understanding and facilitating categorization and assessment.</li>
        
        <li>→ <a href="#entity-scientific-integrity" class="entity-link">scientific integrity</a>: The MAP function supports the identification and documentation of scientific integrity considerations.</li>
        
        <li>→ <a href="#entity-tevv-considerations" class="entity-link">TEVV considerations</a>: The MAP function supports the identification and documentation of TEVV considerations.</li>
        
        <li>→ <a href="#entity-organizational-risk-tolerance" class="entity-link">organizational risk tolerance</a>: The MAP function evaluates potential costs related to organizational risk tolerance in AI systems.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The MAP function includes practices for engaging with relevant AI actors.</li>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function includes evaluations based on risks identified in the MAP function.</li>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The MAP function identifies high-priority AI risks that need to be addressed.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The MAP function supports AI actors in assessing risks associated with AI systems.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The MAP function supports the AI RMF by suggesting documentation processes for AI system performance and trustworthiness.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-38-rephrase">Chunk 38</a> • <a href="#chunk-39-rephrase">Chunk 39</a> • <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-41-rephrase">Chunk 41</a> • <a href="#chunk-42-rephrase">Chunk 42</a> • <a href="#chunk-44-rephrase">Chunk 44</a> • <a href="#chunk-46-rephrase">Chunk 46</a> • <a href="#chunk-47-rephrase">Chunk 47</a> • <a href="#chunk-50-rephrase">Chunk 50</a> • <a href="#chunk-58-rephrase">Chunk 58</a> • <a href="#chunk-63-rephrase">Chunk 63</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-map,-measure,-and-manage-functions">
      <h3>MAP, MEASURE, and MANAGE functions</h3>
      
      <div class="desc">Functions within the AI RMF that focus on managing and measuring risks associated with AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF includes the MAP, MEASURE, and MANAGE functions to address AI system risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-measure">
      <h3>Measure</h3>
      
      <div class="desc">The MEASURE function within the AI RMF focuses on evaluating the effectiveness of risk management strategies and assessing AI risks and outcomes.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-core" class="entity-link">AI RMF Core</a>: The AI RMF Core includes the MEASURE function as one of its components.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a> • <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-32-rephrase">Chunk 32</a> • <a href="#chunk-33-rephrase">Chunk 33</a> • <a href="#chunk-49-rephrase">Chunk 49</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-measure-function">
      <h3>MEASURE function</h3>
      
      <div class="desc">The MEASURE function evaluates AI risks through objective assessments and analytical outputs, focusing on safety, security, and transparency.</div>
       
      <ul>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The outcomes of the MAP function support the MEASURE function in assessing AI risks.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The MEASURE function evaluates AI risks through various metrics and methodologies as part of an overall risk management strategy.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The MEASURE function supports the identification and analysis of AI risks.</li>
        
        <li>→ <a href="#entity-manage-function" class="entity-link">MANAGE function</a>: The MEASURE function informs the MANAGE function regarding AI risks.</li>
        
        <li>← <a href="#entity-tevv-processes" class="entity-link">TEVV processes</a>: TEVV processes are composed of methodologies that are part of the MEASURE function.</li>
        
        <li>← <a href="#entity-system-trustworthiness" class="entity-link">system trustworthiness</a>: The effectiveness of the MEASURE function reflects the system trustworthiness of AI systems.</li>
        
        <li>← <a href="#entity-risk-monitoring-and-response-efforts" class="entity-link">risk monitoring and response efforts</a>: The MEASURE function contributes to risk monitoring and response efforts by providing metrics and outcomes.</li>
        
        <li>→ <a href="#entity-tevv-metrics" class="entity-link">TEVV metrics</a>: The effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.</li>
        
        <li>→ <a href="#entity-map-function" class="entity-link">MAP function</a>: The MEASURE function includes evaluations based on risks identified in the MAP function.</li>
        
        <li>→ <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The MEASURE function is aligned with the guidelines provided in the NIST AI RMF 1.0 document.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The MEASURE function provides analytical outputs that inform the assessment of AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-39-rephrase">Chunk 39</a> • <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-44-rephrase">Chunk 44</a> • <a href="#chunk-45-rephrase">Chunk 45</a> • <a href="#chunk-47-rephrase">Chunk 47</a> • <a href="#chunk-48-rephrase">Chunk 48</a> • <a href="#chunk-50-rephrase">Chunk 50</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-methodologies">
      <h3>methodologies</h3>
      
      <div class="desc">Structured approaches or techniques used to measure and manage risks in AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-organization-developing-the-ai-system" class="entity-link">organization developing the AI system</a>: The organization developing the AI system may use specific methodologies for risk measurement.</li>
        
        <li>← <a href="#entity-impacts-on-a-population" class="entity-link">impacts on a population</a>: Methodologies for measuring impacts on a population are applicable to understanding AI harms.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-11-rephrase">Chunk 11</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-metrics-and-measurement-methodologies">
      <h3>metrics and measurement methodologies</h3>
      
      <div class="desc">Approaches and techniques used to quantify and assess AI risks, ensuring adherence to scientific, legal, and ethical norms.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: Metrics and measurement methodologies support the identification and assessment of AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-45-rephrase">Chunk 45</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-national-ai-initiative-act-of-2020">
      <h3>National AI Initiative Act of 2020</h3>
      
      <div class="desc">A legislative act that calls for the development of a national strategy for AI, including standards and guidelines.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The development of the AI RMF is aligned with the objectives of the National AI Initiative Act of 2020.</li>
        
        <li>→ <a href="#entity-national-security-commission-on-artificial-intelligence" class="entity-link">National Security Commission on Artificial Intelligence</a>: The National AI Initiative Act of 2020 aligns with the recommendations of the National Security Commission on Artificial Intelligence.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-7-rephrase">Chunk 7</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-national-ai-initiative-act-of-2020">
      <h3>National AI Initiative Act of 2020</h3>
      
      <div class="desc">A legislative act aimed at promoting and coordinating AI research and development efforts in the United States.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The development of the AI RMF is aligned with the objectives of the National AI Initiative Act of 2020.</li>
        
        <li>→ <a href="#entity-national-security-commission-on-artificial-intelligence" class="entity-link">National Security Commission on Artificial Intelligence</a>: The National AI Initiative Act of 2020 aligns with the recommendations of the National Security Commission on Artificial Intelligence.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-8-rephrase">Chunk 8</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-national-artificial-intelligence-initiative-act-of-2020">
      <h3>National Artificial Intelligence Initiative Act of 2020</h3>
      
      <div class="desc">This legislative act directs the development and implementation of AI initiatives in the U.S., establishing frameworks for managing AI risks and promoting responsible AI development.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: The act directs the development of AI initiatives, including risk management practices.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF is developed as a resource directed by the National Artificial Intelligence Initiative Act.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-5-rephrase">Chunk 5</a> • <a href="#chunk-6-rephrase">Chunk 6</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-national-institute-of-standards-and-technology">
      <h3>National Institute of Standards and Technology</h3>
      
      <div class="desc">A federal agency that develops and promotes measurement standards, including those for artificial intelligence.</div>
       
      <ul>
        
        <li>← <a href="#entity-u.s.-department-of-commerce" class="entity-link">U.S. Department of Commerce</a>: The U.S. Department of Commerce manages the National Institute of Standards and Technology.</li>
        
        <li>← <a href="#entity-laurie-e.-locascio" class="entity-link">Laurie E. Locascio</a>: Laurie E. Locascio is the Director of the National Institute of Standards and Technology.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-0-rephrase">Chunk 0</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-national-security-commission-on-artificial-intelligence">
      <h3>National Security Commission on Artificial Intelligence</h3>
      
      <div class="desc">This commission provides recommendations on advancing AI technology while ensuring national security.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The AI RMF is consistent with recommendations from the National Security Commission on Artificial Intelligence.</li>
        
        <li>← <a href="#entity-national-ai-initiative-act-of-2020" class="entity-link">National AI Initiative Act of 2020</a>: The National AI Initiative Act of 2020 aligns with the recommendations of the National Security Commission on Artificial Intelligence.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-8-rephrase">Chunk 8</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-negative-ai-risks">
      <h3>negative AI risks</h3>
      
      <div class="desc">Negative AI risks refer to the potential adverse effects and consequences that arise from the deployment of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-technology" class="entity-link">AI technology</a>: The deployment of AI technology can create opportunities for negative AI risks if not managed properly.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-22-rephrase">Chunk 22</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-negative-residual-risks">
      <h3>negative residual risks</h3>
      
      <div class="desc">The sum of all unmitigated risks that affect downstream acquirers of AI systems and end users.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems can contribute to negative residual risks for downstream acquirers and end users.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-50-rephrase">Chunk 50</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist">
      <h3>NIST</h3>
      
      <div class="desc">The National Institute of Standards and Technology (NIST) is a U.S. federal agency responsible for developing standards and guidelines for various technologies, including artificial intelligence.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>: The AI RMF Playbook is published by NIST.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 document is published by NIST.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: NIST aligns the AI RMF with applicable international standards and guidelines.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The AI RMF 1.0 document is authored by NIST.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: NIST develops the AI RMF 1.0 framework to guide AI risk management.</li>
        
        <li>→ <a href="#entity-oecd-framework" class="entity-link">OECD framework</a>: NIST modified the OECD framework to emphasize TEVV processes in the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: NIST develops the AI RMF 1.0 framework to assist organizations in managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-community" class="entity-link">AI community</a>: NIST collaborates with the AI community to evaluate the effectiveness of the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-1-rephrase">Chunk 1</a> • <a href="#chunk-2-rephrase">Chunk 2</a> • <a href="#chunk-6-rephrase">Chunk 6</a> • <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-8-rephrase">Chunk 8</a> • <a href="#chunk-17-rephrase">Chunk 17</a> • <a href="#chunk-31-rephrase">Chunk 31</a> • <a href="#chunk-46-rephrase">Chunk 46</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-ai-100-1">
      <h3>NIST AI 100-1</h3>
      
      <div class="desc">NIST AI 100-1 is a publication that outlines the Artificial Intelligence Risk Management Framework (AI RMF 1.0), providing guidelines and standards for managing risks associated with AI systems throughout their lifecycle.</div>
       
      <ul>
        
        <li>→ <a href="#entity-artificial-intelligence-risk-management-framework-(ai-rmf-1.0)" class="entity-link">Artificial Intelligence Risk Management Framework (AI RMF 1.0)</a>: NIST AI 100-1 includes the Artificial Intelligence Risk Management Framework (AI RMF 1.0).</li>
        
        <li>→ <a href="#entity-u.s.-department-of-commerce" class="entity-link">U.S. Department of Commerce</a>: NIST AI 100-1 is published by the U.S. Department of Commerce.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: NIST AI 100-1 outlines and publishes the AI RMF 1.0 framework for managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: NIST AI 100-1 includes the AI RMF 1.0 framework as part of its guidelines for AI risk management.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: NIST AI 100-1 provides guidelines and standards that inform the implementation of the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: Includes the AI Risk Management Framework 1.0 as part of its content.</li>
        
        <li>→ <a href="#entity-nist-special-publication-1270" class="entity-link">NIST Special Publication 1270</a>: NIST AI 100-1 includes references to NIST Special Publication 1270 for further information on bias.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: NIST AI 100-1 aligns with the principles and framework of AI RMF 1.0 for effective risk management.</li>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework is published as part of the NIST AI 100-1 document.</li>
        
        <li>→ <a href="#entity-manage-3" class="entity-link">MANAGE 3</a>: NIST AI 100-1 includes the MANAGE 3 function for managing AI risks and benefits.</li>
        
        <li>→ <a href="#entity-manage-4" class="entity-link">MANAGE 4</a>: NIST AI 100-1 includes the MANAGE 4 function for documenting and monitoring risk treatments.</li>
        
        <li>← <a href="#entity-ai-rmf-profiles" class="entity-link">AI RMF Profiles</a>: AI RMF Profiles are associated with the NIST AI 100-1 document.</li>
        
        <li>→ <a href="#entity-ai-rmf-1" class="entity-link">AI RMF 1</a>: NIST AI 100-1 is a document published by NIST related to the AI RMF.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF is published by NIST as part of their guidelines on AI risk management.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-0-rephrase">Chunk 0</a> • <a href="#chunk-10-rephrase">Chunk 10</a> • <a href="#chunk-11-rephrase">Chunk 11</a> • <a href="#chunk-13-rephrase">Chunk 13</a> • <a href="#chunk-14-rephrase">Chunk 14</a> • <a href="#chunk-16-rephrase">Chunk 16</a> • <a href="#chunk-23-rephrase">Chunk 23</a> • <a href="#chunk-24-rephrase">Chunk 24</a> • <a href="#chunk-25-rephrase">Chunk 25</a> • <a href="#chunk-28-rephrase">Chunk 28</a> • <a href="#chunk-30-rephrase">Chunk 30</a> • <a href="#chunk-32-rephrase">Chunk 32</a> • <a href="#chunk-34-rephrase">Chunk 34</a> • <a href="#chunk-36-rephrase">Chunk 36</a> • <a href="#chunk-39-rephrase">Chunk 39</a> • <a href="#chunk-41-rephrase">Chunk 41</a> • <a href="#chunk-42-rephrase">Chunk 42</a> • <a href="#chunk-43-rephrase">Chunk 43</a> • <a href="#chunk-44-rephrase">Chunk 44</a> • <a href="#chunk-51-rephrase">Chunk 51</a> • <a href="#chunk-52-rephrase">Chunk 52</a> • <a href="#chunk-57-rephrase">Chunk 57</a> • <a href="#chunk-58-rephrase">Chunk 58</a> • <a href="#chunk-59-rephrase">Chunk 59</a> • <a href="#chunk-61-rephrase">Chunk 61</a> • <a href="#chunk-62-rephrase">Chunk 62</a> • <a href="#chunk-63-rephrase">Chunk 63</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-ai-rmf-1.0">
      <h3>NIST AI RMF 1.0</h3>
      
      <div class="desc">The AI RMF 1.0 is a framework published by NIST that outlines risk management practices for AI systems, providing guidance for organizations.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-technologies" class="entity-link">AI technologies</a>: NIST AI RMF 1.0 provides a framework for managing risks related to AI technologies.</li>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: The AI RMF 1.0 document is authored by NIST.</li>
        
        <li>→ <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>: The AI RMF 1.0 includes additional resources provided in the AI RMF Playbook.</li>
        
        <li>→ <a href="#entity-national-ai-initiative-act-of-2020" class="entity-link">National AI Initiative Act of 2020</a>: The development of the AI RMF is aligned with the objectives of the National AI Initiative Act of 2020.</li>
        
        <li>→ <a href="#entity-national-security-commission-on-artificial-intelligence" class="entity-link">National Security Commission on Artificial Intelligence</a>: The AI RMF is consistent with recommendations from the National Security Commission on Artificial Intelligence.</li>
        
        <li>→ <a href="#entity-govern" class="entity-link">GOVERN</a>: The AI RMF 1.0 framework is composed of the function GOVERN, among others.</li>
        
        <li>→ <a href="#entity-map" class="entity-link">MAP</a>: The AI RMF 1.0 framework includes the function MAP as part of its structure.</li>
        
        <li>→ <a href="#entity-measure" class="entity-link">MEASURE</a>: The AI RMF 1.0 framework includes the function MEASURE to evaluate AI risks.</li>
        
        <li>→ <a href="#entity-manage" class="entity-link">MANAGE</a>: The AI RMF 1.0 framework includes the function MANAGE for overseeing AI risk management.</li>
        
        <li>→ <a href="#entity-transparency" class="entity-link">Transparency</a>: The NIST AI RMF 1.0 document provides guidelines that promote transparency in AI systems.</li>
        
        <li>→ <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>: The NIST AI RMF Playbook is an online companion resource that is part of the NIST AI RMF framework.</li>
        
        <li>→ <a href="#entity-govern" class="entity-link">GOVERN</a>: The AI RMF framework includes the GOVERN function as part of its risk management process.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: The NIST AI RMF framework is applicable to various stages of the AI lifecycle.</li>
        
        <li>→ <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>: The NIST AI RMF Playbook is published as part of the NIST AI RMF framework.</li>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function is aligned with the guidelines provided in the NIST AI RMF 1.0 document.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The NIST AI RMF 1.0 document describes practices related to managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: NIST AI RMF 1.0 is a document published by NIST that serves as a guideline for AI actors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-3-rephrase">Chunk 3</a> • <a href="#chunk-7-rephrase">Chunk 7</a> • <a href="#chunk-26-rephrase">Chunk 26</a> • <a href="#chunk-48-rephrase">Chunk 48</a> • <a href="#chunk-49-rephrase">Chunk 49</a> • <a href="#chunk-56-rephrase">Chunk 56</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-ai-rmf-1.0">
      <h3>NIST AI RMF 1.0</h3>
      
      <div class="desc">The NIST AI RMF 1.0 is a framework designed to manage AI risks and guide organizations in the responsible use of AI technologies.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-technologies" class="entity-link">AI technologies</a>: NIST AI RMF 1.0 provides a framework for managing risks related to AI technologies.</li>
        
        <li>→ <a href="#entity-nist" class="entity-link">NIST</a>: The AI RMF 1.0 document is authored by NIST.</li>
        
        <li>→ <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>: The AI RMF 1.0 includes additional resources provided in the AI RMF Playbook.</li>
        
        <li>→ <a href="#entity-national-ai-initiative-act-of-2020" class="entity-link">National AI Initiative Act of 2020</a>: The development of the AI RMF is aligned with the objectives of the National AI Initiative Act of 2020.</li>
        
        <li>→ <a href="#entity-national-security-commission-on-artificial-intelligence" class="entity-link">National Security Commission on Artificial Intelligence</a>: The AI RMF is consistent with recommendations from the National Security Commission on Artificial Intelligence.</li>
        
        <li>→ <a href="#entity-govern" class="entity-link">GOVERN</a>: The AI RMF 1.0 framework is composed of the function GOVERN, among others.</li>
        
        <li>→ <a href="#entity-map" class="entity-link">MAP</a>: The AI RMF 1.0 framework includes the function MAP as part of its structure.</li>
        
        <li>→ <a href="#entity-measure" class="entity-link">MEASURE</a>: The AI RMF 1.0 framework includes the function MEASURE to evaluate AI risks.</li>
        
        <li>→ <a href="#entity-manage" class="entity-link">MANAGE</a>: The AI RMF 1.0 framework includes the function MANAGE for overseeing AI risk management.</li>
        
        <li>→ <a href="#entity-transparency" class="entity-link">Transparency</a>: The NIST AI RMF 1.0 document provides guidelines that promote transparency in AI systems.</li>
        
        <li>→ <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>: The NIST AI RMF Playbook is an online companion resource that is part of the NIST AI RMF framework.</li>
        
        <li>→ <a href="#entity-govern" class="entity-link">GOVERN</a>: The AI RMF framework includes the GOVERN function as part of its risk management process.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: The NIST AI RMF framework is applicable to various stages of the AI lifecycle.</li>
        
        <li>→ <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>: The NIST AI RMF Playbook is published as part of the NIST AI RMF framework.</li>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function is aligned with the guidelines provided in the NIST AI RMF 1.0 document.</li>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: The NIST AI RMF 1.0 document describes practices related to managing AI risks.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: NIST AI RMF 1.0 is a document published by NIST that serves as a guideline for AI actors.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-33-rephrase">Chunk 33</a> • <a href="#chunk-45-rephrase">Chunk 45</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-ai-rmf-playbook">
      <h3>NIST AI RMF Playbook</h3>
      
      <div class="desc">An online companion resource to the AI RMF that provides tactical actions for organizations to achieve outcomes related to AI risk management.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF Playbook is an online companion resource that is part of the NIST AI RMF framework.</li>
        
        <li>→ <a href="#entity-nist-trustworthy-and-responsible-ai-resource-center" class="entity-link">NIST Trustworthy and Responsible AI Resource Center</a>: The NIST AI RMF Playbook is available through the NIST Trustworthy and Responsible AI Resource Center.</li>
        
        <li>← <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The GOVERN function is supported by the practices described in the NIST AI RMF Playbook.</li>
        
        <li>→ <a href="#entity-map-function" class="entity-link">MAP function</a>: The NIST AI RMF Playbook includes the MAP function as part of its guidelines for managing AI risks.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF Playbook is published as part of the NIST AI RMF framework.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-33-rephrase">Chunk 33</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-ai-rmf-playbook">
      <h3>NIST AI RMF Playbook</h3>
      
      <div class="desc">The NIST AI RMF Playbook outlines practices for governing and managing AI risks, including a framework for mapping and measuring AI system impacts.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF Playbook is an online companion resource that is part of the NIST AI RMF framework.</li>
        
        <li>→ <a href="#entity-nist-trustworthy-and-responsible-ai-resource-center" class="entity-link">NIST Trustworthy and Responsible AI Resource Center</a>: The NIST AI RMF Playbook is available through the NIST Trustworthy and Responsible AI Resource Center.</li>
        
        <li>← <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The GOVERN function is supported by the practices described in the NIST AI RMF Playbook.</li>
        
        <li>→ <a href="#entity-map-function" class="entity-link">MAP function</a>: The NIST AI RMF Playbook includes the MAP function as part of its guidelines for managing AI risks.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF Playbook is published as part of the NIST AI RMF framework.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-35-rephrase">Chunk 35</a> • <a href="#chunk-40-rephrase">Chunk 40</a> • <a href="#chunk-45-rephrase">Chunk 45</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-cybersecurity-framework">
      <h3>NIST Cybersecurity Framework</h3>
      
      <div class="desc">A framework that provides guidelines for managing cybersecurity risks, applicable to AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: The NIST Cybersecurity Framework is applicable to AI safety risk management approaches.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The NIST Cybersecurity Framework aligns with the AI RMF in addressing security and privacy considerations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-25-rephrase">Chunk 25</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-cybersecurity-framework">
      <h3>NIST Cybersecurity Framework</h3>
      
      <div class="desc">A framework that provides guidelines for managing cybersecurity risks, focusing on securing and resilient practices.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: The NIST Cybersecurity Framework is applicable to AI safety risk management approaches.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The NIST Cybersecurity Framework aligns with the AI RMF in addressing security and privacy considerations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-privacy-framework">
      <h3>NIST Privacy Framework</h3>
      
      <div class="desc">A tool for improving privacy through enterprise risk management, focusing on safeguarding human autonomy, identity, and dignity.</div>
       
      <ul>
        
        <li>→ <a href="#entity-privacy-enhancing-technologies-(pets)" class="entity-link">Privacy-enhancing technologies (PETs)</a>: The NIST Privacy Framework supports the use of privacy-enhancing technologies in AI system design.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The NIST Privacy Framework aligns with the AI RMF to enhance privacy protections in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-29-rephrase">Chunk 29</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-privacy-framework">
      <h3>NIST Privacy Framework</h3>
      
      <div class="desc">A framework designed to help organizations manage privacy risks and enhance privacy protections.</div>
       
      <ul>
        
        <li>→ <a href="#entity-privacy-enhancing-technologies-(pets)" class="entity-link">Privacy-enhancing technologies (PETs)</a>: The NIST Privacy Framework supports the use of privacy-enhancing technologies in AI system design.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The NIST Privacy Framework aligns with the AI RMF to enhance privacy protections in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-risk-management-framework">
      <h3>NIST Risk Management Framework</h3>
      
      <div class="desc">A framework that outlines a structured approach to risk management, relevant to AI safety and security.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: The NIST Risk Management Framework is applicable to AI safety risk management approaches.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The NIST Risk Management Framework provides a structured approach that complements the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-25-rephrase">Chunk 25</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-risk-management-framework">
      <h3>NIST Risk Management Framework</h3>
      
      <div class="desc">A framework that provides a structured process for managing risks associated with information systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: The NIST Risk Management Framework is applicable to AI safety risk management approaches.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The NIST Risk Management Framework provides a structured approach that complements the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-special-publication-1270">
      <h3>NIST Special Publication 1270</h3>
      
      <div class="desc">NIST Special Publication 1270 provides guidance on identifying and managing bias in artificial intelligence.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 includes references to NIST Special Publication 1270 for further information on bias.</li>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: NIST Special Publication 1270 aligns with the AI RMF 1.0 framework for managing AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-30-rephrase">Chunk 30</a> • <a href="#chunk-31-rephrase">Chunk 31</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist-trustworthy-and-responsible-ai-resource-center">
      <h3>NIST Trustworthy and Responsible AI Resource Center</h3>
      
      <div class="desc">A center that encompasses the AI RMF and the Playbook, focusing on promoting trustworthy and responsible AI practices.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-rmf-playbook" class="entity-link">NIST AI RMF Playbook</a>: The NIST AI RMF Playbook is available through the NIST Trustworthy and Responsible AI Resource Center.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-33-rephrase">Chunk 33</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-nist.ai.100-1">
      <h3>NIST.AI.100-1</h3>
      
      <div class="desc">A publication related to the AI RMF, available for free online.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The document NIST.AI.100-1 is a publication related to the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-65-rephrase">Chunk 65</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-non-ai-alternative-systems">
      <h3>non-AI alternative systems</h3>
      
      <div class="desc">Systems or approaches that do not utilize artificial intelligence but can serve as alternatives.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: Resources required to manage AI risks are considered alongside viable non-AI alternative systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-50-rephrase">Chunk 50</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-oecd">
      <h3>OECD</h3>
      
      <div class="desc">The OECD (Organisation for Economic Co-operation and Development) has developed a framework for classifying AI lifecycle activities according to five key socio-technical dimensions relevant for AI policy and governance.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: OECD defines AI actors as those involved in the AI system lifecycle.</li>
        
        <li>→ <a href="#entity-ai-lifecycle-activities" class="entity-link">AI lifecycle activities</a>: The OECD developed a framework for classifying AI lifecycle activities according to socio-technical dimensions.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-17-rephrase">Chunk 17</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-oecd-framework-for-the-classification-of-ai-systems">
      <h3>OECD Framework for the Classification of AI systems</h3>
      
      <div class="desc">A framework developed by the OECD for classifying AI systems, referenced in the context of AI lifecycle dimensions.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The OECD Framework influences the design and understanding of AI systems within the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The OECD framework is aligned with the principles outlined in the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-2-rephrase">Chunk 2</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-oecd-framework-for-the-classification-of-ai-systems">
      <h3>OECD Framework for the Classification of AI systems</h3>
      
      <div class="desc">A framework developed by OECD for classifying AI systems, referenced in the context of AI lifecycle and dimensions.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The OECD Framework influences the design and understanding of AI systems within the AI RMF 1.0.</li>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The OECD framework is aligned with the principles outlined in the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-18-rephrase">Chunk 18</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-oecd-recommendation-on-ai:2019">
      <h3>OECD Recommendation on AI:2019</h3>
      
      <div class="desc">A recommendation that provides guidelines for the responsible development and use of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The OECD Recommendation on AI:2019 aligns with the principles outlined in the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-4-rephrase">Chunk 4</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-operation-and-monitoring">
      <h3>Operation and Monitoring</h3>
      
      <div class="desc">Operation and Monitoring tasks are conducted in the Application Context/Operate and Monitor phase, focusing on assessing system output and impacts.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: Operation and Monitoring includes AI actors such as system operators and compliance experts responsible for system assessment.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-54-rephrase">Chunk 54</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-operator-and-practitioner-proficiency">
      <h3>operator and practitioner proficiency</h3>
      
      <div class="desc">The skills and knowledge required for effective operation and trustworthiness of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-system-performance-and-trustworthiness" class="entity-link">AI system performance and trustworthiness</a>: Proficiency in operation supports the performance and trustworthiness of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-43-rephrase">Chunk 43</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizational-culture">
      <h3>organizational culture</h3>
      
      <div class="desc">Organizational culture shapes how an organization operates and manages risk, reflecting shared values and beliefs.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-management" class="entity-link">risk management</a>: A strong organizational culture fosters effective risk management practices within an organization.</li>
        
        <li>← <a href="#entity-governing-authorities" class="entity-link">governing authorities</a>: Governing authorities direct the organizational culture by establishing overarching policies and values.</li>
        
        <li>→ <a href="#entity-risk-management" class="entity-link">risk management</a>: Organizational culture is aligned with the principles of risk management as set by senior leadership.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-34-rephrase">Chunk 34</a> • <a href="#chunk-35-rephrase">Chunk 35</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizational-management">
      <h3>organizational management</h3>
      
      <div class="desc">Entities responsible for overseeing and managing the implementation and operation of AI systems within organizations.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Organizational management oversees the implementation and operation of AI systems within organizations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-55-rephrase">Chunk 55</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizational-risk-priorities">
      <h3>organizational risk priorities</h3>
      
      <div class="desc">The priorities set by an organization to manage risks associated with its operations, particularly in AI.</div>
       
      <ul>
        
        <li>← <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The GOVERN function supports the establishment of organizational risk priorities.</li>
        
        <li>← <a href="#entity-ai-risk-management-training" class="entity-link">AI risk management training</a>: AI risk management training contributes to achieving the organization&#x27;s risk priorities.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-36-rephrase">Chunk 36</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizational-risk-tolerance">
      <h3>organizational risk tolerance</h3>
      
      <div class="desc">The level of risk that an organization is willing to accept when implementing AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function evaluates potential costs related to organizational risk tolerance in AI systems.</li>
        
        <li>← <a href="#entity-ai-system-categorization" class="entity-link">AI system categorization</a>: The categorization of AI systems is aligned with the organization&#x27;s risk tolerance.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-42-rephrase">Chunk 42</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizational-risk-tolerance">
      <h3>organizational risk tolerance</h3>
      
      <div class="desc">The level of risk that an organization is willing to accept in pursuit of its objectives.</div>
       
      <ul>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function evaluates potential costs related to organizational risk tolerance in AI systems.</li>
        
        <li>← <a href="#entity-ai-system-categorization" class="entity-link">AI system categorization</a>: The categorization of AI systems is aligned with the organization&#x27;s risk tolerance.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-43-rephrase">Chunk 43</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizational-risk-tolerances">
      <h3>organizational risk tolerances</h3>
      
      <div class="desc">The levels of risk that an organization is willing to accept when deploying AI technologies.</div>
       
      <ul>
        
        <li>← <a href="#entity-interdisciplinary-ai-actors" class="entity-link">interdisciplinary AI actors</a>: Interdisciplinary AI actors evaluate and document the organizational risk tolerances related to AI technologies.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-41-rephrase">Chunk 41</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizations">
      <h3>organizations</h3>
      
      <div class="desc">Organizations manage AI risks and exhibit varied risk tolerances based on their priorities and resources.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are applicable to various organizations that may have different risk tolerances.</li>
        
        <li>→ <a href="#entity-ai-trustworthiness" class="entity-link">AI trustworthiness</a>: Organizations manage the characteristics of AI trustworthiness while facing tradeoffs.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports organizations&#x27; abilities to operate under applicable legal or regulatory regimes.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-13-rephrase">Chunk 13</a> • <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-organizations">
      <h3>organizations</h3>
      
      <div class="desc">Entities that are supported by the AI RMF to operate under applicable domestic and international legal or regulatory regimes.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are applicable to various organizations that may have different risk tolerances.</li>
        
        <li>→ <a href="#entity-ai-trustworthiness" class="entity-link">AI trustworthiness</a>: Organizations manage the characteristics of AI trustworthiness while facing tradeoffs.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports organizations&#x27; abilities to operate under applicable legal or regulatory regimes.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-65-rephrase">Chunk 65</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-other-ai-actors">
      <h3>Other AI actors</h3>
      
      <div class="desc">Entities that provide norms or guidance for specifying and managing AI risks, including trade associations, standards organizations, advocacy groups, and civil society organizations.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: Other AI actors provide guidance for managing AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-57-rephrase">Chunk 57</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-people-&amp;-planet-dimension">
      <h3>People &amp; Planet dimension</h3>
      
      <div class="desc">This dimension represents human rights and the broader well-being of society and the planet within the AI RMF context.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: The People &amp; Planet dimension includes AI actors who inform the primary audience regarding societal impacts.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-18-rephrase">Chunk 18</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-plan-and-design-function">
      <h3>Plan and Design function</h3>
      
      <div class="desc">A function performed throughout the AI system lifecycle, focusing on planning and designing AI applications.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-system-lifecycle" class="entity-link">AI system lifecycle</a>: The Plan and Design function supports the various stages of the AI system lifecycle.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-19-rephrase">Chunk 19</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-policies-and-norms">
      <h3>policies and norms</h3>
      
      <div class="desc">Established rules and standards that influence risk tolerances for AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-tolerance" class="entity-link">Risk Tolerance</a>: Policies and norms established by organizations influence their risk tolerances.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-13-rephrase">Chunk 13</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-potentially-impacted-communities">
      <h3>potentially impacted communities</h3>
      
      <div class="desc">Groups of people who may be affected by the deployment and decisions of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-internal-team" class="entity-link">internal team</a>: The internal team may engage with potentially impacted communities based on the risk level of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-39-rephrase">Chunk 39</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-pre-trained-models">
      <h3>pre-trained models</h3>
      
      <div class="desc">AI models that have been previously trained on a dataset and can be fine-tuned for specific tasks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-transfer-learning" class="entity-link">transfer learning</a>: Pre-trained models are often utilized in transfer learning to enhance model performance.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-58-rephrase">Chunk 58</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-predictive-accuracy">
      <h3>predictive accuracy</h3>
      
      <div class="desc">The ability of an AI system to make correct predictions, which may be compromised in favor of interpretability.</div>
       
      <ul>
        
        <li>→ <a href="#entity-interpretability" class="entity-link">interpretability</a>: A tradeoff exists between predictive accuracy and interpretability in certain scenarios.</li>
        
        <li>← <a href="#entity-data-sparsity" class="entity-link">data sparsity</a>: Data sparsity can negatively affect predictive accuracy when using privacy-enhancing techniques.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-privacy">
      <h3>privacy</h3>
      
      <div class="desc">Privacy concerns address the ethical use of personal data in AI systems, safeguarding human autonomy and dignity.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: AI risks are aligned with privacy concerns, particularly regarding the use of data in AI systems.</li>
        
        <li>← <a href="#entity-interpretability" class="entity-link">interpretability</a>: There is a tradeoff between optimizing for interpretability and achieving privacy in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-16-rephrase">Chunk 16</a> • <a href="#chunk-28-rephrase">Chunk 28</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-privacy">
      <h3>privacy</h3>
      
      <div class="desc">The protection of personal data, which can sometimes be at odds with the interpretability of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risks" class="entity-link">AI risks</a>: AI risks are aligned with privacy concerns, particularly regarding the use of data in AI systems.</li>
        
        <li>← <a href="#entity-interpretability" class="entity-link">interpretability</a>: There is a tradeoff between optimizing for interpretability and achieving privacy in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-privacy-and-cybersecurity-risk-management">
      <h3>Privacy and cybersecurity risk management</h3>
      
      <div class="desc">Approaches and considerations for managing privacy and cybersecurity risks in the context of AI system design and deployment.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Privacy and cybersecurity risk management considerations are applicable to the design and use of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-59-rephrase">Chunk 59</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-privacy-risk">
      <h3>privacy risk</h3>
      
      <div class="desc">Risks related to the privacy of data handled by AI systems, which are examined and documented.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-system" class="entity-link">AI system</a>: The privacy risk of the AI system is examined and documented.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-47-rephrase">Chunk 47</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-privacy-enhancing-technologies-(pets)">
      <h3>Privacy-enhancing technologies (PETs)</h3>
      
      <div class="desc">Technologies designed to protect privacy in AI systems, including methods like de-identification and aggregation.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-privacy-framework" class="entity-link">NIST Privacy Framework</a>: The NIST Privacy Framework supports the use of privacy-enhancing technologies in AI system design.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-29-rephrase">Chunk 29</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-procurement-tasks">
      <h3>Procurement tasks</h3>
      
      <div class="desc">Tasks conducted by AI actors with authority for the acquisition of AI models, products, or services from third-party developers.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: Procurement tasks are managed by AI actors with financial, legal, or policy authority.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-56-rephrase">Chunk 56</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-reliable-metrics">
      <h3>reliable metrics</h3>
      
      <div class="desc">Metrics that are consistent, accurate, and trustworthy for measuring risk and trustworthiness in AI.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-measurement-challenge" class="entity-link">risk measurement challenge</a>: The lack of reliable metrics contributes to challenges in measuring risk and trustworthiness in AI.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-11-rephrase">Chunk 11</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-residual-risk">
      <h3>residual risk</h3>
      
      <div class="desc">Residual risk is the risk that remains after risk treatment has been applied, impacting end users and communities.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI RMF 1.0 includes the concept of residual risk, emphasizing its importance in understanding the impacts on end users.</li>
        
        <li>← <a href="#entity-iso-guide-73" class="entity-link">ISO GUIDE 73</a>: ISO GUIDE 73 defines residual risk and its implications for risk management practices.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-15-rephrase">Chunk 15</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-resilience">
      <h3>Resilience</h3>
      
      <div class="desc">The ability to return to normal function after an unexpected adverse event, relating to robustness and the unexpected or adversarial use of data.</div>
       
      <ul>
        
        <li>→ <a href="#entity-security" class="entity-link">Security</a>: Resilience is a component of security, which also includes additional protocols.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-26-rephrase">Chunk 26</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-responsible-ai">
      <h3>Responsible AI</h3>
      
      <div class="desc">A framework that emphasizes human centricity, social responsibility, and sustainability in the design, development, and use of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: AI risk management drives responsible uses and practices in AI development.</li>
        
        <li>→ <a href="#entity-iso-26000:2010" class="entity-link">ISO 26000:2010</a>: Responsible AI practices align with the principles of social responsibility outlined in ISO 26000:2010.</li>
        
        <li>→ <a href="#entity-iso/iec-tr-24368:2022" class="entity-link">ISO/IEC TR 24368:2022</a>: Responsible AI is in accord with the professional responsibility defined by ISO/IEC TR 24368:2022.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-5-rephrase">Chunk 5</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-criteria">
      <h3>risk criteria</h3>
      
      <div class="desc">Risk criteria are established guidelines that help organizations assess and manage risks, including tolerance and response strategies.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-tolerance" class="entity-link">risk tolerance</a>: Risk criteria are aligned with the concept of risk tolerance, helping organizations define acceptable levels of risk.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-14-rephrase">Chunk 14</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-management">
      <h3>risk management</h3>
      
      <div class="desc">Risk management involves coordinated activities to identify, assess, and mitigate risks within an organization, particularly focusing on minimizing negative impacts associated with AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-iso-31000:2018" class="entity-link">ISO 31000:2018</a>: Risk management processes are aligned with the guidelines provided by ISO 31000:2018.</li>
        
        <li>→ <a href="#entity-individuals,-communities,-and-society" class="entity-link">individuals, communities, and society</a>: Risk management provides frameworks to understand and mitigate impacts on individuals, communities, and society.</li>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF develops approaches to enhance risk management specifically for AI technologies.</li>
        
        <li>← <a href="#entity-inscrutability" class="entity-link">inscrutability</a>: Inscrutability identifies challenges in risk management for AI systems due to their opaque nature and limited explainability.</li>
        
        <li>← <a href="#entity-human-baseline" class="entity-link">human baseline</a>: The human baseline provides a framework for comparing AI systems against human performance in decision-making tasks.</li>
        
        <li>→ <a href="#entity-accountability" class="entity-link">accountability</a>: Risk management practices help foster accountability in the deployment of AI systems.</li>
        
        <li>← <a href="#entity-organizational-culture" class="entity-link">organizational culture</a>: A strong organizational culture fosters effective risk management practices within an organization.</li>
        
        <li>← <a href="#entity-senior-leadership" class="entity-link">senior leadership</a>: Senior leadership influences the approach to risk management within an organization by setting the tone and expectations.</li>
        
        <li>← <a href="#entity-organizational-culture" class="entity-link">organizational culture</a>: Organizational culture is aligned with the principles of risk management as set by senior leadership.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-9-rephrase">Chunk 9</a> • <a href="#chunk-27-rephrase">Chunk 27</a> • <a href="#chunk-34-rephrase">Chunk 34</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-management-culture">
      <h3>risk management culture</h3>
      
      <div class="desc">A risk management culture refers to the organizational mindset that recognizes and addresses various AI risks, promoting purposeful allocation of resources.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 supports the development of a risk management culture within organizations by providing guidelines for risk assessment.</li>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are applicable to the risk management culture as they require specific risk assessments and management strategies.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-14-rephrase">Chunk 14</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-management-framework">
      <h3>risk management framework</h3>
      
      <div class="desc">A structured approach to identifying, assessing, and mitigating risks associated with AI.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The risk management framework is developed to augment existing practices like the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-13-rephrase">Chunk 13</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-measurement-challenges">
      <h3>risk measurement challenges</h3>
      
      <div class="desc">Challenges associated with quantifying and qualifying risks related to AI systems, particularly when risks are not well-defined.</div>
       
      <ul>
        
        <li>← <a href="#entity-third-party-software,-hardware,-and-data" class="entity-link">third-party software, hardware, and data</a>: Third-party resources contribute to the challenges faced in measuring AI risks effectively.</li>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The complexities of AI systems illustrate the challenges in defining and measuring associated risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-10-rephrase">Chunk 10</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-metrics">
      <h3>risk metrics</h3>
      
      <div class="desc">Quantitative measures used to assess the risks associated with AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-system" class="entity-link">AI system</a>: The AI system&#x27;s risk metrics may not align with those used by organizations deploying the system.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-11-rephrase">Chunk 11</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-monitoring-and-response-efforts">
      <h3>risk monitoring and response efforts</h3>
      
      <div class="desc">Activities aimed at overseeing and addressing identified risks in AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The MEASURE function contributes to risk monitoring and response efforts by providing metrics and outcomes.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-45-rephrase">Chunk 45</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-tolerance">
      <h3>Risk Tolerance</h3>
      
      <div class="desc">Risk tolerance reflects an organization&#39;s readiness to accept risks in pursuit of objectives, influenced by various contextual factors.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF supports the prioritization of risk tolerance in AI systems.</li>
        
        <li>→ <a href="#entity-iso-guide-73" class="entity-link">ISO GUIDE 73</a>: Risk tolerance is influenced by guidelines established in ISO GUIDE 73.</li>
        
        <li>← <a href="#entity-policies-and-norms" class="entity-link">policies and norms</a>: Policies and norms established by organizations influence their risk tolerances.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-13-rephrase">Chunk 13</a> • <a href="#chunk-35-rephrase">Chunk 35</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-risk-tolerance">
      <h3>risk tolerance</h3>
      
      <div class="desc">Risk tolerance is the level of risk that an organization is willing to accept while pursuing its objectives.</div>
       
      <ul>
        
        <li>← <a href="#entity-risk-criteria" class="entity-link">risk criteria</a>: Risk criteria are aligned with the concept of risk tolerance, helping organizations define acceptable levels of risk.</li>
        
        <li>→ <a href="#entity-govern-function" class="entity-link">GOVERN function</a>: The risk tolerance of an organization is applicable to the activities outlined in the GOVERN function.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-14-rephrase">Chunk 14</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-robustness">
      <h3>robustness</h3>
      
      <div class="desc">The ability of a system to maintain its level of performance under a variety of circumstances.</div>
       
      <ul>
        
        <li>← <a href="#entity-iso/iec-ts-5723:2022" class="entity-link">ISO/IEC TS 5723:2022</a>: Defines robustness as the ability to maintain performance under various circumstances.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Contributes to appropriate system functionality in a broad set of conditions.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-23-rephrase">Chunk 23</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-safety-risks">
      <h3>safety risks</h3>
      
      <div class="desc">Safety risks associated with AI systems can lead to serious harm, necessitating tailored management approaches to ensure they remain within acceptable limits.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-risk-management" class="entity-link">AI risk management</a>: AI risk management practices evaluate and prioritize safety risks to minimize potential harms.</li>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are evaluated regularly for safety risks.</li>
        
        <li>← <a href="#entity-ai-system" class="entity-link">AI system</a>: The AI system is evaluated for safety risks as identified in the MAP function.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-24-rephrase">Chunk 24</a> • <a href="#chunk-46-rephrase">Chunk 46</a> • <a href="#chunk-47-rephrase">Chunk 47</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-scientific-integrity">
      <h3>scientific integrity</h3>
      
      <div class="desc">A category that encompasses the principles of maintaining accuracy and reliability in scientific research related to AI.</div>
       
      <ul>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function supports the identification and documentation of scientific integrity considerations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-42-rephrase">Chunk 42</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-secure-software-development-framework">
      <h3>Secure Software Development Framework</h3>
      
      <div class="desc">A framework that outlines best practices for developing secure software to mitigate security risks.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The Secure Software Development Framework shares principles that can inform the AI RMF.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-security">
      <h3>Security</h3>
      
      <div class="desc">Encompasses resilience and includes protocols to avoid, protect against, respond to, or recover from attacks.</div>
       
      <ul>
        
        <li>← <a href="#entity-resilience" class="entity-link">Resilience</a>: Resilience is a component of security, which also includes additional protocols.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-26-rephrase">Chunk 26</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-security-and-resilience">
      <h3>security and resilience</h3>
      
      <div class="desc">Characteristics of AI systems that relate to their ability to withstand adverse events and maintain functionality.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-safety-risk-management-approaches" class="entity-link">AI safety risk management approaches</a>: Security and resilience are included as key considerations in AI safety risk management approaches.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-25-rephrase">Chunk 25</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-senior-leadership">
      <h3>senior leadership</h3>
      
      <div class="desc">Senior leadership refers to the top executives in an organization who set the tone for risk management and influence organizational culture.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-management" class="entity-link">risk management</a>: Senior leadership influences the approach to risk management within an organization by setting the tone and expectations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-34-rephrase">Chunk 34</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-small-to-medium-sized-organizations">
      <h3>small to medium-sized organizations</h3>
      
      <div class="desc">These organizations may face unique challenges in managing AI risks compared to larger organizations due to their limited resources and capabilities.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-risks" class="entity-link">AI risks</a>: Small to medium-sized organizations face unique challenges in managing AI risks due to their limited resources.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-16-rephrase">Chunk 16</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-social-and-organizational-behavior">
      <h3>social and organizational behavior</h3>
      
      <div class="desc">The behaviors and practices within social and organizational contexts that influence AI trustworthiness.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-trustworthiness-characteristics" class="entity-link">AI trustworthiness characteristics</a>: AI trustworthiness characteristics are influenced by social and organizational behavior.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-20-rephrase">Chunk 20</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-societal-dynamics">
      <h3>societal dynamics</h3>
      
      <div class="desc">Factors related to human behavior and societal interactions that influence the deployment and functioning of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are influenced by societal dynamics and human behavior.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-4-rephrase">Chunk 4</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-stakeholders">
      <h3>stakeholders</h3>
      
      <div class="desc">Individuals or groups involved in implementing AI risk management and learning from the framework.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF provides a framework for stakeholders to learn from implementing AI risk management.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-65-rephrase">Chunk 65</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-standards,-guidelines,-best-practices,-methodologies,-and-tools">
      <h3>standards, guidelines, best practices, methodologies, and tools</h3>
      
      <div class="desc">A collection of existing resources that help manage AI risks and foster awareness in the field.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF fosters greater awareness of existing resources for managing AI risks.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-65-rephrase">Chunk 65</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-system-trustworthiness">
      <h3>system trustworthiness</h3>
      
      <div class="desc">The degree to which an AI system can be trusted based on its performance and risk assessment.</div>
       
      <ul>
        
        <li>→ <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The effectiveness of the MEASURE function reflects the system trustworthiness of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-45-rephrase">Chunk 45</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-systemic-bias">
      <h3>systemic bias</h3>
      
      <div class="desc">A type of bias that can be present in AI datasets and organizational practices, reflecting broader societal norms.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI RMF 1.0 identifies systemic bias as a major category of AI bias.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-30-rephrase">Chunk 30</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-target-profile">
      <h3>Target Profile</h3>
      
      <div class="desc">Indicates the outcomes needed to achieve the desired or target AI risk management goals.</div>
       
      <ul>
        
        <li>← <a href="#entity-current-profile" class="entity-link">Current Profile</a>: Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-52-rephrase">Chunk 52</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-test,-evaluation,-verification,-and-validation-(tevv)">
      <h3>Test, Evaluation, Verification, and Validation (TEVV)</h3>
      
      <div class="desc">TEVV tasks are performed throughout the AI lifecycle to ensure the reliability and effectiveness of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors support the TEVV framework by performing necessary tasks throughout the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The TEVV framework applies to AI systems to ensure their reliability and compliance.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-54-rephrase">Chunk 54</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-test,-evaluation,-verification,-and-validation-(tevv)">
      <h3>Test, Evaluation, Verification, and Validation (TEVV)</h3>
      
      <div class="desc">A framework encompassing tasks performed throughout the AI lifecycle to ensure the reliability and compliance of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI actors</a>: AI actors support the TEVV framework by performing necessary tasks throughout the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: The TEVV framework applies to AI systems to ensure their reliability and compliance.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-55-rephrase">Chunk 55</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-tevv">
      <h3>TEVV</h3>
      
      <div class="desc">TEVV refers to tasks that provide insights into technical, societal, legal, and ethical standards, assisting in risk management throughout the AI lifecycle.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf" class="entity-link">AI RMF</a>: The AI RMF supports the integration of TEVV tasks throughout the AI lifecycle.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: TEVV tasks are integrated throughout the AI lifecycle to manage risks effectively.</li>
        
        <li>→ <a href="#entity-ai-actors" class="entity-link">AI actors</a>: TEVV tasks involve AI actors who ensure the reliability of AI systems throughout their lifecycle.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-18-rephrase">Chunk 18</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-tevv-considerations">
      <h3>TEVV considerations</h3>
      
      <div class="desc">Considerations related to Trustworthiness, Explainability, Verifiability, and Validity in the context of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-map-function" class="entity-link">MAP function</a>: The MAP function supports the identification and documentation of TEVV considerations.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-42-rephrase">Chunk 42</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-tevv-findings">
      <h3>TEVV findings</h3>
      
      <div class="desc">Findings related to Trustworthiness, Explainability, Verifiability, and Validity that can be evaluated by subject matter experts.</div>
       
      <ul>
        
        <li>← <a href="#entity-subject-matter-experts" class="entity-link">subject matter experts</a>: Subject matter experts evaluate TEVV findings to align them with deployment conditions.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-21-rephrase">Chunk 21</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-tevv-metrics">
      <h3>TEVV metrics</h3>
      
      <div class="desc">Metrics and processes employed to evaluate the effectiveness of the MEASURE function in assessing AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: The effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-47-rephrase">Chunk 47</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-tevv-practices">
      <h3>TEVV practices</h3>
      
      <div class="desc">Practices related to Testing, Evaluation, Validation, and Verification of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: The AI RMF 1.0 framework includes TEVV practices for evaluating AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-31-rephrase">Chunk 31</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-tevv-processes">
      <h3>TEVV processes</h3>
      
      <div class="desc">TEVV (test, evaluation, verification, and validation) processes are crucial throughout the AI lifecycle to ensure the reliability and safety of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.</li>
        
        <li>→ <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: TEVV processes are composed of methodologies that are part of the MEASURE function.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-17-rephrase">Chunk 17</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-tevv-processes">
      <h3>TEVV processes</h3>
      
      <div class="desc">Test, evaluation, verification, and validation processes that ensure the effectiveness of AI systems.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.</li>
        
        <li>→ <a href="#entity-measure-function" class="entity-link">MEASURE function</a>: TEVV processes are composed of methodologies that are part of the MEASURE function.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-44-rephrase">Chunk 44</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-third-party-ai-technologies">
      <h3>third-party AI technologies</h3>
      
      <div class="desc">AI technologies developed by external organizations that may pose additional risks.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems may face risks from third-party AI technologies that are outside an organization&#x27;s control.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Third-party AI technologies may introduce risks when integrated into an organization&#x27;s AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-60-rephrase">Chunk 60</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-third-party-ai-technologies">
      <h3>third-party AI technologies</h3>
      
      <div class="desc">AI technologies developed by external organizations that may pose risks when integrated into an organization&#39;s systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems may face risks from third-party AI technologies that are outside an organization&#x27;s control.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Third-party AI technologies may introduce risks when integrated into an organization&#x27;s AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-61-rephrase">Chunk 61</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-third-party-data">
      <h3>third-party data</h3>
      
      <div class="desc">Data sourced from external entities that can be integrated into AI products or services.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-products-or-services" class="entity-link">AI products or services</a>: Third-party data can be integrated into AI products or services, affecting their risk profile.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-11-rephrase">Chunk 11</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-third-party-entities">
      <h3>Third-party entities</h3>
      
      <div class="desc">Providers, developers, vendors, and evaluators of data, algorithms, models, and systems that are external to the organization acquiring their technologies or services.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-actors" class="entity-link">AI Actors</a>: AI actors support the involvement of third-party entities in AI design and development tasks.</li>
        
        <li>→ <a href="#entity-end-users" class="entity-link">End users</a>: Third-party entities provide technologies and services that end users utilize in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-57-rephrase">Chunk 57</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-third-party-software-and-data">
      <h3>third-party software and data</h3>
      
      <div class="desc">External software and data sources that may be integrated into AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-technology" class="entity-link">AI technology</a>: AI technology includes the use of third-party software and data.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-43-rephrase">Chunk 43</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-third-party-software,-hardware,-and-data">
      <h3>third-party software, hardware, and data</h3>
      
      <div class="desc">External resources that can influence the development and deployment of AI systems, potentially complicating risk measurement.</div>
       
      <ul>
        
        <li>→ <a href="#entity-risk-measurement-challenges" class="entity-link">risk measurement challenges</a>: Third-party resources contribute to the challenges faced in measuring AI risks effectively.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-10-rephrase">Chunk 10</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-traditional-software">
      <h3>traditional software</h3>
      
      <div class="desc">Software systems that do not incorporate artificial intelligence and follow conventional programming paradigms.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-based-technology" class="entity-link">AI-based technology</a>: AI-based technology presents new risks compared to traditional software.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-58-rephrase">Chunk 58</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-training-data">
      <h3>training data</h3>
      
      <div class="desc">Training data refers to the datasets used to train AI systems, which can influence their performance and decision-making capabilities.</div>
       
      <ul>
        
        <li>→ <a href="#entity-transparency" class="entity-link">transparency</a>: Maintaining the provenance of training data supports transparency and accountability in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-27-rephrase">Chunk 27</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-transfer-learning">
      <h3>transfer learning</h3>
      
      <div class="desc">Transfer learning is a machine learning technique that reuses a model developed for one task as the starting point for a model on a different task.</div>
       
      <ul>
        
        <li>← <a href="#entity-pre-trained-models" class="entity-link">pre-trained models</a>: Pre-trained models are often utilized in transfer learning to enhance model performance.</li>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Transfer learning is a methodology that can be applied to enhance the capabilities of AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-58-rephrase">Chunk 58</a> • <a href="#chunk-61-rephrase">Chunk 61</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-transparency">
      <h3>Transparency</h3>
      
      <div class="desc">Transparency in AI refers to the availability of information regarding the system&#39;s operations and outputs, promoting fairness and accountability.</div>
       
      <ul>
        
        <li>← <a href="#entity-accountability" class="entity-link">Accountability</a>: Accountability in AI systems is supported by transparency.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI Lifecycle</a>: Transparency is relevant at various stages of the AI lifecycle.</li>
        
        <li>← <a href="#entity-nist-ai-rmf-1.0" class="entity-link">NIST AI RMF 1.0</a>: The NIST AI RMF 1.0 document provides guidelines that promote transparency in AI systems.</li>
        
        <li>← <a href="#entity-explainable-ai" class="entity-link">Explainable AI</a>: Explainable AI frameworks promote transparency in AI systems by clarifying decision-making processes.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-26-rephrase">Chunk 26</a> • <a href="#chunk-30-rephrase">Chunk 30</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-transparency">
      <h3>transparency</h3>
      
      <div class="desc">Transparency in AI emphasizes clarity and openness about the mechanisms and processes involved in AI system operations.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems benefit from transparency to enhance accountability and trustworthiness, particularly in relation to bias.</li>
        
        <li>← <a href="#entity-training-data" class="entity-link">training data</a>: Maintaining the provenance of training data supports transparency and accountability in AI systems.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-27-rephrase">Chunk 27</a> • <a href="#chunk-28-rephrase">Chunk 28</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-trustworthiness-characteristics">
      <h3>trustworthiness characteristics</h3>
      
      <div class="desc">Trustworthiness characteristics are attributes that determine the reliability, fairness, and transparency of AI systems.</div>
       
      <ul>
        
        <li>← <a href="#entity-trustworthy-ai-systems" class="entity-link">trustworthy AI systems</a>: Trustworthy AI systems are characterized by validity, reliability, accountability, and transparency.</li>
        
        <li>→ <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: Trustworthiness characteristics influence the evaluation and management of AI systems throughout the AI lifecycle.</li>
        
        <li>← <a href="#entity-ai-designer" class="entity-link">AI designer</a>: AI designers may have a different perception of trustworthiness characteristics compared to other AI actors.</li>
        
        <li>← <a href="#entity-ai-developer" class="entity-link">AI developer</a>: AI developers may interpret trustworthiness characteristics differently based on their role in the AI lifecycle.</li>
        
        <li>← <a href="#entity-ai-deployer" class="entity-link">AI deployer</a>: AI deployers assess trustworthiness characteristics based on the operational context of the AI system.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-22-rephrase">Chunk 22</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-trustworthy-ai">
      <h3>trustworthy AI</h3>
      
      <div class="desc">AI systems that are valid, reliable, safe, secure, resilient, accountable, transparent, explainable, interpretable, privacy-enhanced, and fair with harmful bias managed.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-1.0" class="entity-link">AI RMF 1.0</a>: AI RMF 1.0 provides guidance for achieving trustworthy AI.</li>
        
        <li>→ <a href="#entity-ai-trustworthiness-characteristics" class="entity-link">AI trustworthiness characteristics</a>: Trustworthy AI encompasses various characteristics that define its trustworthiness.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-20-rephrase">Chunk 20</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-trustworthy-ai-system">
      <h3>trustworthy AI system</h3>
      
      <div class="desc">A trustworthy AI system is designed to be reliable and fit for purpose, ensuring that all AI actors share responsibilities in its development and deployment.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-lifecycle" class="entity-link">AI lifecycle</a>: The AI lifecycle supports the development of trustworthy AI systems by outlining the necessary stages and responsibilities involved.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-12-rephrase">Chunk 12</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-trustworthy-ai-systems">
      <h3>trustworthy AI systems</h3>
      
      <div class="desc">AI systems characterized by necessary conditions of trustworthiness, including validity, reliability, accountability, and transparency.</div>
       
      <ul>
        
        <li>→ <a href="#entity-trustworthiness-characteristics" class="entity-link">trustworthiness characteristics</a>: Trustworthy AI systems are characterized by validity, reliability, accountability, and transparency.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-3-rephrase">Chunk 3</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-trustworthy-characteristics">
      <h3>trustworthy characteristics</h3>
      
      <div class="desc">Trustworthy characteristics are attributes of AI systems that ensure their reliable and ethical operation.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-systems" class="entity-link">AI systems</a>: AI systems are evaluated for trustworthy characteristics.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-44-rephrase">Chunk 44</a> • <a href="#chunk-46-rephrase">Chunk 46</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-u.s.-department-of-commerce">
      <h3>U.S. Department of Commerce</h3>
      
      <div class="desc">A government department responsible for promoting economic growth, which oversees the National Institute of Standards and Technology.</div>
       
      <ul>
        
        <li>← <a href="#entity-nist-ai-100-1" class="entity-link">NIST AI 100-1</a>: NIST AI 100-1 is published by the U.S. Department of Commerce.</li>
        
        <li>→ <a href="#entity-national-institute-of-standards-and-technology" class="entity-link">National Institute of Standards and Technology</a>: The U.S. Department of Commerce manages the National Institute of Standards and Technology.</li>
        
        <li>← <a href="#entity-gina-m.-raimondo" class="entity-link">Gina M. Raimondo</a>: Gina M. Raimondo is the Secretary directing the U.S. Department of Commerce.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-0-rephrase">Chunk 0</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-version-control-table">
      <h3>Version Control Table</h3>
      
      <div class="desc">A tool used to track changes in documents, including version numbers, dates, and descriptions of changes.</div>
       
      <ul>
        
        <li>← <a href="#entity-ai-rmf-playbook" class="entity-link">AI RMF Playbook</a>: The AI RMF Playbook includes a Version Control Table to track changes.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-1-rephrase">Chunk 1</a>
      </div>
      
    </div>
    
    <div class="entity-card" id="entity-video-compression-models">
      <h3>video compression models</h3>
      
      <div class="desc">AI models specifically designed to improve the efficiency of video data compression.</div>
       
      <ul>
        
        <li>→ <a href="#entity-ai-systems" class="entity-link">AI systems</a>: Video compression models function as a specific application of AI systems that may not require human oversight.</li>
        
      </ul>
       
      <div class="chunk-links">
        Appears in: <a href="#chunk-61-rephrase">Chunk 61</a>
      </div>
      
    </div>
     
    <div class="footer">
      <div>
        Generated by
        <a title="Knwl AI" target="_blank" href="https://knwl.ai">Knwl</a>,
        &copy; 2026
        <a href="https://graphsandnetworks.com" title="Orbifold Consulting"
          >Orbifold Consulting</a
        >.
      </div>
      <div class="disclaimer">The information contained in this document has been extracted and processed using automated artificial intelligence tools (Knwl.AI). While we strive for accuracy, this data is generated via machine learning algorithms and natural language processing, which may result in errors, omissions, or misinterpretations of the original source material. This document is provided &#34;as is&#34; and for informational purposes only. Orbifold Consulting makes no warranties, express or implied, regarding the accuracy, completeness, or reliability of this information. Users are advised to independently verify any critical data against original source documents before making business, legal, or financial decisions. Orbifold Consulting assumes no liability for any actions taken in reliance upon this information.</div>
    </div>

    

    <script>
      const rawData = {
  "title": "NIST AI Risk Management Framework (AI RMF 1.0)",
  "summary": "The NIST AI 100-1 document outlines the Artificial Intelligence Risk Management Framework (AI RMF 1.0), which aims to provide guidelines for managing risks associated with AI systems. It emphasizes the importance of understanding and addressing various risks, impacts, and challenges in AI risk management, including risk measurement and prioritization. The framework is designed to be a living document, with plans for regular updates and community input, and includes a core structure for governance, mapping, measuring, and managing AI risks. Additionally, it highlights the need for trustworthy AI systems that are valid, safe, secure, accountable, and fair, while also addressing harmful biases.",
  "url": "https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf",
  "schema": {
    "entity_types": [
      "document",
      "organization",
      "person",
      "framework",
      "function",
      "category",
      "action",
      "outcome",
      "resource",
      "technology",
      "perspective",
      "sector",
      "domain",
      "standard",
      "guideline",
      "practice",
      "methodology",
      "tool",
      "risk",
      "system"
    ],
    "relation_types": [
      "published_by",
      "authored_by",
      "directed_by",
      "composed_of",
      "subdivided_into",
      "available_at",
      "reviewed_by",
      "updated_by",
      "aligned_with",
      "applicable_to",
      "supports",
      "includes",
      "contributes_to",
      "develops",
      "evaluates",
      "deploys",
      "manages",
      "illustrates",
      "fosters",
      "operates_under",
      "infused_throughout",
      "reflects",
      "creates_opportunities_for",
      "identifies",
      "provides"
    ],
    "reasoning": "The text is centered around the NIST AI Risk Management Framework (AI RMF), which is a document published by the National Institute of Standards and Technology (NIST). Key entities include organizations like NIST, people such as Gina M. Raimondo and Laurie E. Locascio, and the framework itself. The framework is composed of functions, categories, actions, and outcomes, which are central to its structure. Relations describe how these entities interact, such as the framework being published by NIST, composed of functions, and aligned with standards and guidelines. The text also emphasizes the framework's applicability across various sectors and domains, and its role in managing AI risks, which informs the choice of relation types."
  },
  "stats": [
    {
      "num_chunks": 66,
      "schema_discovery_time": 8.34,
      "extraction_wall_time": 120.8,
      "consolidation_time": 51.23,
      "total_cpu_time": 1074.88,
      "total_time": 180.37,
      "parallelism": 8.9,
      "throughput_tps": 207.9,
      "time": {
        "min": 9.97,
        "max": 25.56,
        "avg": 16.29,
        "median": 15.64
      },
      "tokens": {
        "min": 228,
        "max": 400,
        "avg": 381,
        "total": 25116
      },
      "entities": {
        "total": 458,
        "avg": 6.9
      },
      "relations": {
        "total": 339,
        "avg": 5.1
      },
      "community_detection_time": 53.15,
      "communities": {
        "count": 32,
        "singletons": 0,
        "largest": 36,
        "size": {
          "min": 2,
          "max": 36,
          "avg": 7.9,
          "median": 2.0
        }
      },
      "run": {
        "timestamp": "2026-02-10 18:08:06",
        "file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf",
        "backend": "openai",
        "extraction_model": "gpt-4o-mini",
        "discovery_model": "gpt-4o",
        "max_tokens": 400,
        "overlap_tokens": 50,
        "max_concurrent": 10,
        "use_cache": true,
        "no_discovery": false
      }
    }
  ],
  "communities": [
    {
      "id": 0,
      "topics": [
        "Subdivision",
        "Framework",
        "Person"
      ],
      "description": "Around Subdivision, Framework, Person.",
      "members": [
        "AI RMF 1",
        "AI RMF Profiles",
        "AI RMF temporal profiles",
        "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
        "Gina M. Raimondo",
        "Laurie E. Locascio",
        "MANAGE 3",
        "MANAGE 3.1",
        "MANAGE 3.2",
        "MANAGE 4",
        "MANAGE 4.1",
        "MANAGE 4.2",
        "MANAGE 4.3",
        "NIST AI 100-1",
        "NIST Special Publication 1270",
        "National Institute of Standards and Technology",
        "U.S. Department of Commerce"
      ]
    },
    {
      "id": 1,
      "topics": [
        "Function",
        "Category",
        "Person"
      ],
      "description": "Around Function, Category, Person.",
      "members": [
        "AI Actor Tasks",
        "AI Deployment",
        "AI Development",
        "AI Impact Assessment",
        "AI RMF Core",
        "AI actors",
        "AI lifecycle activities",
        "AI lifecycle stages",
        "Domain Expert",
        "GOVERN 5",
        "Govern",
        "Governance and Oversight tasks",
        "Human factors professionals",
        "Manage",
        "Map",
        "Measure",
        "OECD",
        "Operation and Monitoring",
        "People & Planet dimension",
        "Procurement tasks",
        "diverse team",
        "downstream risks"
      ]
    },
    {
      "id": 2,
      "topics": [
        "Concept",
        "Framework",
        "Technology"
      ],
      "description": "Around Concept, Framework, Technology.",
      "members": [
        "AI Lifecycle",
        "AI technologies",
        "Accountability",
        "Explainable AI",
        "GOVERN",
        "General public",
        "Interpretability",
        "MANAGE",
        "MAP",
        "MEASURE",
        "NIST AI RMF 1.0",
        "National AI Initiative Act of 2020",
        "National Security Commission on Artificial Intelligence",
        "Privacy",
        "Transparency",
        "society"
      ]
    },
    {
      "id": 3,
      "topics": [
        "Outcome",
        "Methodology"
      ],
      "description": "Around Outcome, Methodology.",
      "members": [
        "impacts on a population",
        "methodologies",
        "organization developing the AI system"
      ]
    },
    {
      "id": 4,
      "topics": [
        "Person",
        "Category",
        "System"
      ],
      "description": "Around Person, Category, System.",
      "members": [
        "AI deployer",
        "AI designer",
        "AI developer",
        "trustworthiness characteristics",
        "trustworthy AI systems"
      ]
    },
    {
      "id": 5,
      "topics": [
        "Category",
        "Framework",
        "Risk"
      ],
      "description": "Around Category, Framework, Risk.",
      "members": [
        "AI RMF 1.0",
        "AI RMF Playbook",
        "AI Risk Management Framework Roadmap",
        "AI Risks",
        "AI Risks and Trustworthiness",
        "AI community",
        "Bias",
        "Core and Profiles",
        "Fairness in AI",
        "Foundational Information",
        "ISO GUIDE 73",
        "NIST",
        "OECD framework",
        "Risk Tolerance",
        "TEVV practices",
        "Version Control Table",
        "computational and statistical bias",
        "human-cognitive bias",
        "policies and norms",
        "residual risk",
        "risk management culture",
        "risk management framework",
        "systemic bias",
        "trustworthy AI"
      ]
    },
    {
      "id": 6,
      "topics": [
        "System",
        "Function"
      ],
      "description": "Around System, Function.",
      "members": [
        "AI system lifecycle",
        "Plan and Design function"
      ]
    },
    {
      "id": 7,
      "topics": [
        "Function",
        "Technology",
        "Resource"
      ],
      "description": "Around Function, Technology, Resource.",
      "members": [
        "AI models and algorithms",
        "AI trustworthiness characteristics",
        "datasets",
        "human judgment",
        "social and organizational behavior"
      ]
    },
    {
      "id": 8,
      "topics": [
        "Resource"
      ],
      "description": "Around Resource.",
      "members": [
        "TEVV findings",
        "subject matter experts"
      ]
    },
    {
      "id": 9,
      "topics": [
        "Category",
        "Framework",
        "Risk"
      ],
      "description": "Around Category, Framework, Risk.",
      "members": [
        "AI systems",
        "Affected individuals/communities",
        "ISO/IEC TS 5723:2022",
        "Privacy and cybersecurity risk management",
        "Test, Evaluation, Verification, and Validation (TEVV)",
        "accuracy",
        "cognitive biases",
        "compliance experts",
        "data sparsity",
        "enterprise risk management",
        "explainability",
        "fairness",
        "generative AI",
        "harmful bias",
        "human decision maker",
        "human roles and responsibilities",
        "impacts to individuals, groups, communities, organizations, and society",
        "inequitable outcomes",
        "interpretability",
        "machine learning attacks",
        "negative residual risks",
        "organizational management",
        "pre-trained models",
        "predictive accuracy",
        "privacy",
        "reliability",
        "risk measurement challenges",
        "robustness",
        "societal dynamics",
        "third-party AI technologies",
        "third-party software, hardware, and data",
        "training data",
        "transfer learning",
        "transparency",
        "trustworthy characteristics",
        "video compression models"
      ]
    },
    {
      "id": 10,
      "topics": [
        "Organization",
        "Risk",
        "Standard"
      ],
      "description": "Around Organization, Risk, Standard.",
      "members": [
        "AI risk management",
        "AI system",
        "ISO 26000:2010",
        "ISO/IEC TR 24368:2022",
        "Responsible AI",
        "civil society organizations",
        "end users",
        "environmental groups",
        "environmental impact",
        "external collaborators",
        "fairness and bias",
        "internal team",
        "potentially impacted communities",
        "privacy risk",
        "risk metrics",
        "safety risks"
      ]
    },
    {
      "id": 11,
      "topics": [
        "Framework",
        "Category",
        "Standard"
      ],
      "description": "Around Framework, Category, Standard.",
      "members": [
        "AI RMF",
        "AI safety risk management approaches",
        "AI technology",
        "AI trustworthiness",
        "ISO/IEC 22989:2022",
        "MAP, MEASURE, and MANAGE functions",
        "NIST Cybersecurity Framework",
        "NIST Privacy Framework",
        "NIST Risk Management Framework",
        "NIST.AI.100-1",
        "National Artificial Intelligence Initiative Act of 2020",
        "OECD Framework for the Classification of AI systems",
        "OECD Recommendation on AI:2019",
        "Privacy-enhancing technologies (PETs)",
        "Secure Software Development Framework",
        "TEVV",
        "negative AI risks",
        "organizations",
        "security and resilience",
        "stakeholders",
        "standards, guidelines, best practices, methodologies, and tools",
        "third-party software and data"
      ]
    },
    {
      "id": 12,
      "topics": [
        "Concept"
      ],
      "description": "Around Concept.",
      "members": [
        "Resilience",
        "Security"
      ]
    },
    {
      "id": 13,
      "topics": [
        "Category",
        "Standard",
        "Organization"
      ],
      "description": "Around Category, Standard, Organization.",
      "members": [
        "ISO 31000:2018",
        "accountability",
        "governing authorities",
        "human baseline",
        "individuals, communities, and society",
        "inscrutability",
        "organizational culture",
        "risk management",
        "senior leadership"
      ]
    },
    {
      "id": 14,
      "topics": [
        "Practice",
        "Function",
        "Document"
      ],
      "description": "Around Practice, Function, Document.",
      "members": [
        "AI risk management training",
        "GOVERN function",
        "NIST AI RMF Playbook",
        "NIST Trustworthy and Responsible AI Resource Center",
        "executive leadership",
        "human oversight processes",
        "organizational risk priorities",
        "risk criteria",
        "risk tolerance"
      ]
    },
    {
      "id": 15,
      "topics": [
        "Document"
      ],
      "description": "Around Document.",
      "members": [
        "GOVERN 5.2",
        "system design"
      ]
    },
    {
      "id": 16,
      "topics": [
        "Document"
      ],
      "description": "Around Document.",
      "members": [
        "GOVERN 6",
        "third-party software"
      ]
    },
    {
      "id": 17,
      "topics": [
        "Document"
      ],
      "description": "Around Document.",
      "members": [
        "GOVERN 6.2",
        "high-risk systems"
      ]
    },
    {
      "id": 18,
      "topics": [
        "Person",
        "Risk"
      ],
      "description": "Around Person, Risk.",
      "members": [
        "interdisciplinary AI actors",
        "organizational risk tolerances"
      ]
    },
    {
      "id": 19,
      "topics": [
        "Function"
      ],
      "description": "Around Function.",
      "members": [
        "AI system performance and trustworthiness",
        "operator and practitioner proficiency"
      ]
    },
    {
      "id": 20,
      "topics": [
        "Category",
        "Function",
        "Organization"
      ],
      "description": "Around Category, Function, Organization.",
      "members": [
        "AI lifecycle",
        "AI risk management activities",
        "AI risks",
        "AI system categorization",
        "Framework users",
        "GOVERN 5.1",
        "Human Factors",
        "MANAGE function",
        "MAP function",
        "MEASURE function",
        "Other AI actors",
        "TEVV considerations",
        "TEVV metrics",
        "TEVV processes",
        "affected communities",
        "contextually sensitive evaluations",
        "cybersecurity",
        "domain experts",
        "individuals and communities",
        "large organizations",
        "metrics and measurement methodologies",
        "non-AI alternative systems",
        "organizational risk tolerance",
        "risk monitoring and response efforts",
        "scientific integrity",
        "small to medium-sized organizations",
        "system trustworthiness",
        "trustworthy AI system"
      ]
    },
    {
      "id": 21,
      "topics": [
        "Document"
      ],
      "description": "Around Document.",
      "members": [
        "Current Profile",
        "Target Profile"
      ]
    },
    {
      "id": 22,
      "topics": [
        "Technology",
        "Risk",
        "Category"
      ],
      "description": "Around Technology, Risk, Category.",
      "members": [
        "AI-based technology",
        "data quality issues",
        "traditional software"
      ]
    },
    {
      "id": 23,
      "topics": [
        "Resource"
      ],
      "description": "Around Resource.",
      "members": [
        "AI products or services",
        "third-party data"
      ]
    },
    {
      "id": 24,
      "topics": [
        "Risk"
      ],
      "description": "Around Risk.",
      "members": [
        "emergent risks",
        "risk management efforts"
      ]
    },
    {
      "id": 25,
      "topics": [
        "Category"
      ],
      "description": "Around Category.",
      "members": [
        "reliable metrics",
        "risk measurement challenge"
      ]
    },
    {
      "id": 26,
      "topics": [
        "Sector"
      ],
      "description": "Around Sector.",
      "members": [
        "civil society",
        "methods"
      ]
    },
    {
      "id": 27,
      "topics": [
        "Standard"
      ],
      "description": "Around Standard.",
      "members": [
        "ISO 9000:2015",
        "validation"
      ]
    },
    {
      "id": 28,
      "topics": [
        "Document"
      ],
      "description": "Around Document.",
      "members": [
        "GOVERN 6.1",
        "third-party entities"
      ]
    },
    {
      "id": 29,
      "topics": [
        "System"
      ],
      "description": "Around System.",
      "members": [
        "AI system components",
        "internal risk controls"
      ]
    },
    {
      "id": 30,
      "topics": [
        "Standard"
      ],
      "description": "Around Standard.",
      "members": [
        "evaluations involving human subjects",
        "human subject protection"
      ]
    },
    {
      "id": 31,
      "topics": [
        "Organization"
      ],
      "description": "Around Organization.",
      "members": [
        "AI Actors",
        "End users",
        "Third-party entities"
      ]
    }
  ],
  "consolidated": {
    "entities": [
      {
        "name": "NIST AI 100-1",
        "type": "document",
        "description": "NIST AI 100-1 is a publication that outlines the Artificial Intelligence Risk Management Framework (AI RMF 1.0), providing guidelines and standards for managing risks associated with AI systems throughout their lifecycle.",
        "chunk_ids": [
          0,
          10,
          11,
          13,
          14,
          16,
          23,
          24,
          25,
          28,
          30,
          32,
          34,
          36,
          39,
          41,
          42,
          43,
          44,
          51,
          52,
          57,
          58,
          59,
          61,
          62,
          63
        ],
        "community_id": 0
      },
      {
        "name": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
        "type": "framework",
        "description": "A framework intended to manage risks associated with artificial intelligence, designed to be a living document that will be regularly updated.",
        "chunk_ids": [
          0
        ],
        "community_id": 0
      },
      {
        "name": "U.S. Department of Commerce",
        "type": "organization",
        "description": "A government department responsible for promoting economic growth, which oversees the National Institute of Standards and Technology.",
        "chunk_ids": [
          0
        ],
        "community_id": 0
      },
      {
        "name": "Gina M. Raimondo",
        "type": "person",
        "description": "The Secretary of the U.S. Department of Commerce.",
        "chunk_ids": [
          0
        ],
        "community_id": 0
      },
      {
        "name": "National Institute of Standards and Technology",
        "type": "organization",
        "description": "A federal agency that develops and promotes measurement standards, including those for artificial intelligence.",
        "chunk_ids": [
          0
        ],
        "community_id": 0
      },
      {
        "name": "Laurie E. Locascio",
        "type": "person",
        "description": "The Director of the National Institute of Standards and Technology and Under Secretary of Commerce for Standards and Technology.",
        "chunk_ids": [
          0
        ],
        "community_id": 0
      },
      {
        "name": "AI RMF Playbook",
        "type": "document",
        "description": "A playbook developed by NIST that outlines the framework for managing risks associated with artificial intelligence.",
        "chunk_ids": [
          1
        ]
      },
      {
        "name": "NIST",
        "type": "organization",
        "description": "The National Institute of Standards and Technology (NIST) is a U.S. federal agency responsible for developing standards and guidelines for various technologies, including artificial intelligence.",
        "chunk_ids": [
          1,
          2,
          6,
          7,
          8,
          17,
          31,
          46
        ],
        "community_id": 5
      },
      {
        "name": "Version Control Table",
        "type": "tool",
        "description": "A tool used to track changes in documents, including version numbers, dates, and descriptions of changes.",
        "chunk_ids": [
          1
        ],
        "community_id": 5
      },
      {
        "name": "AI Risks and Trustworthiness",
        "type": "category",
        "description": "This category within the AI RMF Playbook addresses various aspects of AI risks and the criteria necessary for AI systems to be considered trustworthy.",
        "chunk_ids": [
          1,
          19
        ],
        "community_id": 5
      },
      {
        "name": "Foundational Information",
        "type": "section",
        "description": "A section in the AI RMF Playbook that provides essential background information on AI risk management.",
        "chunk_ids": [
          1
        ],
        "community_id": 5
      },
      {
        "name": "Core and Profiles",
        "type": "section",
        "description": "A section in the AI RMF Playbook that outlines the core components and profiles related to AI risk management.",
        "chunk_ids": [
          1
        ],
        "community_id": 5
      },
      {
        "name": "AI RMF 1.0",
        "type": "document",
        "description": "AI RMF 1.0 is a framework developed by NIST for managing risks associated with AI systems, emphasizing governance, fairness, and bias management.",
        "chunk_ids": [
          2,
          15,
          28,
          29,
          34,
          46,
          51
        ]
      },
      {
        "name": "AI RMF Core",
        "type": "framework",
        "description": "The core components of the AI Risk Management Framework, which include functions such as Govern, Map, Measure, and Manage.",
        "chunk_ids": [
          2
        ]
      },
      {
        "name": "Govern",
        "type": "function",
        "description": "The GOVERN function of the AI RMF focuses on establishing governance structures for AI systems and cultivating a culture of risk management within organizations.",
        "chunk_ids": [
          2,
          7,
          32,
          33,
          34,
          49
        ],
        "community_id": 1
      },
      {
        "name": "Map",
        "type": "function",
        "description": "The MAP function in the AI RMF involves identifying and assessing AI risks and their impacts, serving as a starting point for risk management processes.",
        "chunk_ids": [
          2,
          7,
          32,
          33,
          49
        ],
        "community_id": 1
      },
      {
        "name": "Measure",
        "type": "function",
        "description": "The MEASURE function within the AI RMF focuses on evaluating the effectiveness of risk management strategies and assessing AI risks and outcomes.",
        "chunk_ids": [
          2,
          7,
          32,
          33,
          49
        ],
        "community_id": 1
      },
      {
        "name": "Manage",
        "type": "function",
        "description": "The MANAGE function in the AI RMF deals with the ongoing management of identified risks throughout the AI system lifecycle, allocating resources as defined by governance structures.",
        "chunk_ids": [
          2,
          7,
          32,
          33,
          49
        ],
        "community_id": 1
      },
      {
        "name": "AI Risks",
        "type": "risk",
        "description": "AI risks refer to potential negative impacts and uncertainties associated with the deployment and use of AI systems, differing from traditional software risks.",
        "chunk_ids": [
          2,
          3,
          16,
          35,
          40,
          44,
          45,
          48,
          50
        ],
        "community_id": 5
      },
      {
        "name": "AI Actor Tasks",
        "type": "category",
        "description": "Tasks performed by various actors involved in the AI lifecycle, including testing, evaluation, verification, and validation.",
        "chunk_ids": [
          2
        ],
        "community_id": 1
      },
      {
        "name": "OECD Framework for the Classification of AI systems",
        "type": "framework",
        "description": "A framework developed by the OECD for classifying AI systems, referenced in the context of AI lifecycle dimensions.",
        "chunk_ids": [
          2
        ]
      },
      {
        "name": "AI actors",
        "type": "category",
        "description": "AI actors encompass individuals and organizations involved in the AI lifecycle, including those who build, use, verify, and validate AI models.",
        "chunk_ids": [
          3,
          6,
          17,
          19,
          54,
          57,
          58
        ]
      },
      {
        "name": "trustworthy AI systems",
        "type": "system",
        "description": "AI systems characterized by necessary conditions of trustworthiness, including validity, reliability, accountability, and transparency.",
        "chunk_ids": [
          3
        ],
        "community_id": 4
      },
      {
        "name": "AI risk management activities",
        "type": "function",
        "description": "Functions that organize activities to govern, map, measure, and manage risks associated with AI technologies.",
        "chunk_ids": [
          3
        ],
        "community_id": 20
      },
      {
        "name": "NIST AI RMF 1.0",
        "type": "document",
        "description": "The AI RMF 1.0 is a framework published by NIST that outlines risk management practices for AI systems, providing guidance for organizations.",
        "chunk_ids": [
          3,
          7,
          26,
          48,
          49,
          56
        ]
      },
      {
        "name": "AI technologies",
        "type": "technology",
        "description": "Technologies that have the potential to transform various sectors of society, including commerce, health, transportation, and cybersecurity.",
        "chunk_ids": [
          3
        ],
        "community_id": 2
      },
      {
        "name": "AI RMF",
        "type": "framework",
        "description": "The AI RMF is designed to help organizations manage risks associated with AI systems, promoting trustworthy development and use across various contexts.",
        "chunk_ids": [
          4,
          6,
          9,
          17,
          18,
          60,
          63,
          65
        ],
        "community_id": 11
      },
      {
        "name": "AI systems",
        "type": "system",
        "description": "AI systems are technological solutions that utilize artificial intelligence to perform tasks and make decisions, often requiring varying levels of human oversight.",
        "chunk_ids": [
          4,
          23,
          24,
          27,
          55,
          60,
          62
        ]
      },
      {
        "name": "OECD Recommendation on AI:2019",
        "type": "standard",
        "description": "A recommendation that provides guidelines for the responsible development and use of AI systems.",
        "chunk_ids": [
          4
        ],
        "community_id": 11
      },
      {
        "name": "ISO/IEC 22989:2022",
        "type": "standard",
        "description": "An international standard that addresses the characteristics and requirements of AI systems.",
        "chunk_ids": [
          4
        ],
        "community_id": 11
      },
      {
        "name": "AI risk management",
        "type": "function",
        "description": "AI risk management is a key component of responsible AI development, aimed at mitigating and managing risks associated with AI systems.",
        "chunk_ids": [
          4,
          5
        ]
      },
      {
        "name": "societal dynamics",
        "type": "category",
        "description": "Factors related to human behavior and societal interactions that influence the deployment and functioning of AI systems.",
        "chunk_ids": [
          4
        ],
        "community_id": 9
      },
      {
        "name": "inequitable outcomes",
        "type": "outcome",
        "description": "Undesirable results that can be amplified or perpetuated by AI systems if not properly controlled.",
        "chunk_ids": [
          4
        ],
        "community_id": 9
      },
      {
        "name": "Responsible AI",
        "type": "framework",
        "description": "A framework that emphasizes human centricity, social responsibility, and sustainability in the design, development, and use of AI systems.",
        "chunk_ids": [
          5
        ],
        "community_id": 10
      },
      {
        "name": "ISO 26000:2010",
        "type": "standard",
        "description": "An international standard that outlines social responsibility and the impacts of organizational decisions on society and the environment.",
        "chunk_ids": [
          5
        ],
        "community_id": 10
      },
      {
        "name": "ISO/IEC TR 24368:2022",
        "type": "standard",
        "description": "A standard that defines professional responsibility in the context of AI, ensuring that professionals recognize their influence on society and the future of AI.",
        "chunk_ids": [
          5
        ],
        "community_id": 10
      },
      {
        "name": "National Artificial Intelligence Initiative Act of 2020",
        "type": "document",
        "description": "This legislative act directs the development and implementation of AI initiatives in the U.S., establishing frameworks for managing AI risks and promoting responsible AI development.",
        "chunk_ids": [
          5,
          6
        ],
        "community_id": 11
      },
      {
        "name": "AI RMF Playbook",
        "type": "resource",
        "description": "A supplementary resource related to the AI RMF, providing additional guidance and tools for implementing the framework.",
        "chunk_ids": [
          7
        ],
        "community_id": 5
      },
      {
        "name": "National AI Initiative Act of 2020",
        "type": "standard",
        "description": "A legislative act that calls for the development of a national strategy for AI, including standards and guidelines.",
        "chunk_ids": [
          7
        ]
      },
      {
        "name": "National Security Commission on Artificial Intelligence",
        "type": "organization",
        "description": "This commission provides recommendations on advancing AI technology while ensuring national security.",
        "chunk_ids": [
          7,
          8
        ],
        "community_id": 2
      },
      {
        "name": "AI lifecycle",
        "type": "category",
        "description": "The AI lifecycle encompasses the stages through which an AI system progresses, from development to deployment and management.",
        "chunk_ids": [
          7,
          33,
          38,
          48
        ]
      },
      {
        "name": "National AI Initiative Act of 2020",
        "type": "document",
        "description": "A legislative act aimed at promoting and coordinating AI research and development efforts in the United States.",
        "chunk_ids": [
          8
        ],
        "community_id": 2
      },
      {
        "name": "AI RMF 1.0",
        "type": "framework",
        "description": "The AI RMF 1.0 provides structured guidelines for managing risks associated with AI systems, emphasizing trustworthiness and effective risk management practices.",
        "chunk_ids": [
          8,
          10,
          11,
          13,
          14,
          16,
          19,
          20,
          23,
          24,
          25,
          30,
          31,
          32,
          36,
          39,
          41,
          42,
          43,
          44,
          58,
          59,
          61,
          62
        ],
        "community_id": 5
      },
      {
        "name": "AI Risk Management Framework Roadmap",
        "type": "document",
        "description": "A roadmap that outlines priority research and guidance to enhance the AI RMF.",
        "chunk_ids": [
          8
        ],
        "community_id": 5
      },
      {
        "name": "ISO 31000:2018",
        "type": "standard",
        "description": "ISO 31000:2018 is an international standard that provides guidelines and principles for managing risks within organizations.",
        "chunk_ids": [
          8,
          9
        ],
        "community_id": 13
      },
      {
        "name": "AI systems",
        "type": "technology",
        "description": "AI systems are technological solutions that utilize artificial intelligence to perform tasks typically requiring human intelligence, impacting individuals and society in various ways.",
        "chunk_ids": [
          9,
          10,
          13,
          14,
          15,
          30,
          34,
          46,
          50,
          59,
          61
        ],
        "community_id": 9
      },
      {
        "name": "risk management",
        "type": "function",
        "description": "Risk management involves coordinated activities to identify, assess, and mitigate risks within an organization, particularly focusing on minimizing negative impacts associated with AI systems.",
        "chunk_ids": [
          9,
          27,
          34
        ],
        "community_id": 13
      },
      {
        "name": "individuals, communities, and society",
        "type": "category",
        "description": "Stakeholders that can experience the impacts of AI systems, both positive and negative.",
        "chunk_ids": [
          9
        ],
        "community_id": 13
      },
      {
        "name": "third-party software, hardware, and data",
        "type": "resource",
        "description": "External resources that can influence the development and deployment of AI systems, potentially complicating risk measurement.",
        "chunk_ids": [
          10
        ],
        "community_id": 9
      },
      {
        "name": "risk measurement challenges",
        "type": "category",
        "description": "Challenges associated with quantifying and qualifying risks related to AI systems, particularly when risks are not well-defined.",
        "chunk_ids": [
          10
        ],
        "community_id": 9
      },
      {
        "name": "AI system",
        "type": "system",
        "description": "An AI system is a technological framework that employs artificial intelligence to perform tasks typically requiring human intelligence, ensuring safety, security, and responsible governance.",
        "chunk_ids": [
          11,
          40,
          41,
          42,
          47
        ],
        "community_id": 10
      },
      {
        "name": "risk metrics",
        "type": "category",
        "description": "Quantitative measures used to assess the risks associated with AI systems.",
        "chunk_ids": [
          11
        ],
        "community_id": 10
      },
      {
        "name": "methodologies",
        "type": "methodology",
        "description": "Structured approaches or techniques used to measure and manage risks in AI systems.",
        "chunk_ids": [
          11
        ],
        "community_id": 3
      },
      {
        "name": "third-party data",
        "type": "resource",
        "description": "Data sourced from external entities that can be integrated into AI products or services.",
        "chunk_ids": [
          11
        ],
        "community_id": 23
      },
      {
        "name": "emergent risks",
        "type": "risk",
        "description": "Newly arising risks that may not have been previously identified or considered in risk management efforts.",
        "chunk_ids": [
          11
        ],
        "community_id": 24
      },
      {
        "name": "reliable metrics",
        "type": "category",
        "description": "Metrics that are consistent, accurate, and trustworthy for measuring risk and trustworthiness in AI.",
        "chunk_ids": [
          11
        ],
        "community_id": 25
      },
      {
        "name": "impacts on a population",
        "type": "outcome",
        "description": "The effects or consequences that AI systems may have on different groups within a population.",
        "chunk_ids": [
          11
        ],
        "community_id": 3
      },
      {
        "name": "AI lifecycle",
        "type": "framework",
        "description": "The AI lifecycle encompasses the stages of development, deployment, and evaluation of AI systems, guiding their progression from design to post-deployment.",
        "chunk_ids": [
          12,
          22,
          26
        ]
      },
      {
        "name": "AI developer",
        "type": "person",
        "description": "An AI developer is an individual or organization that creates AI software and implements systems based on specific designs, considering associated risks.",
        "chunk_ids": [
          12,
          22
        ],
        "community_id": 4
      },
      {
        "name": "AI deployer",
        "type": "person",
        "description": "An AI deployer is responsible for implementing and operating AI models in real-world contexts, facing distinct risks compared to developers.",
        "chunk_ids": [
          12,
          22
        ],
        "community_id": 4
      },
      {
        "name": "trustworthy AI system",
        "type": "system",
        "description": "A trustworthy AI system is designed to be reliable and fit for purpose, ensuring that all AI actors share responsibilities in its development and deployment.",
        "chunk_ids": [
          12
        ],
        "community_id": 20
      },
      {
        "name": "inscrutability",
        "type": "risk",
        "description": "Inscrutability refers to the challenges in understanding and measuring risks associated with AI systems due to their opaque nature and lack of transparency.",
        "chunk_ids": [
          12
        ],
        "community_id": 13
      },
      {
        "name": "human baseline",
        "type": "category",
        "description": "Human baseline refers to the metrics used for comparison when assessing AI systems that augment or replace human activities, particularly in decision-making.",
        "chunk_ids": [
          12
        ],
        "community_id": 13
      },
      {
        "name": "Risk Tolerance",
        "type": "category",
        "description": "Risk tolerance reflects an organization's readiness to accept risks in pursuit of objectives, influenced by various contextual factors.",
        "chunk_ids": [
          13,
          35
        ],
        "community_id": 5
      },
      {
        "name": "ISO GUIDE 73",
        "type": "standard",
        "description": "ISO GUIDE 73 provides definitions and guidelines for risk management, including concepts like risk tolerance and residual risk.",
        "chunk_ids": [
          13,
          15
        ],
        "community_id": 5
      },
      {
        "name": "organizations",
        "type": "category",
        "description": "Organizations manage AI risks and exhibit varied risk tolerances based on their priorities and resources.",
        "chunk_ids": [
          13,
          21
        ]
      },
      {
        "name": "policies and norms",
        "type": "guideline",
        "description": "Established rules and standards that influence risk tolerances for AI systems.",
        "chunk_ids": [
          13
        ],
        "community_id": 5
      },
      {
        "name": "risk management framework",
        "type": "framework",
        "description": "A structured approach to identifying, assessing, and mitigating risks associated with AI.",
        "chunk_ids": [
          13
        ],
        "community_id": 5
      },
      {
        "name": "civil society",
        "type": "sector",
        "description": "A sector that contributes to the development and debate of methods for informing harm/cost-benefit tradeoffs in AI.",
        "chunk_ids": [
          13
        ],
        "community_id": 26
      },
      {
        "name": "risk management culture",
        "type": "category",
        "description": "A risk management culture refers to the organizational mindset that recognizes and addresses various AI risks, promoting purposeful allocation of resources.",
        "chunk_ids": [
          14
        ],
        "community_id": 5
      },
      {
        "name": "risk criteria",
        "type": "guideline",
        "description": "Risk criteria are established guidelines that help organizations assess and manage risks, including tolerance and response strategies.",
        "chunk_ids": [
          14
        ],
        "community_id": 14
      },
      {
        "name": "risk tolerance",
        "type": "standard",
        "description": "Risk tolerance is the level of risk that an organization is willing to accept while pursuing its objectives.",
        "chunk_ids": [
          14
        ],
        "community_id": 14
      },
      {
        "name": "residual risk",
        "type": "risk",
        "description": "Residual risk is the risk that remains after risk treatment has been applied, impacting end users and communities.",
        "chunk_ids": [
          15
        ],
        "community_id": 5
      },
      {
        "name": "end users",
        "type": "person",
        "description": "End users are individuals or communities that interact with AI systems, providing insights and feedback on their impacts and associated risks.",
        "chunk_ids": [
          15,
          19,
          39,
          48,
          54,
          57
        ],
        "community_id": 10
      },
      {
        "name": "AI actors",
        "type": "organization",
        "description": "AI actors are stakeholders involved in the AI lifecycle, each with distinct responsibilities and varying levels of risk awareness.",
        "chunk_ids": [
          15,
          18,
          32,
          50,
          56
        ]
      },
      {
        "name": "cybersecurity",
        "type": "category",
        "description": "Cybersecurity encompasses the practices and technologies used to protect systems, networks, and data from cyber threats.",
        "chunk_ids": [
          16
        ],
        "community_id": 20
      },
      {
        "name": "privacy",
        "type": "category",
        "description": "Privacy concerns address the ethical use of personal data in AI systems, safeguarding human autonomy and dignity.",
        "chunk_ids": [
          16,
          28
        ]
      },
      {
        "name": "small to medium-sized organizations",
        "type": "organization",
        "description": "These organizations may face unique challenges in managing AI risks compared to larger organizations due to their limited resources and capabilities.",
        "chunk_ids": [
          16
        ],
        "community_id": 20
      },
      {
        "name": "large organizations",
        "type": "organization",
        "description": "Large organizations typically have more resources and capabilities to manage AI risks effectively compared to smaller counterparts.",
        "chunk_ids": [
          16
        ],
        "community_id": 20
      },
      {
        "name": "OECD",
        "type": "organization",
        "description": "The OECD (Organisation for Economic Co-operation and Development) has developed a framework for classifying AI lifecycle activities according to five key socio-technical dimensions relevant for AI policy and governance.",
        "chunk_ids": [
          17
        ],
        "community_id": 1
      },
      {
        "name": "TEVV processes",
        "type": "function",
        "description": "TEVV (test, evaluation, verification, and validation) processes are crucial throughout the AI lifecycle to ensure the reliability and safety of AI systems.",
        "chunk_ids": [
          17
        ]
      },
      {
        "name": "AI lifecycle",
        "type": "domain",
        "description": "The AI lifecycle encompasses all stages of AI system development and use, including design, development, deployment, evaluation, and management of AI risks.",
        "chunk_ids": [
          17
        ]
      },
      {
        "name": "TEVV",
        "type": "function",
        "description": "TEVV refers to tasks that provide insights into technical, societal, legal, and ethical standards, assisting in risk management throughout the AI lifecycle.",
        "chunk_ids": [
          18
        ],
        "community_id": 11
      },
      {
        "name": "People & Planet dimension",
        "type": "category",
        "description": "This dimension represents human rights and the broader well-being of society and the planet within the AI RMF context.",
        "chunk_ids": [
          18
        ],
        "community_id": 1
      },
      {
        "name": "OECD Framework for the Classification of AI systems",
        "type": "standard",
        "description": "A framework developed by OECD for classifying AI systems, referenced in the context of AI lifecycle and dimensions.",
        "chunk_ids": [
          18
        ],
        "community_id": 11
      },
      {
        "name": "Plan and Design function",
        "type": "function",
        "description": "A function performed throughout the AI system lifecycle, focusing on planning and designing AI applications.",
        "chunk_ids": [
          19
        ],
        "community_id": 6
      },
      {
        "name": "AI system lifecycle",
        "type": "system",
        "description": "The stages through which an AI system progresses, from conception to deployment and beyond.",
        "chunk_ids": [
          19
        ],
        "community_id": 6
      },
      {
        "name": "environmental groups",
        "type": "organization",
        "description": "Organizations focused on environmental issues that can provide context and understanding of AI impacts.",
        "chunk_ids": [
          19
        ],
        "community_id": 10
      },
      {
        "name": "civil society organizations",
        "type": "organization",
        "description": "Organizations that represent the interests of the public and can contribute to AI risk management.",
        "chunk_ids": [
          19
        ],
        "community_id": 10
      },
      {
        "name": "trustworthy AI",
        "type": "category",
        "description": "AI systems that are valid, reliable, safe, secure, resilient, accountable, transparent, explainable, interpretable, privacy-enhanced, and fair with harmful bias managed.",
        "chunk_ids": [
          20
        ],
        "community_id": 5
      },
      {
        "name": "AI trustworthiness characteristics",
        "type": "function",
        "description": "Characteristics that define the trustworthiness of AI systems, including accountability and transparency.",
        "chunk_ids": [
          20
        ],
        "community_id": 7
      },
      {
        "name": "social and organizational behavior",
        "type": "domain",
        "description": "The behaviors and practices within social and organizational contexts that influence AI trustworthiness.",
        "chunk_ids": [
          20
        ],
        "community_id": 7
      },
      {
        "name": "datasets",
        "type": "resource",
        "description": "Datasets are collections of data used to train AI systems, impacting their trustworthiness and relevance.",
        "chunk_ids": [
          20,
          58
        ],
        "community_id": 7
      },
      {
        "name": "AI models and algorithms",
        "type": "technology",
        "description": "The computational methods and structures that underpin AI systems and affect their trustworthiness.",
        "chunk_ids": [
          20
        ],
        "community_id": 7
      },
      {
        "name": "human judgment",
        "type": "function",
        "description": "The decision-making process employed by humans to determine metrics related to AI trustworthiness.",
        "chunk_ids": [
          20
        ],
        "community_id": 7
      },
      {
        "name": "AI trustworthiness",
        "type": "concept",
        "description": "A social concept that encompasses various characteristics of AI systems, emphasizing that trustworthiness is influenced by tradeoffs among these characteristics.",
        "chunk_ids": [
          21
        ],
        "community_id": 11
      },
      {
        "name": "interpretability",
        "type": "characteristic",
        "description": "The degree to which an AI system's decisions can be understood by humans, which may conflict with other characteristics like privacy.",
        "chunk_ids": [
          21
        ]
      },
      {
        "name": "privacy",
        "type": "characteristic",
        "description": "The protection of personal data, which can sometimes be at odds with the interpretability of AI systems.",
        "chunk_ids": [
          21
        ],
        "community_id": 9
      },
      {
        "name": "predictive accuracy",
        "type": "characteristic",
        "description": "The ability of an AI system to make correct predictions, which may be compromised in favor of interpretability.",
        "chunk_ids": [
          21
        ],
        "community_id": 9
      },
      {
        "name": "data sparsity",
        "type": "condition",
        "description": "A situation where there is insufficient data, which can lead to tradeoffs in AI system performance.",
        "chunk_ids": [
          21
        ],
        "community_id": 9
      },
      {
        "name": "TEVV findings",
        "type": "resource",
        "description": "Findings related to Trustworthiness, Explainability, Verifiability, and Validity that can be evaluated by subject matter experts.",
        "chunk_ids": [
          21
        ],
        "community_id": 8
      },
      {
        "name": "AI lifecycle",
        "type": "process",
        "description": "The stages through which an AI system progresses, from development to deployment, where contextual awareness can be enhanced.",
        "chunk_ids": [
          21
        ],
        "community_id": 20
      },
      {
        "name": "contextually sensitive evaluations",
        "type": "practice",
        "description": "Evaluations that take into account the specific context in which an AI system operates, aiming to identify benefits and risks.",
        "chunk_ids": [
          21
        ],
        "community_id": 20
      },
      {
        "name": "trustworthiness characteristics",
        "type": "category",
        "description": "Trustworthiness characteristics are attributes that determine the reliability, fairness, and transparency of AI systems.",
        "chunk_ids": [
          22
        ],
        "community_id": 4
      },
      {
        "name": "AI designer",
        "type": "person",
        "description": "An AI designer is an individual responsible for creating the design and specifications of an AI system.",
        "chunk_ids": [
          22
        ],
        "community_id": 4
      },
      {
        "name": "ISO 9000:2015",
        "type": "standard",
        "description": "ISO 9000:2015 is an international standard that specifies requirements for a quality management system.",
        "chunk_ids": [
          22
        ],
        "community_id": 27
      },
      {
        "name": "ISO/IEC TS 5723:2022",
        "type": "standard",
        "description": "ISO/IEC TS 5723:2022 defines reliability, accuracy, and safety requirements for AI systems, ensuring they do not endanger human life or the environment.",
        "chunk_ids": [
          22,
          23,
          24,
          25
        ],
        "community_id": 9
      },
      {
        "name": "AI technology",
        "type": "technology",
        "description": "AI technology includes tools and systems that utilize artificial intelligence to perform tasks typically requiring human intelligence.",
        "chunk_ids": [
          22,
          43,
          65
        ],
        "community_id": 11
      },
      {
        "name": "negative AI risks",
        "type": "risk",
        "description": "Negative AI risks refer to the potential adverse effects and consequences that arise from the deployment of AI systems.",
        "chunk_ids": [
          22
        ],
        "community_id": 11
      },
      {
        "name": "accuracy",
        "type": "function",
        "description": "The closeness of results of observations, computations, or estimates to the true values.",
        "chunk_ids": [
          23
        ],
        "community_id": 9
      },
      {
        "name": "robustness",
        "type": "function",
        "description": "The ability of a system to maintain its level of performance under a variety of circumstances.",
        "chunk_ids": [
          23
        ],
        "community_id": 9
      },
      {
        "name": "AI risk management",
        "type": "practice",
        "description": "A practice focused on minimizing potential negative impacts of AI systems, including the need for human intervention in certain cases.",
        "chunk_ids": [
          24
        ],
        "community_id": 10
      },
      {
        "name": "safety risks",
        "type": "risk",
        "description": "Safety risks associated with AI systems can lead to serious harm, necessitating tailored management approaches to ensure they remain within acceptable limits.",
        "chunk_ids": [
          24,
          46,
          47
        ],
        "community_id": 10
      },
      {
        "name": "NIST Cybersecurity Framework",
        "type": "guideline",
        "description": "A framework that provides guidelines for managing cybersecurity risks, applicable to AI systems.",
        "chunk_ids": [
          25
        ]
      },
      {
        "name": "NIST Risk Management Framework",
        "type": "guideline",
        "description": "A framework that outlines a structured approach to risk management, relevant to AI safety and security.",
        "chunk_ids": [
          25
        ]
      },
      {
        "name": "AI safety risk management approaches",
        "type": "category",
        "description": "Approaches that focus on managing risks associated with AI systems to ensure safety and reliability.",
        "chunk_ids": [
          25
        ],
        "community_id": 11
      },
      {
        "name": "security and resilience",
        "type": "category",
        "description": "Characteristics of AI systems that relate to their ability to withstand adverse events and maintain functionality.",
        "chunk_ids": [
          25
        ],
        "community_id": 11
      },
      {
        "name": "Resilience",
        "type": "concept",
        "description": "The ability to return to normal function after an unexpected adverse event, relating to robustness and the unexpected or adversarial use of data.",
        "chunk_ids": [
          26
        ],
        "community_id": 12
      },
      {
        "name": "Security",
        "type": "concept",
        "description": "Encompasses resilience and includes protocols to avoid, protect against, respond to, or recover from attacks.",
        "chunk_ids": [
          26
        ],
        "community_id": 12
      },
      {
        "name": "Accountability",
        "type": "concept",
        "description": "A characteristic of trustworthy AI that presupposes transparency.",
        "chunk_ids": [
          26
        ],
        "community_id": 2
      },
      {
        "name": "Transparency",
        "type": "concept",
        "description": "Transparency in AI refers to the availability of information regarding the system's operations and outputs, promoting fairness and accountability.",
        "chunk_ids": [
          26,
          30
        ],
        "community_id": 2
      },
      {
        "name": "transparency",
        "type": "category",
        "description": "Transparency in AI emphasizes clarity and openness about the mechanisms and processes involved in AI system operations.",
        "chunk_ids": [
          27,
          28
        ],
        "community_id": 9
      },
      {
        "name": "accountability",
        "type": "category",
        "description": "Accountability in the context of AI systems involves the responsibility of developers and deployers to ensure that the systems operate fairly and ethically.",
        "chunk_ids": [
          27
        ],
        "community_id": 13
      },
      {
        "name": "training data",
        "type": "resource",
        "description": "Training data refers to the datasets used to train AI systems, which can influence their performance and decision-making capabilities.",
        "chunk_ids": [
          27
        ],
        "community_id": 9
      },
      {
        "name": "explainability",
        "type": "framework",
        "description": "Explainability refers to the representation of the mechanisms underlying the operation of AI systems, helping users understand how decisions are made.",
        "chunk_ids": [
          27
        ],
        "community_id": 9
      },
      {
        "name": "interpretability",
        "type": "framework",
        "description": "Interpretability refers to the meaning of the outputs generated by AI systems in the context of their designed functional purposes.",
        "chunk_ids": [
          27
        ],
        "community_id": 9
      },
      {
        "name": "Explainable AI",
        "type": "framework",
        "description": "A framework that focuses on making AI systems understandable to users by providing insights into their functionality and trustworthiness.",
        "chunk_ids": [
          28
        ],
        "community_id": 2
      },
      {
        "name": "Interpretability",
        "type": "category",
        "description": "A characteristic of AI systems that allows users to understand the meaning or context of decisions made by the system.",
        "chunk_ids": [
          28
        ],
        "community_id": 2
      },
      {
        "name": "NIST Privacy Framework",
        "type": "document",
        "description": "A tool for improving privacy through enterprise risk management, focusing on safeguarding human autonomy, identity, and dignity.",
        "chunk_ids": [
          29
        ]
      },
      {
        "name": "Privacy-enhancing technologies (PETs)",
        "type": "technology",
        "description": "Technologies designed to protect privacy in AI systems, including methods like de-identification and aggregation.",
        "chunk_ids": [
          29
        ],
        "community_id": 11
      },
      {
        "name": "Fairness in AI",
        "type": "framework",
        "description": "A concept that includes concerns for equality and equity, addressing issues such as harmful bias and discrimination in AI systems.",
        "chunk_ids": [
          29
        ],
        "community_id": 5
      },
      {
        "name": "Bias",
        "type": "risk",
        "description": "A broader concept than demographic balance and data representativeness, which can affect fairness in AI systems.",
        "chunk_ids": [
          29
        ],
        "community_id": 5
      },
      {
        "name": "NIST Special Publication 1270",
        "type": "document",
        "description": "NIST Special Publication 1270 provides guidance on identifying and managing bias in artificial intelligence.",
        "chunk_ids": [
          30,
          31
        ],
        "community_id": 0
      },
      {
        "name": "systemic bias",
        "type": "category",
        "description": "A type of bias that can be present in AI datasets and organizational practices, reflecting broader societal norms.",
        "chunk_ids": [
          30
        ],
        "community_id": 5
      },
      {
        "name": "computational and statistical bias",
        "type": "category",
        "description": "Biases that arise from systematic errors in AI datasets and algorithmic processes, often due to non-representative samples.",
        "chunk_ids": [
          30
        ],
        "community_id": 5
      },
      {
        "name": "human-cognitive bias",
        "type": "category",
        "description": "Biases related to individual or group perceptions of AI system information, influencing decision-making processes.",
        "chunk_ids": [
          30
        ],
        "community_id": 5
      },
      {
        "name": "fairness",
        "type": "concept",
        "description": "A principle that aims to ensure equitable treatment and outcomes in the context of AI systems.",
        "chunk_ids": [
          30
        ],
        "community_id": 9
      },
      {
        "name": "AI community",
        "type": "category",
        "description": "A collective term for organizations and individuals involved in the development and management of artificial intelligence technologies.",
        "chunk_ids": [
          31
        ],
        "community_id": 5
      },
      {
        "name": "TEVV practices",
        "type": "practice",
        "description": "Practices related to Testing, Evaluation, Validation, and Verification of AI systems.",
        "chunk_ids": [
          31
        ],
        "community_id": 5
      },
      {
        "name": "AI RMF Core",
        "type": "system",
        "description": "The core component of the AI RMF that includes actions and outcomes to manage AI risks and develop trustworthy AI systems.",
        "chunk_ids": [
          32
        ],
        "community_id": 1
      },
      {
        "name": "downstream risks",
        "type": "risk",
        "description": "Risks that arise as a consequence of AI system deployment and operation.",
        "chunk_ids": [
          32
        ],
        "community_id": 1
      },
      {
        "name": "diverse team",
        "type": "category",
        "description": "A team composed of individuals with varied backgrounds and perspectives contributing to AI development.",
        "chunk_ids": [
          32
        ],
        "community_id": 1
      },
      {
        "name": "NIST AI RMF 1.0",
        "type": "framework",
        "description": "The NIST AI RMF 1.0 is a framework designed to manage AI risks and guide organizations in the responsible use of AI technologies.",
        "chunk_ids": [
          33,
          45
        ],
        "community_id": 2
      },
      {
        "name": "NIST AI RMF Playbook",
        "type": "resource",
        "description": "An online companion resource to the AI RMF that provides tactical actions for organizations to achieve outcomes related to AI risk management.",
        "chunk_ids": [
          33
        ]
      },
      {
        "name": "NIST Trustworthy and Responsible AI Resource Center",
        "type": "organization",
        "description": "A center that encompasses the AI RMF and the Playbook, focusing on promoting trustworthy and responsible AI practices.",
        "chunk_ids": [
          33
        ],
        "community_id": 14
      },
      {
        "name": "organizational culture",
        "type": "category",
        "description": "Organizational culture shapes how an organization operates and manages risk, reflecting shared values and beliefs.",
        "chunk_ids": [
          34,
          35
        ],
        "community_id": 13
      },
      {
        "name": "governing authorities",
        "type": "organization",
        "description": "Governing authorities are entities that establish policies and frameworks to guide an organization's mission, goals, and risk tolerance.",
        "chunk_ids": [
          34
        ],
        "community_id": 13
      },
      {
        "name": "senior leadership",
        "type": "person",
        "description": "Senior leadership refers to the top executives in an organization who set the tone for risk management and influence organizational culture.",
        "chunk_ids": [
          34
        ],
        "community_id": 13
      },
      {
        "name": "GOVERN function",
        "type": "function",
        "description": "The GOVERN function provides a framework for organizations to establish policies and processes for managing AI risks, clarifying roles and responsibilities within Human-AI team configurations.",
        "chunk_ids": [
          35,
          36,
          40,
          43,
          62,
          63
        ],
        "community_id": 14
      },
      {
        "name": "NIST AI RMF Playbook",
        "type": "document",
        "description": "The NIST AI RMF Playbook outlines practices for governing and managing AI risks, including a framework for mapping and measuring AI system impacts.",
        "chunk_ids": [
          35,
          40,
          45
        ],
        "community_id": 14
      },
      {
        "name": "organizational risk priorities",
        "type": "category",
        "description": "The priorities set by an organization to manage risks associated with its operations, particularly in AI.",
        "chunk_ids": [
          36
        ],
        "community_id": 14
      },
      {
        "name": "AI risk management training",
        "type": "practice",
        "description": "Training provided to personnel and partners to equip them with the skills needed to manage AI risks effectively.",
        "chunk_ids": [
          36
        ],
        "community_id": 14
      },
      {
        "name": "executive leadership",
        "type": "person",
        "description": "The individuals in leadership positions within the organization responsible for decision-making regarding AI risks.",
        "chunk_ids": [
          36
        ],
        "community_id": 14
      },
      {
        "name": "GOVERN 5",
        "type": "document",
        "description": "A guideline that emphasizes the importance of robust engagement with relevant AI actors.",
        "chunk_ids": [
          38
        ],
        "community_id": 1
      },
      {
        "name": "GOVERN 5.1",
        "type": "document",
        "description": "A guideline detailing the need for organizational policies to collect and integrate feedback from external sources regarding AI risks.",
        "chunk_ids": [
          38
        ],
        "community_id": 20
      },
      {
        "name": "GOVERN 5.2",
        "type": "document",
        "description": "A guideline that establishes mechanisms for incorporating feedback from relevant AI actors into system design.",
        "chunk_ids": [
          38
        ],
        "community_id": 15
      },
      {
        "name": "GOVERN 6",
        "type": "document",
        "description": "A guideline that addresses AI risks and benefits arising from third-party software and data.",
        "chunk_ids": [
          38
        ],
        "community_id": 16
      },
      {
        "name": "GOVERN 6.1",
        "type": "document",
        "description": "A guideline focused on policies addressing AI risks associated with third-party entities.",
        "chunk_ids": [
          38
        ],
        "community_id": 28
      },
      {
        "name": "GOVERN 6.2",
        "type": "document",
        "description": "A guideline that outlines contingency processes for handling failures in high-risk third-party data or AI systems.",
        "chunk_ids": [
          38
        ],
        "community_id": 17
      },
      {
        "name": "MAP function",
        "type": "function",
        "description": "The MAP function establishes the context for AI systems, identifying risks and informing decision-making related to model management and safety.",
        "chunk_ids": [
          38,
          39,
          40,
          41,
          42,
          44,
          46,
          47,
          50,
          58,
          63
        ],
        "community_id": 20
      },
      {
        "name": "AI actors",
        "type": "person",
        "description": "AI actors are individuals or entities responsible for various aspects of the AI lifecycle, ensuring the integrity and performance of AI systems.",
        "chunk_ids": [
          38,
          55
        ],
        "community_id": 1
      },
      {
        "name": "MEASURE function",
        "type": "function",
        "description": "The MEASURE function evaluates AI risks through objective assessments and analytical outputs, focusing on safety, security, and transparency.",
        "chunk_ids": [
          39,
          40,
          44,
          45,
          47,
          48,
          50
        ],
        "community_id": 20
      },
      {
        "name": "MANAGE function",
        "type": "function",
        "description": "The MANAGE function focuses on addressing and managing identified AI risks based on assessments from the MAP and MEASURE functions.",
        "chunk_ids": [
          39,
          40,
          44,
          50
        ],
        "community_id": 20
      },
      {
        "name": "internal team",
        "type": "organization",
        "description": "A diverse team within an organization that contributes to the development and deployment of AI systems.",
        "chunk_ids": [
          39
        ],
        "community_id": 10
      },
      {
        "name": "external collaborators",
        "type": "organization",
        "description": "Individuals or groups outside the internal team that engage with the team to provide perspectives on AI systems.",
        "chunk_ids": [
          39
        ],
        "community_id": 10
      },
      {
        "name": "potentially impacted communities",
        "type": "category",
        "description": "Groups of people who may be affected by the deployment and decisions of AI systems.",
        "chunk_ids": [
          39
        ],
        "community_id": 10
      },
      {
        "name": "Framework users",
        "type": "person",
        "description": "Framework users are individuals or organizations that utilize the NIST AI RMF to assess and manage risks associated with AI systems.",
        "chunk_ids": [
          40,
          49
        ],
        "community_id": 20
      },
      {
        "name": "organizational risk tolerances",
        "type": "risk",
        "description": "The levels of risk that an organization is willing to accept when deploying AI technologies.",
        "chunk_ids": [
          41
        ],
        "community_id": 18
      },
      {
        "name": "interdisciplinary AI actors",
        "type": "person",
        "description": "Individuals with diverse competencies and skills involved in the development and deployment of AI systems.",
        "chunk_ids": [
          41
        ],
        "community_id": 18
      },
      {
        "name": "scientific integrity",
        "type": "category",
        "description": "A category that encompasses the principles of maintaining accuracy and reliability in scientific research related to AI.",
        "chunk_ids": [
          42
        ],
        "community_id": 20
      },
      {
        "name": "TEVV considerations",
        "type": "category",
        "description": "Considerations related to Trustworthiness, Explainability, Verifiability, and Validity in the context of AI systems.",
        "chunk_ids": [
          42
        ],
        "community_id": 20
      },
      {
        "name": "organizational risk tolerance",
        "type": "risk",
        "description": "The level of risk that an organization is willing to accept when implementing AI systems.",
        "chunk_ids": [
          42
        ]
      },
      {
        "name": "organizational risk tolerance",
        "type": "category",
        "description": "The level of risk that an organization is willing to accept in pursuit of its objectives.",
        "chunk_ids": [
          43
        ],
        "community_id": 20
      },
      {
        "name": "AI system categorization",
        "type": "function",
        "description": "The classification of AI systems based on their capabilities and contexts.",
        "chunk_ids": [
          43
        ],
        "community_id": 20
      },
      {
        "name": "operator and practitioner proficiency",
        "type": "function",
        "description": "The skills and knowledge required for effective operation and trustworthiness of AI systems.",
        "chunk_ids": [
          43
        ],
        "community_id": 19
      },
      {
        "name": "third-party software and data",
        "type": "resource",
        "description": "External software and data sources that may be integrated into AI systems.",
        "chunk_ids": [
          43
        ],
        "community_id": 11
      },
      {
        "name": "internal risk controls",
        "type": "system",
        "description": "Measures and protocols established to manage and mitigate risks within AI systems.",
        "chunk_ids": [
          43
        ],
        "community_id": 29
      },
      {
        "name": "impacts to individuals, groups, communities, organizations, and society",
        "type": "outcome",
        "description": "The effects that AI systems may have on various stakeholders and societal structures.",
        "chunk_ids": [
          43
        ],
        "community_id": 9
      },
      {
        "name": "TEVV processes",
        "type": "methodology",
        "description": "Test, evaluation, verification, and validation processes that ensure the effectiveness of AI systems.",
        "chunk_ids": [
          44
        ],
        "community_id": 20
      },
      {
        "name": "trustworthy characteristics",
        "type": "category",
        "description": "Trustworthy characteristics are attributes of AI systems that ensure their reliable and ethical operation.",
        "chunk_ids": [
          44,
          46
        ],
        "community_id": 9
      },
      {
        "name": "metrics and measurement methodologies",
        "type": "methodology",
        "description": "Approaches and techniques used to quantify and assess AI risks, ensuring adherence to scientific, legal, and ethical norms.",
        "chunk_ids": [
          45
        ],
        "community_id": 20
      },
      {
        "name": "system trustworthiness",
        "type": "outcome",
        "description": "The degree to which an AI system can be trusted based on its performance and risk assessment.",
        "chunk_ids": [
          45
        ],
        "community_id": 20
      },
      {
        "name": "risk monitoring and response efforts",
        "type": "action",
        "description": "Activities aimed at overseeing and addressing identified risks in AI systems.",
        "chunk_ids": [
          45
        ],
        "community_id": 20
      },
      {
        "name": "human subject protection",
        "type": "standard",
        "description": "Requirements that ensure the safety and rights of human subjects involved in evaluations.",
        "chunk_ids": [
          46
        ],
        "community_id": 30
      },
      {
        "name": "TEVV metrics",
        "type": "resource",
        "description": "Metrics and processes employed to evaluate the effectiveness of the MEASURE function in assessing AI systems.",
        "chunk_ids": [
          47
        ],
        "community_id": 20
      },
      {
        "name": "privacy risk",
        "type": "risk",
        "description": "Risks related to the privacy of data handled by AI systems, which are examined and documented.",
        "chunk_ids": [
          47
        ],
        "community_id": 10
      },
      {
        "name": "fairness and bias",
        "type": "risk",
        "description": "Concerns regarding the equitable treatment of individuals by AI systems, which are evaluated and documented.",
        "chunk_ids": [
          47
        ],
        "community_id": 10
      },
      {
        "name": "environmental impact",
        "type": "risk",
        "description": "The effects of AI model training and management activities on the environment, which are assessed and documented.",
        "chunk_ids": [
          47
        ],
        "community_id": 10
      },
      {
        "name": "domain experts",
        "type": "person",
        "description": "Individuals with specialized knowledge in a particular area relevant to AI risk assessment.",
        "chunk_ids": [
          48
        ],
        "community_id": 20
      },
      {
        "name": "affected communities",
        "type": "organization",
        "description": "Groups of people who may be impacted by the deployment of AI systems.",
        "chunk_ids": [
          48
        ],
        "community_id": 20
      },
      {
        "name": "AI risks",
        "type": "category",
        "description": "Risks associated with the deployment and operation of AI systems, which are prioritized and managed through the MANAGE function.",
        "chunk_ids": [
          49
        ],
        "community_id": 20
      },
      {
        "name": "negative residual risks",
        "type": "risk",
        "description": "The sum of all unmitigated risks that affect downstream acquirers of AI systems and end users.",
        "chunk_ids": [
          50
        ],
        "community_id": 9
      },
      {
        "name": "non-AI alternative systems",
        "type": "technology",
        "description": "Systems or approaches that do not utilize artificial intelligence but can serve as alternatives.",
        "chunk_ids": [
          50
        ],
        "community_id": 20
      },
      {
        "name": "MANAGE 3",
        "type": "function",
        "description": "A function focused on managing AI risks and benefits from third-party entities.",
        "chunk_ids": [
          51
        ],
        "community_id": 0
      },
      {
        "name": "MANAGE 3.1",
        "type": "subdivision",
        "description": "A subcategory that involves regular monitoring of AI risks and benefits from third-party resources.",
        "chunk_ids": [
          51
        ],
        "community_id": 0
      },
      {
        "name": "MANAGE 3.2",
        "type": "subdivision",
        "description": "A subcategory that focuses on monitoring pre-trained models used in AI system development.",
        "chunk_ids": [
          51
        ],
        "community_id": 0
      },
      {
        "name": "MANAGE 4",
        "type": "function",
        "description": "A function that documents and monitors risk treatments, including response and recovery plans for identified AI risks.",
        "chunk_ids": [
          51
        ],
        "community_id": 0
      },
      {
        "name": "MANAGE 4.1",
        "type": "subdivision",
        "description": "A subcategory that implements post-deployment monitoring plans for AI systems.",
        "chunk_ids": [
          51
        ],
        "community_id": 0
      },
      {
        "name": "MANAGE 4.2",
        "type": "subdivision",
        "description": "A subcategory that integrates measurable activities for continual improvements into AI system updates.",
        "chunk_ids": [
          51
        ],
        "community_id": 0
      },
      {
        "name": "MANAGE 4.3",
        "type": "subdivision",
        "description": "A subcategory that focuses on communication processes regarding incidents and errors to relevant AI actors.",
        "chunk_ids": [
          51
        ],
        "community_id": 0
      },
      {
        "name": "AI RMF Profiles",
        "type": "framework",
        "description": "AI RMF use-case profiles are implementations of the AI RMF functions, categories, and subcategories for specific settings or applications based on requirements, risk tolerance, and resources.",
        "chunk_ids": [
          52
        ],
        "community_id": 0
      },
      {
        "name": "AI RMF temporal profiles",
        "type": "framework",
        "description": "Descriptions of either the current state or the desired, target state of specific AI risk management activities within a given sector, industry, organization, or application context.",
        "chunk_ids": [
          52
        ],
        "community_id": 0
      },
      {
        "name": "Current Profile",
        "type": "document",
        "description": "Indicates how AI is currently being managed and the related risks in terms of current outcomes.",
        "chunk_ids": [
          52
        ],
        "community_id": 21
      },
      {
        "name": "Target Profile",
        "type": "document",
        "description": "Indicates the outcomes needed to achieve the desired or target AI risk management goals.",
        "chunk_ids": [
          52
        ],
        "community_id": 21
      },
      {
        "name": "AI Development",
        "type": "function",
        "description": "AI Development encompasses tasks performed during the AI Model phase of the lifecycle, focusing on model building and interpretation.",
        "chunk_ids": [
          54
        ],
        "community_id": 1
      },
      {
        "name": "AI Deployment",
        "type": "function",
        "description": "AI Deployment involves tasks performed during the Task and Output phase, ensuring the AI system is effectively used in production.",
        "chunk_ids": [
          54
        ],
        "community_id": 1
      },
      {
        "name": "Operation and Monitoring",
        "type": "function",
        "description": "Operation and Monitoring tasks are conducted in the Application Context/Operate and Monitor phase, focusing on assessing system output and impacts.",
        "chunk_ids": [
          54
        ],
        "community_id": 1
      },
      {
        "name": "Test, Evaluation, Verification, and Validation (TEVV)",
        "type": "function",
        "description": "TEVV tasks are performed throughout the AI lifecycle to ensure the reliability and effectiveness of AI systems.",
        "chunk_ids": [
          54
        ]
      },
      {
        "name": "compliance experts",
        "type": "person",
        "description": "Compliance experts are professionals who ensure AI systems adhere to legal, regulatory, and ethical standards during their operation and monitoring.",
        "chunk_ids": [
          54,
          55
        ],
        "community_id": 9
      },
      {
        "name": "Test, Evaluation, Verification, and Validation (TEVV)",
        "type": "framework",
        "description": "A framework encompassing tasks performed throughout the AI lifecycle to ensure the reliability and compliance of AI systems.",
        "chunk_ids": [
          55
        ],
        "community_id": 9
      },
      {
        "name": "Human Factors",
        "type": "category",
        "description": "Tasks and activities that focus on human-centered design and the integration of human dynamics in the AI lifecycle.",
        "chunk_ids": [
          55
        ],
        "community_id": 20
      },
      {
        "name": "organizational management",
        "type": "organization",
        "description": "Entities responsible for overseeing and managing the implementation and operation of AI systems within organizations.",
        "chunk_ids": [
          55
        ],
        "community_id": 9
      },
      {
        "name": "Human factors professionals",
        "type": "person",
        "description": "Professionals who provide multidisciplinary skills and perspectives to understand the context of use and engage in consultative processes related to AI system design.",
        "chunk_ids": [
          56
        ],
        "community_id": 1
      },
      {
        "name": "Domain Expert",
        "type": "person",
        "description": "Practitioners or scholars who provide knowledge or expertise in a specific industry or application area where an AI system is used.",
        "chunk_ids": [
          56
        ],
        "community_id": 1
      },
      {
        "name": "AI Impact Assessment",
        "type": "function",
        "description": "Tasks that involve assessing and evaluating requirements for AI system accountability and examining the impacts of AI systems.",
        "chunk_ids": [
          56
        ],
        "community_id": 1
      },
      {
        "name": "Procurement tasks",
        "type": "function",
        "description": "Tasks conducted by AI actors with authority for the acquisition of AI models, products, or services from third-party developers.",
        "chunk_ids": [
          56
        ],
        "community_id": 1
      },
      {
        "name": "Governance and Oversight tasks",
        "type": "function",
        "description": "Tasks assumed by AI actors with management and legal authority responsible for the organization in which an AI system is designed and deployed.",
        "chunk_ids": [
          56
        ],
        "community_id": 1
      },
      {
        "name": "Third-party entities",
        "type": "organization",
        "description": "Providers, developers, vendors, and evaluators of data, algorithms, models, and systems that are external to the organization acquiring their technologies or services.",
        "chunk_ids": [
          57
        ],
        "community_id": 31
      },
      {
        "name": "Affected individuals/communities",
        "type": "person",
        "description": "Individuals, groups, communities, or organizations directly or indirectly affected by AI systems or decisions based on their outputs.",
        "chunk_ids": [
          57
        ],
        "community_id": 9
      },
      {
        "name": "Other AI actors",
        "type": "organization",
        "description": "Entities that provide norms or guidance for specifying and managing AI risks, including trade associations, standards organizations, advocacy groups, and civil society organizations.",
        "chunk_ids": [
          57
        ],
        "community_id": 20
      },
      {
        "name": "General public",
        "type": "person",
        "description": "Individuals, communities, and consumers who directly experience the impacts of AI technologies and may motivate actions taken by AI actors.",
        "chunk_ids": [
          57
        ],
        "community_id": 2
      },
      {
        "name": "AI RMF 1",
        "type": "framework",
        "description": "The first version of the AI Risk Management Framework developed by NIST.",
        "chunk_ids": [
          57
        ],
        "community_id": 0
      },
      {
        "name": "AI-based technology",
        "type": "technology",
        "description": "Technologies that utilize artificial intelligence to perform tasks that typically require human intelligence.",
        "chunk_ids": [
          58
        ],
        "community_id": 22
      },
      {
        "name": "traditional software",
        "type": "category",
        "description": "Software systems that do not incorporate artificial intelligence and follow conventional programming paradigms.",
        "chunk_ids": [
          58
        ],
        "community_id": 22
      },
      {
        "name": "pre-trained models",
        "type": "resource",
        "description": "AI models that have been previously trained on a dataset and can be fine-tuned for specific tasks.",
        "chunk_ids": [
          58
        ],
        "community_id": 9
      },
      {
        "name": "transfer learning",
        "type": "methodology",
        "description": "Transfer learning is a machine learning technique that reuses a model developed for one task as the starting point for a model on a different task.",
        "chunk_ids": [
          58,
          61
        ],
        "community_id": 9
      },
      {
        "name": "data quality issues",
        "type": "risk",
        "description": "Problems related to the accuracy, completeness, and reliability of data used in AI systems, which can affect their trustworthiness.",
        "chunk_ids": [
          58
        ],
        "community_id": 22
      },
      {
        "name": "Privacy and cybersecurity risk management",
        "type": "practice",
        "description": "Approaches and considerations for managing privacy and cybersecurity risks in the context of AI system design and deployment.",
        "chunk_ids": [
          59
        ],
        "community_id": 9
      },
      {
        "name": "enterprise risk management",
        "type": "framework",
        "description": "A comprehensive approach to identifying, assessing, and managing risks across an organization, including those related to AI.",
        "chunk_ids": [
          59
        ],
        "community_id": 9
      },
      {
        "name": "NIST Cybersecurity Framework",
        "type": "framework",
        "description": "A framework that provides guidelines for managing cybersecurity risks, focusing on securing and resilient practices.",
        "chunk_ids": [
          60
        ],
        "community_id": 11
      },
      {
        "name": "NIST Privacy Framework",
        "type": "framework",
        "description": "A framework designed to help organizations manage privacy risks and enhance privacy protections.",
        "chunk_ids": [
          60
        ],
        "community_id": 11
      },
      {
        "name": "NIST Risk Management Framework",
        "type": "framework",
        "description": "A framework that provides a structured process for managing risks associated with information systems.",
        "chunk_ids": [
          60
        ],
        "community_id": 11
      },
      {
        "name": "Secure Software Development Framework",
        "type": "framework",
        "description": "A framework that outlines best practices for developing secure software to mitigate security risks.",
        "chunk_ids": [
          60
        ],
        "community_id": 11
      },
      {
        "name": "MAP, MEASURE, and MANAGE functions",
        "type": "function",
        "description": "Functions within the AI RMF that focus on managing and measuring risks associated with AI systems.",
        "chunk_ids": [
          60
        ],
        "community_id": 11
      },
      {
        "name": "harmful bias",
        "type": "risk",
        "description": "A risk associated with AI systems that leads to unfair or discriminatory outcomes.",
        "chunk_ids": [
          60
        ],
        "community_id": 9
      },
      {
        "name": "generative AI",
        "type": "technology",
        "description": "A type of AI that can generate new content or data based on learned patterns.",
        "chunk_ids": [
          60
        ],
        "community_id": 9
      },
      {
        "name": "machine learning attacks",
        "type": "risk",
        "description": "Security threats that exploit vulnerabilities in machine learning models.",
        "chunk_ids": [
          60
        ],
        "community_id": 9
      },
      {
        "name": "third-party AI technologies",
        "type": "technology",
        "description": "AI technologies developed by external organizations that may pose additional risks.",
        "chunk_ids": [
          60
        ]
      },
      {
        "name": "human roles and responsibilities",
        "type": "category",
        "description": "The defined roles and responsibilities of humans in the context of decision-making and oversight of AI systems.",
        "chunk_ids": [
          61
        ],
        "community_id": 9
      },
      {
        "name": "third-party AI technologies",
        "type": "resource",
        "description": "AI technologies developed by external organizations that may pose risks when integrated into an organization's systems.",
        "chunk_ids": [
          61
        ],
        "community_id": 9
      },
      {
        "name": "video compression models",
        "type": "technology",
        "description": "AI models specifically designed to improve the efficiency of video data compression.",
        "chunk_ids": [
          61
        ],
        "community_id": 9
      },
      {
        "name": "human decision maker",
        "type": "person",
        "description": "A human decision maker is an individual who makes decisions, potentially using AI systems as a supplementary opinion.",
        "chunk_ids": [
          62
        ],
        "community_id": 9
      },
      {
        "name": "cognitive biases",
        "type": "risk",
        "description": "Cognitive biases are systematic patterns of deviation from norm or rationality in judgment, which can affect decision-making processes in AI systems.",
        "chunk_ids": [
          62
        ],
        "community_id": 9
      },
      {
        "name": "standards, guidelines, best practices, methodologies, and tools",
        "type": "category",
        "description": "A collection of existing resources that help manage AI risks and foster awareness in the field.",
        "chunk_ids": [
          65
        ],
        "community_id": 11
      },
      {
        "name": "organizations",
        "type": "organization",
        "description": "Entities that are supported by the AI RMF to operate under applicable domestic and international legal or regulatory regimes.",
        "chunk_ids": [
          65
        ],
        "community_id": 11
      },
      {
        "name": "stakeholders",
        "type": "person",
        "description": "Individuals or groups involved in implementing AI risk management and learning from the framework.",
        "chunk_ids": [
          65
        ],
        "community_id": 11
      },
      {
        "name": "NIST.AI.100-1",
        "type": "document",
        "description": "A publication related to the AI RMF, available for free online.",
        "chunk_ids": [
          65
        ],
        "community_id": 11
      }
    ],
    "relations": [
      {
        "source": "NIST AI 100-1",
        "target": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
        "type": "includes",
        "description": "NIST AI 100-1 includes the Artificial Intelligence Risk Management Framework (AI RMF 1.0).",
        "strength": 9.0,
        "chunk_ids": [
          0
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "U.S. Department of Commerce",
        "type": "published_by",
        "description": "NIST AI 100-1 is published by the U.S. Department of Commerce.",
        "strength": 8.0,
        "chunk_ids": [
          0
        ]
      },
      {
        "source": "U.S. Department of Commerce",
        "target": "National Institute of Standards and Technology",
        "type": "manages",
        "description": "The U.S. Department of Commerce manages the National Institute of Standards and Technology.",
        "strength": 9.0,
        "chunk_ids": [
          0
        ]
      },
      {
        "source": "Gina M. Raimondo",
        "target": "U.S. Department of Commerce",
        "type": "directed_by",
        "description": "Gina M. Raimondo is the Secretary directing the U.S. Department of Commerce.",
        "strength": 10.0,
        "chunk_ids": [
          0
        ]
      },
      {
        "source": "Laurie E. Locascio",
        "target": "National Institute of Standards and Technology",
        "type": "directed_by",
        "description": "Laurie E. Locascio is the Director of the National Institute of Standards and Technology.",
        "strength": 10.0,
        "chunk_ids": [
          0
        ]
      },
      {
        "source": "AI RMF Playbook",
        "target": "NIST",
        "type": "published_by",
        "description": "The AI RMF Playbook is published by NIST.",
        "strength": 9.0,
        "chunk_ids": [
          1
        ]
      },
      {
        "source": "AI RMF Playbook",
        "target": "Version Control Table",
        "type": "includes",
        "description": "The AI RMF Playbook includes a Version Control Table to track changes.",
        "strength": 8.0,
        "chunk_ids": [
          1
        ]
      },
      {
        "source": "AI RMF Playbook",
        "target": "Foundational Information",
        "type": "subdivided_into",
        "description": "The AI RMF Playbook is subdivided into sections, including Foundational Information.",
        "strength": 7.0,
        "chunk_ids": [
          1
        ]
      },
      {
        "source": "AI RMF Playbook",
        "target": "Core and Profiles",
        "type": "subdivided_into",
        "description": "The AI RMF Playbook is subdivided into sections, including Core and Profiles.",
        "strength": 7.0,
        "chunk_ids": [
          1
        ]
      },
      {
        "source": "AI Risks and Trustworthiness",
        "target": "AI RMF Playbook",
        "type": "supports",
        "description": "The category of AI Risks and Trustworthiness supports the overall framework outlined in the AI RMF Playbook.",
        "strength": 8.0,
        "chunk_ids": [
          1
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "NIST",
        "type": "published_by",
        "description": "The AI RMF 1.0 document is published by NIST.",
        "strength": 9.0,
        "chunk_ids": [
          2,
          46
        ]
      },
      {
        "source": "AI RMF Core",
        "target": "Govern",
        "type": "composed_of",
        "description": "The AI RMF Core includes the GOVERN function as one of its components.",
        "strength": 8.5,
        "chunk_ids": [
          2,
          32
        ]
      },
      {
        "source": "AI RMF Core",
        "target": "Map",
        "type": "composed_of",
        "description": "The AI RMF Core includes the MAP function as one of its components.",
        "strength": 8.5,
        "chunk_ids": [
          2,
          32
        ]
      },
      {
        "source": "AI RMF Core",
        "target": "Measure",
        "type": "composed_of",
        "description": "The AI RMF Core includes the MEASURE function as one of its components.",
        "strength": 8.5,
        "chunk_ids": [
          2,
          32
        ]
      },
      {
        "source": "AI RMF Core",
        "target": "Manage",
        "type": "composed_of",
        "description": "The AI RMF Core includes the MANAGE function as one of its components.",
        "strength": 8.5,
        "chunk_ids": [
          2,
          32
        ]
      },
      {
        "source": "AI Risks",
        "target": "AI RMF 1.0",
        "type": "aligned_with",
        "description": "AI Risks are addressed within the AI RMF 1.0 framework.",
        "strength": 7.0,
        "chunk_ids": [
          2
        ]
      },
      {
        "source": "AI Actor Tasks",
        "target": "AI RMF Core",
        "type": "includes",
        "description": "AI Actor Tasks are included in the discussions of the AI RMF Core.",
        "strength": 7.0,
        "chunk_ids": [
          2
        ]
      },
      {
        "source": "OECD Framework for the Classification of AI systems",
        "target": "AI RMF 1.0",
        "type": "influences",
        "description": "The OECD Framework influences the design and understanding of AI systems within the AI RMF 1.0.",
        "strength": 6.0,
        "chunk_ids": [
          2
        ]
      },
      {
        "source": "AI actors",
        "target": "AI lifecycle stages",
        "type": "includes",
        "description": "AI actors are involved in various tasks throughout the AI lifecycle stages.",
        "strength": 8.0,
        "chunk_ids": [
          3
        ]
      },
      {
        "source": "trustworthy AI systems",
        "target": "trustworthiness characteristics",
        "type": "composed_of",
        "description": "Trustworthy AI systems are characterized by validity, reliability, accountability, and transparency.",
        "strength": 9.0,
        "chunk_ids": [
          3
        ]
      },
      {
        "source": "AI risk management activities",
        "target": "AI risks",
        "type": "manages",
        "description": "AI risk management activities are designed to govern and manage risks associated with AI technologies.",
        "strength": 8.0,
        "chunk_ids": [
          3
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "AI technologies",
        "type": "provides",
        "description": "NIST AI RMF 1.0 provides a framework for managing risks related to AI technologies.",
        "strength": 7.0,
        "chunk_ids": [
          3
        ]
      },
      {
        "source": "AI technologies",
        "target": "society",
        "type": "transforms",
        "description": "AI technologies have the potential to transform various aspects of society.",
        "strength": 9.0,
        "chunk_ids": [
          3
        ]
      },
      {
        "source": "AI risks",
        "target": "individuals and communities",
        "type": "negatively impacts",
        "description": "AI risks can negatively impact individuals, groups, organizations, and communities.",
        "strength": 8.0,
        "chunk_ids": [
          3
        ]
      },
      {
        "source": "AI RMF",
        "target": "AI systems",
        "type": "describes",
        "description": "The AI RMF describes the nature and functionality of AI systems.",
        "strength": 9.0,
        "chunk_ids": [
          4
        ]
      },
      {
        "source": "AI systems",
        "target": "societal dynamics",
        "type": "influenced_by",
        "description": "AI systems are influenced by societal dynamics and human behavior.",
        "strength": 8.0,
        "chunk_ids": [
          4
        ]
      },
      {
        "source": "AI risk management",
        "target": "AI systems",
        "type": "supports",
        "description": "AI risk management supports the responsible development and use of AI systems.",
        "strength": 9.0,
        "chunk_ids": [
          4
        ]
      },
      {
        "source": "AI systems",
        "target": "inequitable outcomes",
        "type": "creates_opportunities_for",
        "description": "AI systems can create opportunities for inequitable outcomes if not properly managed.",
        "strength": 7.0,
        "chunk_ids": [
          4
        ]
      },
      {
        "source": "OECD Recommendation on AI:2019",
        "target": "AI RMF",
        "type": "aligned_with",
        "description": "The OECD Recommendation on AI:2019 aligns with the principles outlined in the AI RMF.",
        "strength": 8.0,
        "chunk_ids": [
          4
        ]
      },
      {
        "source": "ISO/IEC 22989:2022",
        "target": "AI RMF",
        "type": "aligned_with",
        "description": "ISO/IEC 22989:2022 provides standards that align with the AI RMF.",
        "strength": 8.0,
        "chunk_ids": [
          4
        ]
      },
      {
        "source": "AI risk management",
        "target": "Responsible AI",
        "type": "supports",
        "description": "AI risk management drives responsible uses and practices in AI development.",
        "strength": 9.0,
        "chunk_ids": [
          5
        ]
      },
      {
        "source": "Responsible AI",
        "target": "ISO 26000:2010",
        "type": "aligned_with",
        "description": "Responsible AI practices align with the principles of social responsibility outlined in ISO 26000:2010.",
        "strength": 8.0,
        "chunk_ids": [
          5
        ]
      },
      {
        "source": "Responsible AI",
        "target": "ISO/IEC TR 24368:2022",
        "type": "aligned_with",
        "description": "Responsible AI is in accord with the professional responsibility defined by ISO/IEC TR 24368:2022.",
        "strength": 8.0,
        "chunk_ids": [
          5
        ]
      },
      {
        "source": "National Artificial Intelligence Initiative Act of 2020",
        "target": "AI risk management",
        "type": "directed_by",
        "description": "The act directs the development of AI initiatives, including risk management practices.",
        "strength": 7.0,
        "chunk_ids": [
          5
        ]
      },
      {
        "source": "AI RMF",
        "target": "National Artificial Intelligence Initiative Act of 2020",
        "type": "develops",
        "description": "The AI RMF is developed as a resource directed by the National Artificial Intelligence Initiative Act.",
        "strength": 9.0,
        "chunk_ids": [
          6
        ]
      },
      {
        "source": "AI RMF",
        "target": "AI actors",
        "type": "supports",
        "description": "The AI RMF supports AI actors by providing frameworks for managing risks and enhancing the trustworthiness of AI systems.",
        "strength": 8.5,
        "chunk_ids": [
          6,
          17
        ]
      },
      {
        "source": "AI RMF",
        "target": "NIST",
        "type": "aligned_with",
        "description": "NIST aligns the AI RMF with applicable international standards and guidelines.",
        "strength": 8.0,
        "chunk_ids": [
          6
        ]
      },
      {
        "source": "OECD",
        "target": "AI actors",
        "type": "defines",
        "description": "OECD defines AI actors as those involved in the AI system lifecycle.",
        "strength": 7.0,
        "chunk_ids": [
          6
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "NIST",
        "type": "authored_by",
        "description": "The AI RMF 1.0 document is authored by NIST.",
        "strength": 9.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "AI RMF Playbook",
        "type": "includes",
        "description": "The AI RMF 1.0 includes additional resources provided in the AI RMF Playbook.",
        "strength": 8.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "National AI Initiative Act of 2020",
        "type": "aligned_with",
        "description": "The development of the AI RMF is aligned with the objectives of the National AI Initiative Act of 2020.",
        "strength": 7.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "National Security Commission on Artificial Intelligence",
        "type": "aligned_with",
        "description": "The AI RMF is consistent with recommendations from the National Security Commission on Artificial Intelligence.",
        "strength": 7.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "GOVERN",
        "type": "composed_of",
        "description": "The AI RMF 1.0 framework is composed of the function GOVERN, among others.",
        "strength": 8.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "MAP",
        "type": "composed_of",
        "description": "The AI RMF 1.0 framework includes the function MAP as part of its structure.",
        "strength": 8.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "MEASURE",
        "type": "composed_of",
        "description": "The AI RMF 1.0 framework includes the function MEASURE to evaluate AI risks.",
        "strength": 8.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "MANAGE",
        "type": "composed_of",
        "description": "The AI RMF 1.0 framework includes the function MANAGE for overseeing AI risk management.",
        "strength": 8.0,
        "chunk_ids": [
          7
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "NIST",
        "type": "develops",
        "description": "NIST develops the AI RMF 1.0 framework to guide AI risk management.",
        "strength": 9.0,
        "chunk_ids": [
          8
        ]
      },
      {
        "source": "National AI Initiative Act of 2020",
        "target": "National Security Commission on Artificial Intelligence",
        "type": "aligned_with",
        "description": "The National AI Initiative Act of 2020 aligns with the recommendations of the National Security Commission on Artificial Intelligence.",
        "strength": 8.0,
        "chunk_ids": [
          8
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "AI Risk Management Framework Roadmap",
        "type": "supports",
        "description": "The AI RMF 1.0 supports the development of the AI Risk Management Framework Roadmap.",
        "strength": 7.0,
        "chunk_ids": [
          8
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "ISO 31000:2018",
        "type": "infused_throughout",
        "description": "The principles of ISO 31000:2018 are infused throughout the AI RMF 1.0 framework.",
        "strength": 8.0,
        "chunk_ids": [
          8
        ]
      },
      {
        "source": "AI RMF",
        "target": "AI systems",
        "type": "supports",
        "description": "The AI RMF supports the management of risks associated with AI systems to enhance their trustworthiness.",
        "strength": 9.0,
        "chunk_ids": [
          9
        ]
      },
      {
        "source": "risk management",
        "target": "ISO 31000:2018",
        "type": "aligned_with",
        "description": "Risk management processes are aligned with the guidelines provided by ISO 31000:2018.",
        "strength": 8.0,
        "chunk_ids": [
          9
        ]
      },
      {
        "source": "risk management",
        "target": "individuals, communities, and society",
        "type": "provides",
        "description": "Risk management provides frameworks to understand and mitigate impacts on individuals, communities, and society.",
        "strength": 7.0,
        "chunk_ids": [
          9
        ]
      },
      {
        "source": "AI RMF",
        "target": "risk management",
        "type": "develops",
        "description": "The AI RMF develops approaches to enhance risk management specifically for AI technologies.",
        "strength": 8.0,
        "chunk_ids": [
          9
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "AI RMF 1.0",
        "type": "published_by",
        "description": "NIST AI 100-1 outlines and publishes the AI RMF 1.0 framework for managing AI risks.",
        "strength": 9.3,
        "chunk_ids": [
          10,
          14,
          25,
          30,
          39,
          41,
          42,
          43,
          58,
          59,
          61
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "AI systems",
        "type": "supports",
        "description": "The AI RMF 1.0 framework supports the responsible management of risks associated with AI systems through structured guidelines.",
        "strength": 8.8,
        "chunk_ids": [
          10,
          15,
          59,
          61
        ]
      },
      {
        "source": "third-party software, hardware, and data",
        "target": "risk measurement challenges",
        "type": "contributes_to",
        "description": "Third-party resources contribute to the challenges faced in measuring AI risks effectively.",
        "strength": 7.0,
        "chunk_ids": [
          10
        ]
      },
      {
        "source": "AI systems",
        "target": "risk measurement challenges",
        "type": "illustrates",
        "description": "The complexities of AI systems illustrate the challenges in defining and measuring associated risks.",
        "strength": 8.0,
        "chunk_ids": [
          10
        ]
      },
      {
        "source": "AI system",
        "target": "risk metrics",
        "type": "aligned_with",
        "description": "The AI system's risk metrics may not align with those used by organizations deploying the system.",
        "strength": 7.0,
        "chunk_ids": [
          11
        ]
      },
      {
        "source": "organization developing the AI system",
        "target": "methodologies",
        "type": "operates_under",
        "description": "The organization developing the AI system may use specific methodologies for risk measurement.",
        "strength": 6.0,
        "chunk_ids": [
          11
        ]
      },
      {
        "source": "third-party data",
        "target": "AI products or services",
        "type": "integrated_into",
        "description": "Third-party data can be integrated into AI products or services, affecting their risk profile.",
        "strength": 8.0,
        "chunk_ids": [
          11
        ]
      },
      {
        "source": "emergent risks",
        "target": "risk management efforts",
        "type": "supports",
        "description": "Identifying and tracking emergent risks enhances organizations' risk management efforts.",
        "strength": 9.0,
        "chunk_ids": [
          11
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "AI RMF 1.0",
        "type": "composed_of",
        "description": "NIST AI 100-1 includes the AI RMF 1.0 framework as part of its guidelines for AI risk management.",
        "strength": 8.0,
        "chunk_ids": [
          11,
          24,
          28
        ]
      },
      {
        "source": "reliable metrics",
        "target": "risk measurement challenge",
        "type": "contributes_to",
        "description": "The lack of reliable metrics contributes to challenges in measuring risk and trustworthiness in AI.",
        "strength": 7.0,
        "chunk_ids": [
          11
        ]
      },
      {
        "source": "impacts on a population",
        "target": "methodologies",
        "type": "applicable_to",
        "description": "Methodologies for measuring impacts on a population are applicable to understanding AI harms.",
        "strength": 8.0,
        "chunk_ids": [
          11
        ]
      },
      {
        "source": "AI developer",
        "target": "AI deployer",
        "type": "contributes_to",
        "description": "AI developers contribute to the deployment of AI systems by providing pre-trained models, which are then utilized by AI deployers.",
        "strength": 8.0,
        "chunk_ids": [
          12
        ]
      },
      {
        "source": "AI lifecycle",
        "target": "trustworthy AI system",
        "type": "supports",
        "description": "The AI lifecycle supports the development of trustworthy AI systems by outlining the necessary stages and responsibilities involved.",
        "strength": 9.0,
        "chunk_ids": [
          12
        ]
      },
      {
        "source": "inscrutability",
        "target": "risk management",
        "type": "identifies",
        "description": "Inscrutability identifies challenges in risk management for AI systems due to their opaque nature and limited explainability.",
        "strength": 7.0,
        "chunk_ids": [
          12
        ]
      },
      {
        "source": "human baseline",
        "target": "risk management",
        "type": "provides",
        "description": "The human baseline provides a framework for comparing AI systems against human performance in decision-making tasks.",
        "strength": 8.0,
        "chunk_ids": [
          12
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "Risk Tolerance",
        "type": "supports",
        "description": "The AI RMF supports the prioritization of risk tolerance in AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          13
        ]
      },
      {
        "source": "Risk Tolerance",
        "target": "ISO GUIDE 73",
        "type": "aligned_with",
        "description": "Risk tolerance is influenced by guidelines established in ISO GUIDE 73.",
        "strength": 7.0,
        "chunk_ids": [
          13
        ]
      },
      {
        "source": "AI systems",
        "target": "organizations",
        "type": "applicable_to",
        "description": "AI systems are applicable to various organizations that may have different risk tolerances.",
        "strength": 9.0,
        "chunk_ids": [
          13
        ]
      },
      {
        "source": "policies and norms",
        "target": "Risk Tolerance",
        "type": "influences",
        "description": "Policies and norms established by organizations influence their risk tolerances.",
        "strength": 8.0,
        "chunk_ids": [
          13
        ]
      },
      {
        "source": "risk management framework",
        "target": "AI RMF 1.0",
        "type": "develops",
        "description": "The risk management framework is developed to augment existing practices like the AI RMF.",
        "strength": 7.0,
        "chunk_ids": [
          13
        ]
      },
      {
        "source": "civil society",
        "target": "methods",
        "type": "contributes_to",
        "description": "Civil society contributes to the development of methods for harm/cost-benefit tradeoffs in AI.",
        "strength": 6.0,
        "chunk_ids": [
          13
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "risk management culture",
        "type": "supports",
        "description": "The AI RMF 1.0 supports the development of a risk management culture within organizations by providing guidelines for risk assessment.",
        "strength": 8.0,
        "chunk_ids": [
          14
        ]
      },
      {
        "source": "risk criteria",
        "target": "risk tolerance",
        "type": "aligned_with",
        "description": "Risk criteria are aligned with the concept of risk tolerance, helping organizations define acceptable levels of risk.",
        "strength": 7.0,
        "chunk_ids": [
          14
        ]
      },
      {
        "source": "AI systems",
        "target": "risk management culture",
        "type": "applicable_to",
        "description": "AI systems are applicable to the risk management culture as they require specific risk assessments and management strategies.",
        "strength": 6.0,
        "chunk_ids": [
          14
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "residual risk",
        "type": "includes",
        "description": "AI RMF 1.0 includes the concept of residual risk, emphasizing its importance in understanding the impacts on end users.",
        "strength": 8.0,
        "chunk_ids": [
          15
        ]
      },
      {
        "source": "ISO GUIDE 73",
        "target": "residual risk",
        "type": "defines",
        "description": "ISO GUIDE 73 defines residual risk and its implications for risk management practices.",
        "strength": 9.0,
        "chunk_ids": [
          15
        ]
      },
      {
        "source": "AI actors",
        "target": "AI systems",
        "type": "manages",
        "description": "AI actors manage the risks associated with AI systems throughout their lifecycle, ensuring responsibilities are clearly defined.",
        "strength": 8.0,
        "chunk_ids": [
          15
        ]
      },
      {
        "source": "end users",
        "target": "AI systems",
        "type": "affected_by",
        "description": "End users are affected by the risks associated with AI systems, particularly in terms of potential negative impacts.",
        "strength": 10.0,
        "chunk_ids": [
          15
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "AI risks",
        "type": "supports",
        "description": "The AI RMF 1.0 supports the management of AI risks by providing a structured framework for integration into enterprise risk management.",
        "strength": 9.0,
        "chunk_ids": [
          16
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "AI RMF 1.0",
        "type": "provides",
        "description": "NIST AI 100-1 provides guidelines and standards that inform the implementation of the AI RMF 1.0.",
        "strength": 8.0,
        "chunk_ids": [
          16
        ]
      },
      {
        "source": "AI risks",
        "target": "cybersecurity",
        "type": "aligned_with",
        "description": "AI risks are aligned with cybersecurity risks, as both involve critical concerns regarding data protection and system integrity.",
        "strength": 7.0,
        "chunk_ids": [
          16
        ]
      },
      {
        "source": "AI risks",
        "target": "privacy",
        "type": "aligned_with",
        "description": "AI risks are aligned with privacy concerns, particularly regarding the use of data in AI systems.",
        "strength": 7.0,
        "chunk_ids": [
          16
        ]
      },
      {
        "source": "small to medium-sized organizations",
        "target": "AI risks",
        "type": "faces",
        "description": "Small to medium-sized organizations face unique challenges in managing AI risks due to their limited resources.",
        "strength": 6.0,
        "chunk_ids": [
          16
        ]
      },
      {
        "source": "large organizations",
        "target": "AI risks",
        "type": "manages",
        "description": "Large organizations typically have more resources to manage AI risks effectively compared to smaller organizations.",
        "strength": 8.0,
        "chunk_ids": [
          16
        ]
      },
      {
        "source": "OECD",
        "target": "AI lifecycle activities",
        "type": "develops",
        "description": "The OECD developed a framework for classifying AI lifecycle activities according to socio-technical dimensions.",
        "strength": 8.0,
        "chunk_ids": [
          17
        ]
      },
      {
        "source": "NIST",
        "target": "OECD framework",
        "type": "updated_by",
        "description": "NIST modified the OECD framework to emphasize TEVV processes in the AI lifecycle.",
        "strength": 7.0,
        "chunk_ids": [
          17
        ]
      },
      {
        "source": "AI actors",
        "target": "AI lifecycle",
        "type": "applicable_to",
        "description": "AI actors are involved in various stages of the AI lifecycle.",
        "strength": 8.0,
        "chunk_ids": [
          17
        ]
      },
      {
        "source": "TEVV processes",
        "target": "AI lifecycle",
        "type": "infused_throughout",
        "description": "TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.",
        "strength": 9.0,
        "chunk_ids": [
          17
        ]
      },
      {
        "source": "AI RMF",
        "target": "TEVV",
        "type": "supports",
        "description": "The AI RMF supports the integration of TEVV tasks throughout the AI lifecycle.",
        "strength": 9.0,
        "chunk_ids": [
          18
        ]
      },
      {
        "source": "TEVV",
        "target": "AI lifecycle",
        "type": "infused_throughout",
        "description": "TEVV tasks are integrated throughout the AI lifecycle to manage risks effectively.",
        "strength": 8.0,
        "chunk_ids": [
          18
        ]
      },
      {
        "source": "People & Planet dimension",
        "target": "AI actors",
        "type": "includes",
        "description": "The People & Planet dimension includes AI actors who inform the primary audience regarding societal impacts.",
        "strength": 7.0,
        "chunk_ids": [
          18
        ]
      },
      {
        "source": "OECD Framework for the Classification of AI systems",
        "target": "AI RMF",
        "type": "aligned_with",
        "description": "The OECD framework is aligned with the principles outlined in the AI RMF.",
        "strength": 8.0,
        "chunk_ids": [
          18
        ]
      },
      {
        "source": "Plan and Design function",
        "target": "AI system lifecycle",
        "type": "supports",
        "description": "The Plan and Design function supports the various stages of the AI system lifecycle.",
        "strength": 8.0,
        "chunk_ids": [
          19
        ]
      },
      {
        "source": "environmental groups",
        "target": "AI risk management",
        "type": "contributes_to",
        "description": "Environmental groups contribute to AI risk management by providing context and understanding of potential impacts.",
        "strength": 7.0,
        "chunk_ids": [
          19
        ]
      },
      {
        "source": "civil society organizations",
        "target": "AI risk management",
        "type": "contributes_to",
        "description": "Civil society organizations contribute to AI risk management by offering norms and guidance.",
        "strength": 7.0,
        "chunk_ids": [
          19
        ]
      },
      {
        "source": "end users",
        "target": "AI risk management",
        "type": "contributes_to",
        "description": "End users contribute to AI risk management by sharing their experiences and insights.",
        "strength": 6.0,
        "chunk_ids": [
          19
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "AI actors",
        "type": "includes",
        "description": "The AI RMF 1.0 includes diverse AI actors necessary for effective risk management.",
        "strength": 9.0,
        "chunk_ids": [
          19
        ]
      },
      {
        "source": "AI Risks and Trustworthiness",
        "target": "AI RMF 1.0",
        "type": "aligned_with",
        "description": "AI Risks and Trustworthiness aligns with the principles outlined in the AI RMF 1.0.",
        "strength": 8.0,
        "chunk_ids": [
          19
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "trustworthy AI",
        "type": "supports",
        "description": "AI RMF 1.0 provides guidance for achieving trustworthy AI.",
        "strength": 9.0,
        "chunk_ids": [
          20
        ]
      },
      {
        "source": "trustworthy AI",
        "target": "AI trustworthiness characteristics",
        "type": "includes",
        "description": "Trustworthy AI encompasses various characteristics that define its trustworthiness.",
        "strength": 8.0,
        "chunk_ids": [
          20
        ]
      },
      {
        "source": "AI trustworthiness characteristics",
        "target": "social and organizational behavior",
        "type": "reflects",
        "description": "AI trustworthiness characteristics are influenced by social and organizational behavior.",
        "strength": 7.0,
        "chunk_ids": [
          20
        ]
      },
      {
        "source": "AI trustworthiness characteristics",
        "target": "datasets",
        "type": "applicable_to",
        "description": "Datasets used by AI systems are relevant to the evaluation of AI trustworthiness characteristics.",
        "strength": 8.0,
        "chunk_ids": [
          20
        ]
      },
      {
        "source": "AI trustworthiness characteristics",
        "target": "AI models and algorithms",
        "type": "influences",
        "description": "The selection of AI models and algorithms affects the trustworthiness characteristics of AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          20
        ]
      },
      {
        "source": "human judgment",
        "target": "AI trustworthiness characteristics",
        "type": "contributes_to",
        "description": "Human judgment is essential in determining the metrics related to AI trustworthiness characteristics.",
        "strength": 9.0,
        "chunk_ids": [
          20
        ]
      },
      {
        "source": "organizations",
        "target": "AI trustworthiness",
        "type": "manages",
        "description": "Organizations manage the characteristics of AI trustworthiness while facing tradeoffs.",
        "strength": 8.0,
        "chunk_ids": [
          21
        ]
      },
      {
        "source": "interpretability",
        "target": "privacy",
        "type": "tradeoff",
        "description": "There is a tradeoff between optimizing for interpretability and achieving privacy in AI systems.",
        "strength": 7.0,
        "chunk_ids": [
          21
        ]
      },
      {
        "source": "predictive accuracy",
        "target": "interpretability",
        "type": "tradeoff",
        "description": "A tradeoff exists between predictive accuracy and interpretability in certain scenarios.",
        "strength": 7.0,
        "chunk_ids": [
          21
        ]
      },
      {
        "source": "data sparsity",
        "target": "predictive accuracy",
        "type": "affects",
        "description": "Data sparsity can negatively affect predictive accuracy when using privacy-enhancing techniques.",
        "strength": 6.0,
        "chunk_ids": [
          21
        ]
      },
      {
        "source": "subject matter experts",
        "target": "TEVV findings",
        "type": "evaluates",
        "description": "Subject matter experts evaluate TEVV findings to align them with deployment conditions.",
        "strength": 8.0,
        "chunk_ids": [
          21
        ]
      },
      {
        "source": "AI lifecycle",
        "target": "contextually sensitive evaluations",
        "type": "supports",
        "description": "Enhancing contextual awareness in the AI lifecycle supports contextually sensitive evaluations.",
        "strength": 9.0,
        "chunk_ids": [
          21
        ]
      },
      {
        "source": "trustworthiness characteristics",
        "target": "AI lifecycle",
        "type": "influences",
        "description": "Trustworthiness characteristics influence the evaluation and management of AI systems throughout the AI lifecycle.",
        "strength": 9.0,
        "chunk_ids": [
          22
        ]
      },
      {
        "source": "AI designer",
        "target": "trustworthiness characteristics",
        "type": "perceives",
        "description": "AI designers may have a different perception of trustworthiness characteristics compared to other AI actors.",
        "strength": 7.0,
        "chunk_ids": [
          22
        ]
      },
      {
        "source": "AI developer",
        "target": "trustworthiness characteristics",
        "type": "perceives",
        "description": "AI developers may interpret trustworthiness characteristics differently based on their role in the AI lifecycle.",
        "strength": 7.0,
        "chunk_ids": [
          22
        ]
      },
      {
        "source": "AI deployer",
        "target": "trustworthiness characteristics",
        "type": "perceives",
        "description": "AI deployers assess trustworthiness characteristics based on the operational context of the AI system.",
        "strength": 7.0,
        "chunk_ids": [
          22
        ]
      },
      {
        "source": "ISO 9000:2015",
        "target": "validation",
        "type": "provides",
        "description": "ISO 9000:2015 provides guidelines for validating the requirements for specific intended uses of AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          22
        ]
      },
      {
        "source": "ISO/IEC TS 5723:2022",
        "target": "reliability",
        "type": "defines",
        "description": "ISO/IEC TS 5723:2022 defines reliability as the ability of a system or product to perform as required without failure.",
        "strength": 8.5,
        "chunk_ids": [
          22,
          23
        ]
      },
      {
        "source": "AI technology",
        "target": "negative AI risks",
        "type": "creates_opportunities_for",
        "description": "The deployment of AI technology can create opportunities for negative AI risks if not managed properly.",
        "strength": 9.0,
        "chunk_ids": [
          22
        ]
      },
      {
        "source": "ISO/IEC TS 5723:2022",
        "target": "accuracy",
        "type": "defines",
        "description": "Defines accuracy as the closeness of results to true values.",
        "strength": 9.0,
        "chunk_ids": [
          23
        ]
      },
      {
        "source": "ISO/IEC TS 5723:2022",
        "target": "robustness",
        "type": "defines",
        "description": "Defines robustness as the ability to maintain performance under various circumstances.",
        "strength": 9.0,
        "chunk_ids": [
          23
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "AI RMF 1.0",
        "type": "includes",
        "description": "Includes the AI Risk Management Framework 1.0 as part of its content.",
        "strength": 8.0,
        "chunk_ids": [
          23
        ]
      },
      {
        "source": "accuracy",
        "target": "AI systems",
        "type": "contributes_to",
        "description": "Contributes to the validity and trustworthiness of AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          23
        ]
      },
      {
        "source": "robustness",
        "target": "AI systems",
        "type": "contributes_to",
        "description": "Contributes to appropriate system functionality in a broad set of conditions.",
        "strength": 8.0,
        "chunk_ids": [
          23
        ]
      },
      {
        "source": "AI systems",
        "target": "ISO/IEC TS 5723:2022",
        "type": "aligned_with",
        "description": "AI systems should comply with the safety requirements outlined in ISO/IEC TS 5723:2022 to avoid endangering human life and health.",
        "strength": 9.0,
        "chunk_ids": [
          24
        ]
      },
      {
        "source": "AI risk management",
        "target": "safety risks",
        "type": "evaluates",
        "description": "AI risk management practices evaluate and prioritize safety risks to minimize potential harms.",
        "strength": 9.0,
        "chunk_ids": [
          24
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "AI safety risk management approaches",
        "type": "supports",
        "description": "AI RMF 1.0 supports the development of AI safety risk management approaches.",
        "strength": 8.0,
        "chunk_ids": [
          25
        ]
      },
      {
        "source": "ISO/IEC TS 5723:2022",
        "target": "AI safety risk management approaches",
        "type": "aligned_with",
        "description": "ISO/IEC TS 5723:2022 provides guidelines that align with AI safety risk management approaches.",
        "strength": 7.0,
        "chunk_ids": [
          25
        ]
      },
      {
        "source": "NIST Cybersecurity Framework",
        "target": "AI safety risk management approaches",
        "type": "applicable_to",
        "description": "The NIST Cybersecurity Framework is applicable to AI safety risk management approaches.",
        "strength": 8.0,
        "chunk_ids": [
          25
        ]
      },
      {
        "source": "NIST Risk Management Framework",
        "target": "AI safety risk management approaches",
        "type": "applicable_to",
        "description": "The NIST Risk Management Framework is applicable to AI safety risk management approaches.",
        "strength": 8.0,
        "chunk_ids": [
          25
        ]
      },
      {
        "source": "security and resilience",
        "target": "AI safety risk management approaches",
        "type": "includes",
        "description": "Security and resilience are included as key considerations in AI safety risk management approaches.",
        "strength": 9.0,
        "chunk_ids": [
          25
        ]
      },
      {
        "source": "Resilience",
        "target": "Security",
        "type": "aligned_with",
        "description": "Resilience is a component of security, which also includes additional protocols.",
        "strength": 9.0,
        "chunk_ids": [
          26
        ]
      },
      {
        "source": "Accountability",
        "target": "Transparency",
        "type": "supports",
        "description": "Accountability in AI systems is supported by transparency.",
        "strength": 8.0,
        "chunk_ids": [
          26
        ]
      },
      {
        "source": "Transparency",
        "target": "AI Lifecycle",
        "type": "applicable_to",
        "description": "Transparency is relevant at various stages of the AI lifecycle.",
        "strength": 7.0,
        "chunk_ids": [
          26
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "Transparency",
        "type": "provides",
        "description": "The NIST AI RMF 1.0 document provides guidelines that promote transparency in AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          26
        ]
      },
      {
        "source": "AI systems",
        "target": "transparency",
        "type": "supports",
        "description": "AI systems benefit from transparency to enhance accountability and trustworthiness, particularly in relation to bias.",
        "strength": 8.0,
        "chunk_ids": [
          27,
          30
        ]
      },
      {
        "source": "AI systems",
        "target": "accountability",
        "type": "supports",
        "description": "AI systems require accountability measures to ensure ethical and fair operation.",
        "strength": 9.0,
        "chunk_ids": [
          27
        ]
      },
      {
        "source": "risk management",
        "target": "accountability",
        "type": "fosters",
        "description": "Risk management practices help foster accountability in the deployment of AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          27
        ]
      },
      {
        "source": "training data",
        "target": "transparency",
        "type": "supports",
        "description": "Maintaining the provenance of training data supports transparency and accountability in AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          27
        ]
      },
      {
        "source": "explainability",
        "target": "AI systems",
        "type": "contributes_to",
        "description": "Explainability contributes to a better understanding of AI systems' operations and outputs.",
        "strength": 9.0,
        "chunk_ids": [
          27
        ]
      },
      {
        "source": "interpretability",
        "target": "AI systems",
        "type": "contributes_to",
        "description": "Interpretability helps users understand the meaning of AI systems' outputs in their functional context.",
        "strength": 9.0,
        "chunk_ids": [
          27
        ]
      },
      {
        "source": "Explainable AI",
        "target": "Interpretability",
        "type": "supports",
        "description": "Explainable AI frameworks support the development of interpretability in AI systems.",
        "strength": 9.0,
        "chunk_ids": [
          28
        ]
      },
      {
        "source": "Explainable AI",
        "target": "Transparency",
        "type": "supports",
        "description": "Explainable AI frameworks promote transparency in AI systems by clarifying decision-making processes.",
        "strength": 9.0,
        "chunk_ids": [
          28
        ]
      },
      {
        "source": "Privacy",
        "target": "Explainable AI",
        "type": "aligned_with",
        "description": "Privacy norms align with the principles of explainable AI by emphasizing user autonomy and control over personal data.",
        "strength": 7.0,
        "chunk_ids": [
          28
        ]
      },
      {
        "source": "NIST Privacy Framework",
        "target": "Privacy-enhancing technologies (PETs)",
        "type": "supports",
        "description": "The NIST Privacy Framework supports the use of privacy-enhancing technologies in AI system design.",
        "strength": 8.0,
        "chunk_ids": [
          29
        ]
      },
      {
        "source": "Fairness in AI",
        "target": "Bias",
        "type": "includes",
        "description": "Fairness in AI includes the management of harmful bias and discrimination.",
        "strength": 9.0,
        "chunk_ids": [
          29
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "Fairness in AI",
        "type": "aligned_with",
        "description": "AI RMF 1.0 is aligned with the principles of fairness in AI, emphasizing the need to manage bias.",
        "strength": 7.0,
        "chunk_ids": [
          29
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "NIST Special Publication 1270",
        "type": "includes",
        "description": "NIST AI 100-1 includes references to NIST Special Publication 1270 for further information on bias.",
        "strength": 8.0,
        "chunk_ids": [
          30
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "systemic bias",
        "type": "identifies",
        "description": "AI RMF 1.0 identifies systemic bias as a major category of AI bias.",
        "strength": 8.0,
        "chunk_ids": [
          30
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "computational and statistical bias",
        "type": "identifies",
        "description": "AI RMF 1.0 identifies computational and statistical bias as a major category of AI bias.",
        "strength": 8.0,
        "chunk_ids": [
          30
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "human-cognitive bias",
        "type": "identifies",
        "description": "AI RMF 1.0 identifies human-cognitive bias as a major category of AI bias.",
        "strength": 8.0,
        "chunk_ids": [
          30
        ]
      },
      {
        "source": "AI systems",
        "target": "fairness",
        "type": "supports",
        "description": "AI systems are closely associated with the concept of fairness in relation to bias.",
        "strength": 7.0,
        "chunk_ids": [
          30
        ]
      },
      {
        "source": "NIST Special Publication 1270",
        "target": "AI RMF 1.0",
        "type": "aligned_with",
        "description": "NIST Special Publication 1270 aligns with the AI RMF 1.0 framework for managing AI risks.",
        "strength": 8.0,
        "chunk_ids": [
          31
        ]
      },
      {
        "source": "NIST",
        "target": "AI RMF 1.0",
        "type": "develops",
        "description": "NIST develops the AI RMF 1.0 framework to assist organizations in managing AI risks.",
        "strength": 9.0,
        "chunk_ids": [
          31
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "AI community",
        "type": "supports",
        "description": "The AI RMF 1.0 framework supports the AI community by providing guidelines for risk management.",
        "strength": 7.0,
        "chunk_ids": [
          31
        ]
      },
      {
        "source": "NIST",
        "target": "AI community",
        "type": "collaborates_with",
        "description": "NIST collaborates with the AI community to evaluate the effectiveness of the AI RMF.",
        "strength": 8.0,
        "chunk_ids": [
          31
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "TEVV practices",
        "type": "includes",
        "description": "The AI RMF 1.0 framework includes TEVV practices for evaluating AI systems.",
        "strength": 7.0,
        "chunk_ids": [
          31
        ]
      },
      {
        "source": "AI RMF Core",
        "target": "AI actors",
        "type": "supports",
        "description": "The AI RMF Core supports the engagement with AI actors for diverse perspectives.",
        "strength": 8.0,
        "chunk_ids": [
          32
        ]
      },
      {
        "source": "diverse team",
        "target": "AI RMF Core",
        "type": "contributes_to",
        "description": "A diverse team contributes to the AI RMF Core by providing varied perspectives.",
        "strength": 7.0,
        "chunk_ids": [
          32
        ]
      },
      {
        "source": "AI RMF Core",
        "target": "downstream risks",
        "type": "identifies",
        "description": "The AI RMF Core identifies downstream risks associated with AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          32
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "NIST AI RMF Playbook",
        "type": "composed_of",
        "description": "The NIST AI RMF Playbook is an online companion resource that is part of the NIST AI RMF framework.",
        "strength": 9.0,
        "chunk_ids": [
          33
        ]
      },
      {
        "source": "NIST AI RMF Playbook",
        "target": "NIST Trustworthy and Responsible AI Resource Center",
        "type": "available_at",
        "description": "The NIST AI RMF Playbook is available through the NIST Trustworthy and Responsible AI Resource Center.",
        "strength": 8.0,
        "chunk_ids": [
          33
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "GOVERN",
        "type": "includes",
        "description": "The AI RMF framework includes the GOVERN function as part of its risk management process.",
        "strength": 8.0,
        "chunk_ids": [
          33
        ]
      },
      {
        "source": "GOVERN",
        "target": "MAP",
        "type": "supports",
        "description": "After instituting outcomes in GOVERN, users typically start with the MAP function.",
        "strength": 7.0,
        "chunk_ids": [
          33
        ]
      },
      {
        "source": "MAP",
        "target": "MEASURE",
        "type": "fosters",
        "description": "The MAP function fosters the subsequent MEASURE function in the AI RMF process.",
        "strength": 7.0,
        "chunk_ids": [
          33
        ]
      },
      {
        "source": "MEASURE",
        "target": "MANAGE",
        "type": "fosters",
        "description": "The MEASURE function fosters the ongoing MANAGE function in the AI RMF process.",
        "strength": 7.0,
        "chunk_ids": [
          33
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "AI lifecycle",
        "type": "applicable_to",
        "description": "The NIST AI RMF framework is applicable to various stages of the AI lifecycle.",
        "strength": 8.0,
        "chunk_ids": [
          33
        ]
      },
      {
        "source": "GOVERN",
        "target": "AI RMF 1.0",
        "type": "supports",
        "description": "The GOVERN function supports the implementation of AI RMF 1.0 by integrating governance into AI risk management practices.",
        "strength": 8.0,
        "chunk_ids": [
          34
        ]
      },
      {
        "source": "GOVERN",
        "target": "AI systems",
        "type": "applicable_to",
        "description": "The GOVERN function is applicable to AI systems, ensuring that risk management practices are implemented throughout their lifecycle.",
        "strength": 9.0,
        "chunk_ids": [
          34
        ]
      },
      {
        "source": "organizational culture",
        "target": "risk management",
        "type": "fosters",
        "description": "A strong organizational culture fosters effective risk management practices within an organization.",
        "strength": 7.0,
        "chunk_ids": [
          34
        ]
      },
      {
        "source": "governing authorities",
        "target": "organizational culture",
        "type": "directed_by",
        "description": "Governing authorities direct the organizational culture by establishing overarching policies and values.",
        "strength": 8.0,
        "chunk_ids": [
          34
        ]
      },
      {
        "source": "senior leadership",
        "target": "risk management",
        "type": "influences",
        "description": "Senior leadership influences the approach to risk management within an organization by setting the tone and expectations.",
        "strength": 9.0,
        "chunk_ids": [
          34
        ]
      },
      {
        "source": "GOVERN function",
        "target": "NIST AI RMF Playbook",
        "type": "supports",
        "description": "The GOVERN function is supported by the practices described in the NIST AI RMF Playbook.",
        "strength": 8.0,
        "chunk_ids": [
          35
        ]
      },
      {
        "source": "organizational culture",
        "target": "risk management",
        "type": "aligned_with",
        "description": "Organizational culture is aligned with the principles of risk management as set by senior leadership.",
        "strength": 7.0,
        "chunk_ids": [
          35
        ]
      },
      {
        "source": "risk tolerance",
        "target": "GOVERN function",
        "type": "applicable_to",
        "description": "The risk tolerance of an organization is applicable to the activities outlined in the GOVERN function.",
        "strength": 9.0,
        "chunk_ids": [
          35
        ]
      },
      {
        "source": "AI risks",
        "target": "GOVERN function",
        "type": "managed_by",
        "description": "AI risks are managed through the processes and practices established in the GOVERN function.",
        "strength": 10.0,
        "chunk_ids": [
          35
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "AI RMF 1.0",
        "type": "aligned_with",
        "description": "NIST AI 100-1 aligns with the principles and framework of AI RMF 1.0 for effective risk management.",
        "strength": 8.0,
        "chunk_ids": [
          36,
          62
        ]
      },
      {
        "source": "GOVERN function",
        "target": "organizational risk priorities",
        "type": "supports",
        "description": "The GOVERN function supports the establishment of organizational risk priorities.",
        "strength": 9.0,
        "chunk_ids": [
          36
        ]
      },
      {
        "source": "AI risk management training",
        "target": "organizational risk priorities",
        "type": "contributes_to",
        "description": "AI risk management training contributes to achieving the organization's risk priorities.",
        "strength": 7.0,
        "chunk_ids": [
          36
        ]
      },
      {
        "source": "executive leadership",
        "target": "AI risk management training",
        "type": "manages",
        "description": "Executive leadership manages the implementation of AI risk management training within the organization.",
        "strength": 8.0,
        "chunk_ids": [
          36
        ]
      },
      {
        "source": "GOVERN 5",
        "target": "AI actors",
        "type": "supports",
        "description": "GOVERN 5 supports robust engagement with relevant AI actors.",
        "strength": 8.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "GOVERN 5.1",
        "target": "AI risks",
        "type": "addresses",
        "description": "GOVERN 5.1 addresses AI risks by integrating feedback from external sources.",
        "strength": 7.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "GOVERN 5.2",
        "target": "system design",
        "type": "contributes_to",
        "description": "GOVERN 5.2 contributes to system design by incorporating feedback from relevant AI actors.",
        "strength": 8.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "GOVERN 6",
        "target": "third-party software",
        "type": "addresses",
        "description": "GOVERN 6 addresses AI risks and benefits arising from third-party software.",
        "strength": 9.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "GOVERN 6.1",
        "target": "third-party entities",
        "type": "addresses",
        "description": "GOVERN 6.1 addresses risks associated with third-party entities.",
        "strength": 8.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "GOVERN 6.2",
        "target": "high-risk systems",
        "type": "provides",
        "description": "GOVERN 6.2 provides contingency processes for handling failures in high-risk systems.",
        "strength": 9.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "MAP function",
        "target": "AI lifecycle",
        "type": "illustrates",
        "description": "The MAP function illustrates the interdependencies within the AI lifecycle.",
        "strength": 7.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "AI actors",
        "target": "AI lifecycle",
        "type": "operates_under",
        "description": "AI actors operate under the framework of the AI lifecycle.",
        "strength": 8.0,
        "chunk_ids": [
          38
        ]
      },
      {
        "source": "MAP function",
        "target": "MEASURE function",
        "type": "supports",
        "description": "The outcomes of the MAP function support the MEASURE function in assessing AI risks.",
        "strength": 8.0,
        "chunk_ids": [
          39
        ]
      },
      {
        "source": "MAP function",
        "target": "MANAGE function",
        "type": "supports",
        "description": "The MAP function provides foundational outcomes that inform the MANAGE function.",
        "strength": 8.0,
        "chunk_ids": [
          39
        ]
      },
      {
        "source": "internal team",
        "target": "external collaborators",
        "type": "engages_with",
        "description": "The internal team engages with external collaborators to incorporate diverse perspectives in AI development.",
        "strength": 7.0,
        "chunk_ids": [
          39
        ]
      },
      {
        "source": "internal team",
        "target": "end users",
        "type": "engages_with",
        "description": "The internal team interacts with end users to understand their needs and impacts of AI systems.",
        "strength": 7.0,
        "chunk_ids": [
          39
        ]
      },
      {
        "source": "internal team",
        "target": "potentially impacted communities",
        "type": "engages_with",
        "description": "The internal team may engage with potentially impacted communities based on the risk level of AI systems.",
        "strength": 6.0,
        "chunk_ids": [
          39
        ]
      },
      {
        "source": "NIST AI RMF Playbook",
        "target": "MAP function",
        "type": "includes",
        "description": "The NIST AI RMF Playbook includes the MAP function as part of its guidelines for managing AI risks.",
        "strength": 9.0,
        "chunk_ids": [
          40
        ]
      },
      {
        "source": "Framework users",
        "target": "MAP function",
        "type": "utilizes",
        "description": "Framework users utilize the MAP function to inform decisions about AI system design and deployment.",
        "strength": 8.0,
        "chunk_ids": [
          40
        ]
      },
      {
        "source": "MAP function",
        "target": "AI risks",
        "type": "identifies",
        "description": "The MAP function identifies AI risks to help organizations understand potential negative impacts.",
        "strength": 8.0,
        "chunk_ids": [
          40
        ]
      },
      {
        "source": "GOVERN function",
        "target": "AI risks",
        "type": "manages",
        "description": "The GOVERN function manages AI risks through the implementation of policies and procedures.",
        "strength": 7.0,
        "chunk_ids": [
          40
        ]
      },
      {
        "source": "MEASURE function",
        "target": "AI risks",
        "type": "evaluates",
        "description": "The MEASURE function evaluates AI risks through various metrics and methodologies as part of an overall risk management strategy.",
        "strength": 8.0,
        "chunk_ids": [
          40,
          45,
          48
        ]
      },
      {
        "source": "MANAGE function",
        "target": "AI risks",
        "type": "supports",
        "description": "The MANAGE function supports organizations in their efforts to mitigate AI risks.",
        "strength": 7.0,
        "chunk_ids": [
          40
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "MAP function",
        "type": "includes",
        "description": "The AI RMF 1.0 includes the MAP function, which is essential for establishing context and categorizing AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          41,
          42,
          46
        ]
      },
      {
        "source": "MAP function",
        "target": "AI system",
        "type": "applicable_to",
        "description": "The MAP function is applicable to AI systems for ensuring context understanding and facilitating categorization and assessment.",
        "strength": 8.0,
        "chunk_ids": [
          41,
          42
        ]
      },
      {
        "source": "interdisciplinary AI actors",
        "target": "organizational risk tolerances",
        "type": "evaluates",
        "description": "Interdisciplinary AI actors evaluate and document the organizational risk tolerances related to AI technologies.",
        "strength": 7.0,
        "chunk_ids": [
          41
        ]
      },
      {
        "source": "MAP function",
        "target": "scientific integrity",
        "type": "supports",
        "description": "The MAP function supports the identification and documentation of scientific integrity considerations.",
        "strength": 7.0,
        "chunk_ids": [
          42
        ]
      },
      {
        "source": "MAP function",
        "target": "TEVV considerations",
        "type": "supports",
        "description": "The MAP function supports the identification and documentation of TEVV considerations.",
        "strength": 7.0,
        "chunk_ids": [
          42
        ]
      },
      {
        "source": "MAP function",
        "target": "organizational risk tolerance",
        "type": "evaluates",
        "description": "The MAP function evaluates potential costs related to organizational risk tolerance in AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          42
        ]
      },
      {
        "source": "AI system categorization",
        "target": "organizational risk tolerance",
        "type": "aligned_with",
        "description": "The categorization of AI systems is aligned with the organization's risk tolerance.",
        "strength": 8.0,
        "chunk_ids": [
          43
        ]
      },
      {
        "source": "operator and practitioner proficiency",
        "target": "AI system performance and trustworthiness",
        "type": "supports",
        "description": "Proficiency in operation supports the performance and trustworthiness of AI systems.",
        "strength": 9.0,
        "chunk_ids": [
          43
        ]
      },
      {
        "source": "GOVERN function",
        "target": "human oversight processes",
        "type": "operates_under",
        "description": "Human oversight processes operate under the governance established by the GOVERN function.",
        "strength": 7.0,
        "chunk_ids": [
          43
        ]
      },
      {
        "source": "AI technology",
        "target": "third-party software and data",
        "type": "includes",
        "description": "AI technology includes the use of third-party software and data.",
        "strength": 8.0,
        "chunk_ids": [
          43
        ]
      },
      {
        "source": "internal risk controls",
        "target": "AI system components",
        "type": "manages",
        "description": "Internal risk controls manage risks associated with components of AI systems.",
        "strength": 9.0,
        "chunk_ids": [
          43
        ]
      },
      {
        "source": "impacts to individuals, groups, communities, organizations, and society",
        "target": "AI systems",
        "type": "evaluates",
        "description": "The impacts of AI systems are evaluated to understand their effects on various stakeholders.",
        "strength": 8.0,
        "chunk_ids": [
          43
        ]
      },
      {
        "source": "MEASURE function",
        "target": "AI risks",
        "type": "supports",
        "description": "The MEASURE function supports the identification and analysis of AI risks.",
        "strength": 9.0,
        "chunk_ids": [
          44
        ]
      },
      {
        "source": "MAP function",
        "target": "AI actors",
        "type": "includes",
        "description": "The MAP function includes practices for engaging with relevant AI actors.",
        "strength": 8.0,
        "chunk_ids": [
          44
        ]
      },
      {
        "source": "MEASURE function",
        "target": "MANAGE function",
        "type": "informs",
        "description": "The MEASURE function informs the MANAGE function regarding AI risks.",
        "strength": 9.0,
        "chunk_ids": [
          44
        ]
      },
      {
        "source": "TEVV processes",
        "target": "MEASURE function",
        "type": "composed_of",
        "description": "TEVV processes are composed of methodologies that are part of the MEASURE function.",
        "strength": 7.0,
        "chunk_ids": [
          44
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "NIST AI 100-1",
        "type": "published_by",
        "description": "The AI RMF 1.0 framework is published as part of the NIST AI 100-1 document.",
        "strength": 10.0,
        "chunk_ids": [
          44
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "NIST AI RMF Playbook",
        "type": "published_by",
        "description": "The NIST AI RMF Playbook is published as part of the NIST AI RMF framework.",
        "strength": 9.0,
        "chunk_ids": [
          45
        ]
      },
      {
        "source": "metrics and measurement methodologies",
        "target": "AI risks",
        "type": "supports",
        "description": "Metrics and measurement methodologies support the identification and assessment of AI risks.",
        "strength": 8.0,
        "chunk_ids": [
          45
        ]
      },
      {
        "source": "system trustworthiness",
        "target": "MEASURE function",
        "type": "reflects",
        "description": "The effectiveness of the MEASURE function reflects the system trustworthiness of AI systems.",
        "strength": 7.0,
        "chunk_ids": [
          45
        ]
      },
      {
        "source": "risk monitoring and response efforts",
        "target": "MEASURE function",
        "type": "contributes_to",
        "description": "The MEASURE function contributes to risk monitoring and response efforts by providing metrics and outcomes.",
        "strength": 8.0,
        "chunk_ids": [
          45
        ]
      },
      {
        "source": "AI systems",
        "target": "trustworthy characteristics",
        "type": "evaluates",
        "description": "AI systems are evaluated for trustworthy characteristics.",
        "strength": 7.0,
        "chunk_ids": [
          46
        ]
      },
      {
        "source": "evaluations involving human subjects",
        "target": "human subject protection",
        "type": "applicable_to",
        "description": "Evaluations involving human subjects must meet human subject protection requirements.",
        "strength": 8.0,
        "chunk_ids": [
          46
        ]
      },
      {
        "source": "AI systems",
        "target": "safety risks",
        "type": "evaluates",
        "description": "AI systems are evaluated regularly for safety risks.",
        "strength": 7.0,
        "chunk_ids": [
          46
        ]
      },
      {
        "source": "AI system",
        "target": "safety risks",
        "type": "evaluates",
        "description": "The AI system is evaluated for safety risks as identified in the MAP function.",
        "strength": 9.0,
        "chunk_ids": [
          47
        ]
      },
      {
        "source": "AI system",
        "target": "privacy risk",
        "type": "evaluates",
        "description": "The privacy risk of the AI system is examined and documented.",
        "strength": 8.0,
        "chunk_ids": [
          47
        ]
      },
      {
        "source": "AI system",
        "target": "fairness and bias",
        "type": "evaluates",
        "description": "Fairness and bias associated with the AI system are evaluated and results are documented.",
        "strength": 8.0,
        "chunk_ids": [
          47
        ]
      },
      {
        "source": "AI system",
        "target": "environmental impact",
        "type": "assesses",
        "description": "The environmental impact of AI model training and management activities is assessed and documented.",
        "strength": 8.0,
        "chunk_ids": [
          47
        ]
      },
      {
        "source": "MEASURE function",
        "target": "TEVV metrics",
        "type": "evaluates",
        "description": "The effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.",
        "strength": 7.0,
        "chunk_ids": [
          47
        ]
      },
      {
        "source": "MEASURE function",
        "target": "MAP function",
        "type": "aligned_with",
        "description": "The MEASURE function includes evaluations based on risks identified in the MAP function.",
        "strength": 9.0,
        "chunk_ids": [
          47
        ]
      },
      {
        "source": "MEASURE function",
        "target": "NIST AI RMF 1.0",
        "type": "aligned_with",
        "description": "The MEASURE function is aligned with the guidelines provided in the NIST AI RMF 1.0 document.",
        "strength": 8.0,
        "chunk_ids": [
          48
        ]
      },
      {
        "source": "AI risks",
        "target": "domain experts",
        "type": "informed_by",
        "description": "Domain experts provide insights that inform the identification and tracking of AI risks.",
        "strength": 7.0,
        "chunk_ids": [
          48
        ]
      },
      {
        "source": "AI risks",
        "target": "end users",
        "type": "provides",
        "description": "End users provide feedback on AI risks based on their interactions with the systems.",
        "strength": 8.0,
        "chunk_ids": [
          48
        ]
      },
      {
        "source": "AI risks",
        "target": "affected communities",
        "type": "supports",
        "description": "Affected communities are supported in reporting problems related to AI risks.",
        "strength": 7.0,
        "chunk_ids": [
          48
        ]
      },
      {
        "source": "AI lifecycle",
        "target": "AI risks",
        "type": "includes",
        "description": "The AI lifecycle includes various stages where AI risks need to be assessed.",
        "strength": 8.0,
        "chunk_ids": [
          48
        ]
      },
      {
        "source": "MANAGE",
        "target": "GOVERN",
        "type": "aligned_with",
        "description": "The MANAGE function is defined by the parameters set in the GOVERN function.",
        "strength": 9.0,
        "chunk_ids": [
          49
        ]
      },
      {
        "source": "MANAGE",
        "target": "MAP",
        "type": "supports",
        "description": "The MAP function provides analytical output that supports the prioritization and management of AI risks in the MANAGE function.",
        "strength": 8.0,
        "chunk_ids": [
          49
        ]
      },
      {
        "source": "MANAGE",
        "target": "MEASURE",
        "type": "supports",
        "description": "The MEASURE function contributes to the assessment of AI risks, which is essential for the MANAGE function.",
        "strength": 8.0,
        "chunk_ids": [
          49
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "AI risks",
        "type": "describes",
        "description": "The NIST AI RMF 1.0 document describes practices related to managing AI risks.",
        "strength": 7.0,
        "chunk_ids": [
          49
        ]
      },
      {
        "source": "Framework users",
        "target": "MANAGE",
        "type": "operates_under",
        "description": "Framework users apply the MANAGE function to effectively manage AI risks.",
        "strength": 8.0,
        "chunk_ids": [
          49
        ]
      },
      {
        "source": "MANAGE function",
        "target": "AI risks",
        "type": "manages",
        "description": "The MANAGE function is responsible for managing AI risks based on assessments.",
        "strength": 9.0,
        "chunk_ids": [
          50
        ]
      },
      {
        "source": "AI risks",
        "target": "MAP function",
        "type": "identified_by",
        "description": "The MAP function identifies high-priority AI risks that need to be addressed.",
        "strength": 8.0,
        "chunk_ids": [
          50
        ]
      },
      {
        "source": "MEASURE function",
        "target": "AI risks",
        "type": "provides",
        "description": "The MEASURE function provides analytical outputs that inform the assessment of AI risks.",
        "strength": 7.0,
        "chunk_ids": [
          50
        ]
      },
      {
        "source": "AI systems",
        "target": "negative residual risks",
        "type": "affects",
        "description": "AI systems can contribute to negative residual risks for downstream acquirers and end users.",
        "strength": 8.0,
        "chunk_ids": [
          50
        ]
      },
      {
        "source": "AI actors",
        "target": "MANAGE function",
        "type": "informed_by",
        "description": "The planning and implementation of the MANAGE function are informed by input from relevant AI actors.",
        "strength": 8.0,
        "chunk_ids": [
          50
        ]
      },
      {
        "source": "AI risks",
        "target": "non-AI alternative systems",
        "type": "considered_with",
        "description": "Resources required to manage AI risks are considered alongside viable non-AI alternative systems.",
        "strength": 7.0,
        "chunk_ids": [
          50
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "MANAGE 3",
        "type": "includes",
        "description": "NIST AI 100-1 includes the MANAGE 3 function for managing AI risks and benefits.",
        "strength": 8.0,
        "chunk_ids": [
          51
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "MANAGE 4",
        "type": "includes",
        "description": "NIST AI 100-1 includes the MANAGE 4 function for documenting and monitoring risk treatments.",
        "strength": 8.0,
        "chunk_ids": [
          51
        ]
      },
      {
        "source": "MANAGE 3",
        "target": "MANAGE 3.1",
        "type": "subdivided_into",
        "description": "MANAGE 3 is subdivided into MANAGE 3.1 for monitoring AI risks and benefits.",
        "strength": 7.0,
        "chunk_ids": [
          51
        ]
      },
      {
        "source": "MANAGE 3",
        "target": "MANAGE 3.2",
        "type": "subdivided_into",
        "description": "MANAGE 3 is subdivided into MANAGE 3.2 for monitoring pre-trained models.",
        "strength": 7.0,
        "chunk_ids": [
          51
        ]
      },
      {
        "source": "MANAGE 4",
        "target": "MANAGE 4.1",
        "type": "subdivided_into",
        "description": "MANAGE 4 is subdivided into MANAGE 4.1 for post-deployment monitoring plans.",
        "strength": 7.0,
        "chunk_ids": [
          51
        ]
      },
      {
        "source": "MANAGE 4",
        "target": "MANAGE 4.2",
        "type": "subdivided_into",
        "description": "MANAGE 4 is subdivided into MANAGE 4.2 for continual improvements in AI system updates.",
        "strength": 7.0,
        "chunk_ids": [
          51
        ]
      },
      {
        "source": "MANAGE 4",
        "target": "MANAGE 4.3",
        "type": "subdivided_into",
        "description": "MANAGE 4 is subdivided into MANAGE 4.3 for communication regarding incidents and errors.",
        "strength": 7.0,
        "chunk_ids": [
          51
        ]
      },
      {
        "source": "AI RMF Profiles",
        "target": "AI RMF temporal profiles",
        "type": "composed_of",
        "description": "AI RMF Profiles include AI RMF temporal profiles as part of their framework.",
        "strength": 7.0,
        "chunk_ids": [
          52
        ]
      },
      {
        "source": "Current Profile",
        "target": "Target Profile",
        "type": "compares",
        "description": "Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives.",
        "strength": 8.0,
        "chunk_ids": [
          52
        ]
      },
      {
        "source": "AI RMF Profiles",
        "target": "NIST AI 100-1",
        "type": "published_by",
        "description": "AI RMF Profiles are associated with the NIST AI 100-1 document.",
        "strength": 6.0,
        "chunk_ids": [
          52
        ]
      },
      {
        "source": "AI Development",
        "target": "AI actors",
        "type": "includes",
        "description": "AI Development includes various AI actors such as machine learning experts and data scientists.",
        "strength": 9.0,
        "chunk_ids": [
          54
        ]
      },
      {
        "source": "AI Deployment",
        "target": "AI actors",
        "type": "includes",
        "description": "AI Deployment includes AI actors like system integrators and end users who are crucial for effective deployment.",
        "strength": 9.0,
        "chunk_ids": [
          54
        ]
      },
      {
        "source": "Operation and Monitoring",
        "target": "AI actors",
        "type": "includes",
        "description": "Operation and Monitoring includes AI actors such as system operators and compliance experts responsible for system assessment.",
        "strength": 9.0,
        "chunk_ids": [
          54
        ]
      },
      {
        "source": "TEVV",
        "target": "AI actors",
        "type": "includes",
        "description": "TEVV tasks involve AI actors who ensure the reliability of AI systems throughout their lifecycle.",
        "strength": 8.0,
        "chunk_ids": [
          54
        ]
      },
      {
        "source": "AI actors",
        "target": "Test, Evaluation, Verification, and Validation (TEVV)",
        "type": "supports",
        "description": "AI actors support the TEVV framework by performing necessary tasks throughout the AI lifecycle.",
        "strength": 9.0,
        "chunk_ids": [
          55
        ]
      },
      {
        "source": "Test, Evaluation, Verification, and Validation (TEVV)",
        "target": "AI systems",
        "type": "applies_to",
        "description": "The TEVV framework applies to AI systems to ensure their reliability and compliance.",
        "strength": 10.0,
        "chunk_ids": [
          55
        ]
      },
      {
        "source": "Human Factors",
        "target": "AI lifecycle",
        "type": "infused_throughout",
        "description": "Human Factors are integrated throughout the AI lifecycle to enhance user experience and system design.",
        "strength": 8.0,
        "chunk_ids": [
          55
        ]
      },
      {
        "source": "compliance experts",
        "target": "AI systems",
        "type": "evaluates",
        "description": "Compliance experts evaluate AI systems to ensure they meet legal and ethical standards.",
        "strength": 9.0,
        "chunk_ids": [
          55
        ]
      },
      {
        "source": "organizational management",
        "target": "AI systems",
        "type": "manages",
        "description": "Organizational management oversees the implementation and operation of AI systems within organizations.",
        "strength": 8.0,
        "chunk_ids": [
          55
        ]
      },
      {
        "source": "Human factors professionals",
        "target": "AI actors",
        "type": "supports",
        "description": "Human factors professionals provide support to AI actors by informing user experience design and evaluation.",
        "strength": 8.0,
        "chunk_ids": [
          56
        ]
      },
      {
        "source": "Domain Expert",
        "target": "AI Impact Assessment",
        "type": "contributes_to",
        "description": "Domain experts contribute essential guidance for AI impact assessment tasks.",
        "strength": 7.0,
        "chunk_ids": [
          56
        ]
      },
      {
        "source": "AI Impact Assessment",
        "target": "AI actors",
        "type": "includes",
        "description": "AI actors such as impact assessors and evaluators are included in AI impact assessment tasks.",
        "strength": 9.0,
        "chunk_ids": [
          56
        ]
      },
      {
        "source": "Procurement tasks",
        "target": "AI actors",
        "type": "managed_by",
        "description": "Procurement tasks are managed by AI actors with financial, legal, or policy authority.",
        "strength": 8.0,
        "chunk_ids": [
          56
        ]
      },
      {
        "source": "Governance and Oversight tasks",
        "target": "AI actors",
        "type": "operates_under",
        "description": "Governance and oversight tasks operate under the authority of AI actors responsible for organizational management.",
        "strength": 9.0,
        "chunk_ids": [
          56
        ]
      },
      {
        "source": "NIST AI RMF 1.0",
        "target": "AI actors",
        "type": "published_by",
        "description": "NIST AI RMF 1.0 is a document published by NIST that serves as a guideline for AI actors.",
        "strength": 10.0,
        "chunk_ids": [
          56
        ]
      },
      {
        "source": "AI Actors",
        "target": "Third-party entities",
        "type": "supports",
        "description": "AI actors support the involvement of third-party entities in AI design and development tasks.",
        "strength": 7.0,
        "chunk_ids": [
          57
        ]
      },
      {
        "source": "Third-party entities",
        "target": "End users",
        "type": "provides",
        "description": "Third-party entities provide technologies and services that end users utilize in AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          57
        ]
      },
      {
        "source": "Affected individuals/communities",
        "target": "AI systems",
        "type": "impacts",
        "description": "Affected individuals and communities are impacted by the decisions made based on AI systems.",
        "strength": 9.0,
        "chunk_ids": [
          57
        ]
      },
      {
        "source": "Other AI actors",
        "target": "AI risks",
        "type": "provides",
        "description": "Other AI actors provide guidance for managing AI risks.",
        "strength": 8.0,
        "chunk_ids": [
          57
        ]
      },
      {
        "source": "General public",
        "target": "AI technologies",
        "type": "experiences",
        "description": "The general public experiences both positive and negative impacts of AI technologies.",
        "strength": 9.0,
        "chunk_ids": [
          57
        ]
      },
      {
        "source": "NIST AI 100-1",
        "target": "AI RMF 1",
        "type": "published_by",
        "description": "NIST AI 100-1 is a document published by NIST related to the AI RMF.",
        "strength": 10.0,
        "chunk_ids": [
          57
        ]
      },
      {
        "source": "AI RMF 1.0",
        "target": "AI actors",
        "type": "applicable_to",
        "description": "The AI RMF 1.0 framework is applicable to AI actors involved in the development and deployment of AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          58
        ]
      },
      {
        "source": "AI-based technology",
        "target": "traditional software",
        "type": "compared_to",
        "description": "AI-based technology presents new risks compared to traditional software.",
        "strength": 7.0,
        "chunk_ids": [
          58
        ]
      },
      {
        "source": "MAP function",
        "target": "AI actors",
        "type": "supports",
        "description": "The MAP function supports AI actors in assessing risks associated with AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          58
        ]
      },
      {
        "source": "pre-trained models",
        "target": "transfer learning",
        "type": "includes",
        "description": "Pre-trained models are often utilized in transfer learning to enhance model performance.",
        "strength": 7.0,
        "chunk_ids": [
          58
        ]
      },
      {
        "source": "data quality issues",
        "target": "AI-based technology",
        "type": "affects",
        "description": "Data quality issues can negatively impact the trustworthiness of AI-based technology.",
        "strength": 9.0,
        "chunk_ids": [
          58
        ]
      },
      {
        "source": "datasets",
        "target": "AI systems",
        "type": "used_to_train",
        "description": "Datasets are used to train AI systems, but may become outdated or detached from their original context.",
        "strength": 8.0,
        "chunk_ids": [
          58
        ]
      },
      {
        "source": "Privacy and cybersecurity risk management",
        "target": "AI systems",
        "type": "applicable_to",
        "description": "Privacy and cybersecurity risk management considerations are applicable to the design and use of AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          59
        ]
      },
      {
        "source": "enterprise risk management",
        "target": "AI systems",
        "type": "includes",
        "description": "Enterprise risk management includes considerations for managing risks associated with AI systems.",
        "strength": 7.0,
        "chunk_ids": [
          59
        ]
      },
      {
        "source": "NIST Cybersecurity Framework",
        "target": "AI RMF",
        "type": "aligned_with",
        "description": "The NIST Cybersecurity Framework aligns with the AI RMF in addressing security and privacy considerations.",
        "strength": 8.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "NIST Privacy Framework",
        "target": "AI RMF",
        "type": "aligned_with",
        "description": "The NIST Privacy Framework aligns with the AI RMF to enhance privacy protections in AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "NIST Risk Management Framework",
        "target": "AI RMF",
        "type": "aligned_with",
        "description": "The NIST Risk Management Framework provides a structured approach that complements the AI RMF.",
        "strength": 8.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "Secure Software Development Framework",
        "target": "AI RMF",
        "type": "aligned_with",
        "description": "The Secure Software Development Framework shares principles that can inform the AI RMF.",
        "strength": 8.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "AI RMF",
        "target": "MAP, MEASURE, and MANAGE functions",
        "type": "includes",
        "description": "The AI RMF includes the MAP, MEASURE, and MANAGE functions to address AI system risks.",
        "strength": 9.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "AI systems",
        "target": "harmful bias",
        "type": "identifies",
        "description": "AI systems can exhibit harmful bias, which is a significant risk that needs to be managed.",
        "strength": 9.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "AI systems",
        "target": "generative AI",
        "type": "includes",
        "description": "Generative AI is a subset of AI systems that poses unique risks.",
        "strength": 7.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "AI systems",
        "target": "machine learning attacks",
        "type": "identifies",
        "description": "AI systems are vulnerable to various machine learning attacks that can compromise security.",
        "strength": 9.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "AI systems",
        "target": "third-party AI technologies",
        "type": "identifies",
        "description": "AI systems may face risks from third-party AI technologies that are outside an organization's control.",
        "strength": 8.0,
        "chunk_ids": [
          60
        ]
      },
      {
        "source": "third-party AI technologies",
        "target": "AI systems",
        "type": "risk",
        "description": "Third-party AI technologies may introduce risks when integrated into an organization's AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          61
        ]
      },
      {
        "source": "transfer learning",
        "target": "AI systems",
        "type": "methodology",
        "description": "Transfer learning is a methodology that can be applied to enhance the capabilities of AI systems.",
        "strength": 7.0,
        "chunk_ids": [
          61
        ]
      },
      {
        "source": "human roles and responsibilities",
        "target": "AI systems",
        "type": "evaluates",
        "description": "Human roles and responsibilities are evaluated to ensure effective oversight of AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          61
        ]
      },
      {
        "source": "video compression models",
        "target": "AI systems",
        "type": "function",
        "description": "Video compression models function as a specific application of AI systems that may not require human oversight.",
        "strength": 6.0,
        "chunk_ids": [
          61
        ]
      },
      {
        "source": "AI systems",
        "target": "human decision maker",
        "type": "supports",
        "description": "AI systems can support human decision makers by providing additional opinions or insights.",
        "strength": 7.0,
        "chunk_ids": [
          62
        ]
      },
      {
        "source": "cognitive biases",
        "target": "AI systems",
        "type": "influences",
        "description": "Cognitive biases can influence the design and decision-making processes within AI systems.",
        "strength": 8.0,
        "chunk_ids": [
          62
        ]
      },
      {
        "source": "GOVERN function",
        "target": "AI systems",
        "type": "provides",
        "description": "The GOVERN function provides clarity on roles and responsibilities for humans overseeing AI systems.",
        "strength": 9.0,
        "chunk_ids": [
          62
        ]
      },
      {
        "source": "GOVERN function",
        "target": "AI RMF",
        "type": "supports",
        "description": "The GOVERN function supports the AI RMF by clarifying roles and responsibilities in AI team configurations.",
        "strength": 8.0,
        "chunk_ids": [
          63
        ]
      },
      {
        "source": "MAP function",
        "target": "AI RMF",
        "type": "supports",
        "description": "The MAP function supports the AI RMF by suggesting documentation processes for AI system performance and trustworthiness.",
        "strength": 8.0,
        "chunk_ids": [
          63
        ]
      },
      {
        "source": "AI RMF",
        "target": "NIST AI 100-1",
        "type": "published_by",
        "description": "The AI RMF is published by NIST as part of their guidelines on AI risk management.",
        "strength": 9.0,
        "chunk_ids": [
          63
        ]
      },
      {
        "source": "AI RMF",
        "target": "organizations",
        "type": "supports",
        "description": "The AI RMF supports organizations' abilities to operate under applicable legal or regulatory regimes.",
        "strength": 8.0,
        "chunk_ids": [
          65
        ]
      },
      {
        "source": "AI RMF",
        "target": "standards, guidelines, best practices, methodologies, and tools",
        "type": "fosters",
        "description": "The AI RMF fosters greater awareness of existing resources for managing AI risks.",
        "strength": 7.0,
        "chunk_ids": [
          65
        ]
      },
      {
        "source": "AI RMF",
        "target": "stakeholders",
        "type": "provides",
        "description": "The AI RMF provides a framework for stakeholders to learn from implementing AI risk management.",
        "strength": 9.0,
        "chunk_ids": [
          65
        ]
      },
      {
        "source": "AI RMF",
        "target": "AI technology",
        "type": "applicable_to",
        "description": "The AI RMF is applicable to any AI technology and context-specific use cases.",
        "strength": 10.0,
        "chunk_ids": [
          65
        ]
      },
      {
        "source": "NIST.AI.100-1",
        "target": "AI RMF",
        "type": "published_by",
        "description": "The document NIST.AI.100-1 is a publication related to the AI RMF.",
        "strength": 8.0,
        "chunk_ids": [
          65
        ]
      }
    ],
    "communities": [
      {
        "id": 0,
        "topics": [
          "Subdivision",
          "Framework",
          "Person"
        ],
        "description": "Around Subdivision, Framework, Person.",
        "members": [
          "AI RMF 1",
          "AI RMF Profiles",
          "AI RMF temporal profiles",
          "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
          "Gina M. Raimondo",
          "Laurie E. Locascio",
          "MANAGE 3",
          "MANAGE 3.1",
          "MANAGE 3.2",
          "MANAGE 4",
          "MANAGE 4.1",
          "MANAGE 4.2",
          "MANAGE 4.3",
          "NIST AI 100-1",
          "NIST Special Publication 1270",
          "National Institute of Standards and Technology",
          "U.S. Department of Commerce"
        ]
      },
      {
        "id": 1,
        "topics": [
          "Function",
          "Category",
          "Person"
        ],
        "description": "Around Function, Category, Person.",
        "members": [
          "AI Actor Tasks",
          "AI Deployment",
          "AI Development",
          "AI Impact Assessment",
          "AI RMF Core",
          "AI actors",
          "AI lifecycle activities",
          "AI lifecycle stages",
          "Domain Expert",
          "GOVERN 5",
          "Govern",
          "Governance and Oversight tasks",
          "Human factors professionals",
          "Manage",
          "Map",
          "Measure",
          "OECD",
          "Operation and Monitoring",
          "People & Planet dimension",
          "Procurement tasks",
          "diverse team",
          "downstream risks"
        ]
      },
      {
        "id": 2,
        "topics": [
          "Concept",
          "Framework",
          "Technology"
        ],
        "description": "Around Concept, Framework, Technology.",
        "members": [
          "AI Lifecycle",
          "AI technologies",
          "Accountability",
          "Explainable AI",
          "GOVERN",
          "General public",
          "Interpretability",
          "MANAGE",
          "MAP",
          "MEASURE",
          "NIST AI RMF 1.0",
          "National AI Initiative Act of 2020",
          "National Security Commission on Artificial Intelligence",
          "Privacy",
          "Transparency",
          "society"
        ]
      },
      {
        "id": 3,
        "topics": [
          "Outcome",
          "Methodology"
        ],
        "description": "Around Outcome, Methodology.",
        "members": [
          "impacts on a population",
          "methodologies",
          "organization developing the AI system"
        ]
      },
      {
        "id": 4,
        "topics": [
          "Person",
          "Category",
          "System"
        ],
        "description": "Around Person, Category, System.",
        "members": [
          "AI deployer",
          "AI designer",
          "AI developer",
          "trustworthiness characteristics",
          "trustworthy AI systems"
        ]
      },
      {
        "id": 5,
        "topics": [
          "Category",
          "Framework",
          "Risk"
        ],
        "description": "Around Category, Framework, Risk.",
        "members": [
          "AI RMF 1.0",
          "AI RMF Playbook",
          "AI Risk Management Framework Roadmap",
          "AI Risks",
          "AI Risks and Trustworthiness",
          "AI community",
          "Bias",
          "Core and Profiles",
          "Fairness in AI",
          "Foundational Information",
          "ISO GUIDE 73",
          "NIST",
          "OECD framework",
          "Risk Tolerance",
          "TEVV practices",
          "Version Control Table",
          "computational and statistical bias",
          "human-cognitive bias",
          "policies and norms",
          "residual risk",
          "risk management culture",
          "risk management framework",
          "systemic bias",
          "trustworthy AI"
        ]
      },
      {
        "id": 6,
        "topics": [
          "System",
          "Function"
        ],
        "description": "Around System, Function.",
        "members": [
          "AI system lifecycle",
          "Plan and Design function"
        ]
      },
      {
        "id": 7,
        "topics": [
          "Function",
          "Technology",
          "Resource"
        ],
        "description": "Around Function, Technology, Resource.",
        "members": [
          "AI models and algorithms",
          "AI trustworthiness characteristics",
          "datasets",
          "human judgment",
          "social and organizational behavior"
        ]
      },
      {
        "id": 8,
        "topics": [
          "Resource"
        ],
        "description": "Around Resource.",
        "members": [
          "TEVV findings",
          "subject matter experts"
        ]
      },
      {
        "id": 9,
        "topics": [
          "Category",
          "Framework",
          "Risk"
        ],
        "description": "Around Category, Framework, Risk.",
        "members": [
          "AI systems",
          "Affected individuals/communities",
          "ISO/IEC TS 5723:2022",
          "Privacy and cybersecurity risk management",
          "Test, Evaluation, Verification, and Validation (TEVV)",
          "accuracy",
          "cognitive biases",
          "compliance experts",
          "data sparsity",
          "enterprise risk management",
          "explainability",
          "fairness",
          "generative AI",
          "harmful bias",
          "human decision maker",
          "human roles and responsibilities",
          "impacts to individuals, groups, communities, organizations, and society",
          "inequitable outcomes",
          "interpretability",
          "machine learning attacks",
          "negative residual risks",
          "organizational management",
          "pre-trained models",
          "predictive accuracy",
          "privacy",
          "reliability",
          "risk measurement challenges",
          "robustness",
          "societal dynamics",
          "third-party AI technologies",
          "third-party software, hardware, and data",
          "training data",
          "transfer learning",
          "transparency",
          "trustworthy characteristics",
          "video compression models"
        ]
      },
      {
        "id": 10,
        "topics": [
          "Organization",
          "Risk",
          "Standard"
        ],
        "description": "Around Organization, Risk, Standard.",
        "members": [
          "AI risk management",
          "AI system",
          "ISO 26000:2010",
          "ISO/IEC TR 24368:2022",
          "Responsible AI",
          "civil society organizations",
          "end users",
          "environmental groups",
          "environmental impact",
          "external collaborators",
          "fairness and bias",
          "internal team",
          "potentially impacted communities",
          "privacy risk",
          "risk metrics",
          "safety risks"
        ]
      },
      {
        "id": 11,
        "topics": [
          "Framework",
          "Category",
          "Standard"
        ],
        "description": "Around Framework, Category, Standard.",
        "members": [
          "AI RMF",
          "AI safety risk management approaches",
          "AI technology",
          "AI trustworthiness",
          "ISO/IEC 22989:2022",
          "MAP, MEASURE, and MANAGE functions",
          "NIST Cybersecurity Framework",
          "NIST Privacy Framework",
          "NIST Risk Management Framework",
          "NIST.AI.100-1",
          "National Artificial Intelligence Initiative Act of 2020",
          "OECD Framework for the Classification of AI systems",
          "OECD Recommendation on AI:2019",
          "Privacy-enhancing technologies (PETs)",
          "Secure Software Development Framework",
          "TEVV",
          "negative AI risks",
          "organizations",
          "security and resilience",
          "stakeholders",
          "standards, guidelines, best practices, methodologies, and tools",
          "third-party software and data"
        ]
      },
      {
        "id": 12,
        "topics": [
          "Concept"
        ],
        "description": "Around Concept.",
        "members": [
          "Resilience",
          "Security"
        ]
      },
      {
        "id": 13,
        "topics": [
          "Category",
          "Standard",
          "Organization"
        ],
        "description": "Around Category, Standard, Organization.",
        "members": [
          "ISO 31000:2018",
          "accountability",
          "governing authorities",
          "human baseline",
          "individuals, communities, and society",
          "inscrutability",
          "organizational culture",
          "risk management",
          "senior leadership"
        ]
      },
      {
        "id": 14,
        "topics": [
          "Practice",
          "Function",
          "Document"
        ],
        "description": "Around Practice, Function, Document.",
        "members": [
          "AI risk management training",
          "GOVERN function",
          "NIST AI RMF Playbook",
          "NIST Trustworthy and Responsible AI Resource Center",
          "executive leadership",
          "human oversight processes",
          "organizational risk priorities",
          "risk criteria",
          "risk tolerance"
        ]
      },
      {
        "id": 15,
        "topics": [
          "Document"
        ],
        "description": "Around Document.",
        "members": [
          "GOVERN 5.2",
          "system design"
        ]
      },
      {
        "id": 16,
        "topics": [
          "Document"
        ],
        "description": "Around Document.",
        "members": [
          "GOVERN 6",
          "third-party software"
        ]
      },
      {
        "id": 17,
        "topics": [
          "Document"
        ],
        "description": "Around Document.",
        "members": [
          "GOVERN 6.2",
          "high-risk systems"
        ]
      },
      {
        "id": 18,
        "topics": [
          "Person",
          "Risk"
        ],
        "description": "Around Person, Risk.",
        "members": [
          "interdisciplinary AI actors",
          "organizational risk tolerances"
        ]
      },
      {
        "id": 19,
        "topics": [
          "Function"
        ],
        "description": "Around Function.",
        "members": [
          "AI system performance and trustworthiness",
          "operator and practitioner proficiency"
        ]
      },
      {
        "id": 20,
        "topics": [
          "Category",
          "Function",
          "Organization"
        ],
        "description": "Around Category, Function, Organization.",
        "members": [
          "AI lifecycle",
          "AI risk management activities",
          "AI risks",
          "AI system categorization",
          "Framework users",
          "GOVERN 5.1",
          "Human Factors",
          "MANAGE function",
          "MAP function",
          "MEASURE function",
          "Other AI actors",
          "TEVV considerations",
          "TEVV metrics",
          "TEVV processes",
          "affected communities",
          "contextually sensitive evaluations",
          "cybersecurity",
          "domain experts",
          "individuals and communities",
          "large organizations",
          "metrics and measurement methodologies",
          "non-AI alternative systems",
          "organizational risk tolerance",
          "risk monitoring and response efforts",
          "scientific integrity",
          "small to medium-sized organizations",
          "system trustworthiness",
          "trustworthy AI system"
        ]
      },
      {
        "id": 21,
        "topics": [
          "Document"
        ],
        "description": "Around Document.",
        "members": [
          "Current Profile",
          "Target Profile"
        ]
      },
      {
        "id": 22,
        "topics": [
          "Technology",
          "Risk",
          "Category"
        ],
        "description": "Around Technology, Risk, Category.",
        "members": [
          "AI-based technology",
          "data quality issues",
          "traditional software"
        ]
      },
      {
        "id": 23,
        "topics": [
          "Resource"
        ],
        "description": "Around Resource.",
        "members": [
          "AI products or services",
          "third-party data"
        ]
      },
      {
        "id": 24,
        "topics": [
          "Risk"
        ],
        "description": "Around Risk.",
        "members": [
          "emergent risks",
          "risk management efforts"
        ]
      },
      {
        "id": 25,
        "topics": [
          "Category"
        ],
        "description": "Around Category.",
        "members": [
          "reliable metrics",
          "risk measurement challenge"
        ]
      },
      {
        "id": 26,
        "topics": [
          "Sector"
        ],
        "description": "Around Sector.",
        "members": [
          "civil society",
          "methods"
        ]
      },
      {
        "id": 27,
        "topics": [
          "Standard"
        ],
        "description": "Around Standard.",
        "members": [
          "ISO 9000:2015",
          "validation"
        ]
      },
      {
        "id": 28,
        "topics": [
          "Document"
        ],
        "description": "Around Document.",
        "members": [
          "GOVERN 6.1",
          "third-party entities"
        ]
      },
      {
        "id": 29,
        "topics": [
          "System"
        ],
        "description": "Around System.",
        "members": [
          "AI system components",
          "internal risk controls"
        ]
      },
      {
        "id": 30,
        "topics": [
          "Standard"
        ],
        "description": "Around Standard.",
        "members": [
          "evaluations involving human subjects",
          "human subject protection"
        ]
      },
      {
        "id": 31,
        "topics": [
          "Organization"
        ],
        "description": "Around Organization.",
        "members": [
          "AI Actors",
          "End users",
          "Third-party entities"
        ]
      }
    ]
  },
  "chunks": [
    {
      "chunk_idx": 0,
      "text": "NIST AI 100-1\nArtificial Intelligence Risk Management\nFramework (AI RMF 1.0)\n\n\nNIST AI 100-1\nArtificial Intelligence Risk Management\nFramework (AI RMF 1.0)\nThis publication is available free of charge from:\nhttps://doi.org/10.6028/NIST.AI.100-1\nJanuary 2023\nU.S. Department of Commerce\nGina M. Raimondo, Secretary\nNational Institute of Standards and Technology\nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology\n\n\nCertain commercial entities, equipment, or materials may be identified in this document in order to describe \nan experimental procedure or concept adequately. Such identification is not intended to imply recommenda-\ntion or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that \nthe entities, materials, or equipment are necessarily the best available for the purpose. \nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1\nUpdate Schedule and Versions\nThe Artificial Intelligence Risk Management Framework (AI RMF) is intended to be a living document.\nNIST will review the content and usefulness of the Framework regularly to determine if an update is appro-\npriate; a review with formal input from the AI community is expected to take place no later than 2028. The\nFramework will employ a two-number versioning system to track and identify major and minor changes. The\nfirst number will represent the generation of the AI RMF and its companion documents (e.g., 1.0) and will\nchange only with major revisions. Minor revisions will be tracked using \u201c.n\u201d after the generation number\n(e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including\nversion number, date of change, and description of change.",
      "rephrase": "NIST AI 100-1: Artificial Intelligence Risk Management Framework (AI RMF 1.0) is a publication from the U.S. Department of Commerce, released in January 2023. It is available for free at https://doi.org/10.6028/NIST.AI.100-1. The Secretary of Commerce is Gina M. Raimondo, and the Director of NIST is Laurie E. Locascio. \n\nThis document may mention specific companies, equipment, or materials to explain procedures or concepts, but this does not mean that NIST recommends or endorses them as the best options. \n\nThe AI RMF is designed to be updated regularly. NIST will assess its content and usefulness, with a formal review involving the AI community expected by 2028. The Framework will use a two-number versioning system: the first number indicates the main version (e.g., 1.0), which changes with major updates, while minor updates will be indicated by an additional number (e.g., 1.1). A Version Control Table will track all changes, including version numbers, dates, and descriptions.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A publication detailing the Artificial Intelligence Risk Management Framework (AI RMF 1.0), available free of charge."
        },
        {
          "name": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
          "type": "framework",
          "description": "A framework intended to manage risks associated with artificial intelligence, designed to be a living document that will be regularly updated."
        },
        {
          "name": "U.S. Department of Commerce",
          "type": "organization",
          "description": "A government department responsible for promoting economic growth, which oversees the National Institute of Standards and Technology."
        },
        {
          "name": "Gina M. Raimondo",
          "type": "person",
          "description": "The Secretary of the U.S. Department of Commerce."
        },
        {
          "name": "National Institute of Standards and Technology",
          "type": "organization",
          "description": "A federal agency that develops and promotes measurement standards, including those for artificial intelligence."
        },
        {
          "name": "Laurie E. Locascio",
          "type": "person",
          "description": "The Director of the National Institute of Standards and Technology and Under Secretary of Commerce for Standards and Technology."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
          "type": "includes",
          "description": "NIST AI 100-1 includes the Artificial Intelligence Risk Management Framework (AI RMF 1.0).",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "U.S. Department of Commerce",
          "type": "published_by",
          "description": "NIST AI 100-1 is published by the U.S. Department of Commerce.",
          "strength": 8
        },
        {
          "source": "U.S. Department of Commerce",
          "target": "National Institute of Standards and Technology",
          "type": "manages",
          "description": "The U.S. Department of Commerce manages the National Institute of Standards and Technology.",
          "strength": 9
        },
        {
          "source": "Gina M. Raimondo",
          "target": "U.S. Department of Commerce",
          "type": "directed_by",
          "description": "Gina M. Raimondo is the Secretary directing the U.S. Department of Commerce.",
          "strength": 10
        },
        {
          "source": "Laurie E. Locascio",
          "target": "National Institute of Standards and Technology",
          "type": "directed_by",
          "description": "Laurie E. Locascio is the Director of the National Institute of Standards and Technology.",
          "strength": 10
        }
      ],
      "chunk_time": 14.360565750001115,
      "chunk_tokens": 399,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 1,
      "text": " will be tracked using \u201c.n\u201d after the generation number\n(e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including\nversion number, date of change, and description of change. NIST plans to update the AI RMF Playbook\nfrequently. Comments on the AI RMF Playbook may be sent via email to AIframework@nist.gov at any time\nand will be reviewed and integrated on a semi-annual basis.\n\n\nTable of Contents\nExecutive Summary\n1\nPart 1: Foundational Information\n4\n1\nFraming Risk\n4\n1.1\nUnderstanding and Addressing Risks, Impacts, and Harms\n4\n1.2\nChallenges for AI Risk Management\n5\n1.2.1\nRisk Measurement\n5\n1.2.2\nRisk Tolerance\n7\n1.2.3\nRisk Prioritization\n7\n1.2.4\nOrganizational Integration and Management of Risk\n8\n2\nAudience\n9\n3\nAI Risks and Trustworthiness\n12\n3.1\nValid and Reliable\n13\n3.2\nSafe\n14\n3.3\nSecure and Resilient\n15\n3.4\nAccountable and Transparent\n15\n3.5\nExplainable and Interpretable\n16\n3.6\nPrivacy-Enhanced\n17\n3.7\nFair \u2013 with Harmful Bias Managed\n17\n4\nEffectiveness of the AI RMF\n19\nPart 2: Core and Profiles\n20\n5\nAI RMF Core\n20\n5.1\nGovern\n21\n5.2\nMap\n24\n5.3\nMeasure\n28\n5.",
      "rephrase": "Changes will be tracked using a format that includes a generation number followed by '.n' (for example, 1.1). A Version Control Table will document the history of changes, including the version number, date of change, and a description of the change. NIST intends to update the AI RMF Playbook regularly. Feedback on the AI RMF Playbook can be sent anytime to AIframework@nist.gov and will be reviewed and incorporated every six months.\n\nTable of Contents\nExecutive Summary\n1\nPart 1: Foundational Information\n4\n1\nFraming Risk\n4\n1.1\nUnderstanding and Addressing Risks, Impacts, and Harms\n4\n1.2\nChallenges for AI Risk Management\n5\n1.2.1\nRisk Measurement\n5\n1.2.2\nRisk Tolerance\n7\n1.2.3\nRisk Prioritization\n7\n1.2.4\nOrganizational Integration and Management of Risk\n8\n2\nAudience\n9\n3\nAI Risks and Trustworthiness\n12\n3.1\nValid and Reliable\n13\n3.2\nSafe\n14\n3.3\nSecure and Resilient\n15\n3.4\nAccountable and Transparent\n15\n3.5\nExplainable and Interpretable\n16\n3.6\nPrivacy-Enhanced\n17\n3.7\nFair \u2013 with Harmful Bias Managed\n17\n4\nEffectiveness of the AI RMF\n19\nPart 2: Core and Profiles\n20\n5\nAI RMF Core\n20\n5.1\nGovern\n21\n5.2\nMap\n24\n5.3\nMeasure\n28\n5.",
      "entities": [
        {
          "name": "AI RMF Playbook",
          "type": "document",
          "description": "A playbook developed by NIST that outlines the framework for managing risks associated with artificial intelligence."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "The National Institute of Standards and Technology, a U.S. federal agency that develops standards and guidelines for various technologies."
        },
        {
          "name": "Version Control Table",
          "type": "tool",
          "description": "A tool used to track changes in documents, including version numbers, dates, and descriptions of changes."
        },
        {
          "name": "AI Risks and Trustworthiness",
          "type": "category",
          "description": "A category within the AI RMF Playbook that addresses various aspects of AI risks and their trustworthiness."
        },
        {
          "name": "Foundational Information",
          "type": "section",
          "description": "A section in the AI RMF Playbook that provides essential background information on AI risk management."
        },
        {
          "name": "Core and Profiles",
          "type": "section",
          "description": "A section in the AI RMF Playbook that outlines the core components and profiles related to AI risk management."
        }
      ],
      "relations": [
        {
          "source": "AI RMF Playbook",
          "target": "NIST",
          "type": "published_by",
          "description": "The AI RMF Playbook is published by NIST.",
          "strength": 9
        },
        {
          "source": "AI RMF Playbook",
          "target": "Version Control Table",
          "type": "includes",
          "description": "The AI RMF Playbook includes a Version Control Table to track changes.",
          "strength": 8
        },
        {
          "source": "AI RMF Playbook",
          "target": "Foundational Information",
          "type": "subdivided_into",
          "description": "The AI RMF Playbook is subdivided into sections, including Foundational Information.",
          "strength": 7
        },
        {
          "source": "AI RMF Playbook",
          "target": "Core and Profiles",
          "type": "subdivided_into",
          "description": "The AI RMF Playbook is subdivided into sections, including Core and Profiles.",
          "strength": 7
        },
        {
          "source": "AI Risks and Trustworthiness",
          "target": "AI RMF Playbook",
          "type": "supports",
          "description": "The category of AI Risks and Trustworthiness supports the overall framework outlined in the AI RMF Playbook.",
          "strength": 8
        }
      ],
      "chunk_time": 14.530986917001428,
      "chunk_tokens": 370,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 2,
      "text": " RMF\n19\nPart 2: Core and Profiles\n20\n5\nAI RMF Core\n20\n5.1\nGovern\n21\n5.2\nMap\n24\n5.3\nMeasure\n28\n5.4\nManage\n31\n6\nAI RMF Profiles\n33\nAppendix A: Descriptions of AI Actor Tasks from Figures 2 and 3\n35\nAppendix B: How AI Risks Differ from Traditional Software Risks\n38\nAppendix C: AI Risk Management and Human-AI Interaction\n40\nAppendix D: Attributes of the AI RMF\n42\nList of Tables\nTable 1 Categories and subcategories for the GOVERN function.\n22\nTable 2 Categories and subcategories for the MAP function.\n26\nTable 3 Categories and subcategories for the MEASURE function.\n29\nTable 4 Categories and subcategories for the MANAGE function.\n32\ni\n\n\nNIST AI 100-1\nAI RMF 1.0\nList of Figures\nFig. 1\nExamples of potential harms related to AI systems. Trustworthy AI systems\nand their responsible use can mitigate negative risks and contribute to bene-\nfits for people, organizations, and ecosystems.\n5\nFig. 2\nLifecycle and Key Dimensions of an AI System. Modified from OECD\n(2022) OECD Framework for the Classification of AI systems \u2014 OECD\nDigital Economy Papers. The two inner circles show AI systems\u2019 key di-\nmensions and the outer circle shows AI lifecycle stages. Ideally, risk man-\nagement efforts start with the Plan and Design function in the application\ncontext and are performed throughout the AI system lifecycle. See Figure 3\nfor representative AI actors.\n10\nFig. 3\nAI actors across AI lifecycle stages. See Appendix A for detailed descrip-\ntions of AI actor tasks, including details about testing, evaluation, verifica-\ntion, and validation tasks.",
      "rephrase": "The document outlines the AI Risk Management Framework (AI RMF) and is divided into several sections. It includes the following key parts: \n\n1. **AI RMF Core**: This section covers four main functions: \n   - **Govern** (Page 21) \n   - **Map** (Page 24) \n   - **Measure** (Page 28) \n   - **Manage** (Page 31) \n\n2. **AI RMF Profiles** (Page 33) \n\n3. **Appendices**: \n   - Appendix A: Describes tasks performed by AI actors (Page 35) \n   - Appendix B: Discusses how AI risks differ from traditional software risks (Page 38) \n   - Appendix C: Covers AI risk management and human-AI interaction (Page 40) \n   - Appendix D: Lists attributes of the AI RMF (Page 42) \n\n**Tables**: \n- Table 1: Categories for the GOVERN function (Page 22) \n- Table 2: Categories for the MAP function (Page 26) \n- Table 3: Categories for the MEASURE function (Page 29) \n- Table 4: Categories for the MANAGE function (Page 32) \n\n**Figures**: \n- Figure 1: Shows potential harms related to AI systems and how trustworthy AI can reduce risks (Page 5) \n- Figure 2: Illustrates the lifecycle and key dimensions of an AI system, emphasizing the importance of risk management throughout the AI lifecycle (Page 10) \n- Figure 3: Depicts AI actors involved in different stages of the AI lifecycle, with detailed task descriptions in Appendix A.",
      "entities": [
        {
          "name": "AI RMF 1.0",
          "type": "document",
          "description": "A framework developed by NIST for managing risks associated with AI systems."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "The National Institute of Standards and Technology, responsible for developing standards and guidelines for various technologies, including AI."
        },
        {
          "name": "AI RMF Core",
          "type": "framework",
          "description": "The core components of the AI Risk Management Framework, which include functions such as Govern, Map, Measure, and Manage."
        },
        {
          "name": "Govern",
          "type": "function",
          "description": "One of the core functions of the AI RMF, focusing on establishing governance structures for AI systems."
        },
        {
          "name": "Map",
          "type": "function",
          "description": "A core function of the AI RMF that involves mapping AI risks and their impacts."
        },
        {
          "name": "Measure",
          "type": "function",
          "description": "A function within the AI RMF that focuses on measuring the effectiveness of risk management strategies."
        },
        {
          "name": "Manage",
          "type": "function",
          "description": "The function in the AI RMF that deals with managing identified risks throughout the AI system lifecycle."
        },
        {
          "name": "AI Risks",
          "type": "risk",
          "description": "Risks specifically associated with the deployment and use of AI systems, differing from traditional software risks."
        },
        {
          "name": "AI Actor Tasks",
          "type": "category",
          "description": "Tasks performed by various actors involved in the AI lifecycle, including testing, evaluation, verification, and validation."
        },
        {
          "name": "OECD Framework for the Classification of AI systems",
          "type": "framework",
          "description": "A framework developed by the OECD for classifying AI systems, referenced in the context of AI lifecycle dimensions."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "NIST",
          "type": "published_by",
          "description": "The AI RMF 1.0 document is published by NIST.",
          "strength": 9
        },
        {
          "source": "AI RMF Core",
          "target": "Govern",
          "type": "composed_of",
          "description": "The AI RMF Core is composed of several functions, including Govern.",
          "strength": 8
        },
        {
          "source": "AI RMF Core",
          "target": "Map",
          "type": "composed_of",
          "description": "The AI RMF Core includes the Map function as one of its components.",
          "strength": 8
        },
        {
          "source": "AI RMF Core",
          "target": "Measure",
          "type": "composed_of",
          "description": "The Measure function is part of the AI RMF Core framework.",
          "strength": 8
        },
        {
          "source": "AI RMF Core",
          "target": "Manage",
          "type": "composed_of",
          "description": "Manage is another function that is included in the AI RMF Core.",
          "strength": 8
        },
        {
          "source": "AI Risks",
          "target": "AI RMF 1.0",
          "type": "aligned_with",
          "description": "AI Risks are addressed within the AI RMF 1.0 framework.",
          "strength": 7
        },
        {
          "source": "AI Actor Tasks",
          "target": "AI RMF Core",
          "type": "includes",
          "description": "AI Actor Tasks are included in the discussions of the AI RMF Core.",
          "strength": 7
        },
        {
          "source": "OECD Framework for the Classification of AI systems",
          "target": "AI RMF 1.0",
          "type": "influences",
          "description": "The OECD Framework influences the design and understanding of AI systems within the AI RMF 1.0.",
          "strength": 6
        }
      ],
      "chunk_time": 21.21168620899698,
      "chunk_tokens": 400,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 3,
      "text": "\nfor representative AI actors.\n10\nFig. 3\nAI actors across AI lifecycle stages. See Appendix A for detailed descrip-\ntions of AI actor tasks, including details about testing, evaluation, verifica-\ntion, and validation tasks. Note that AI actors in the AI Model dimension\n(Figure 2) are separated as a best practice, with those building and using the\nmodels separated from those verifying and validating the models.\n11\nFig. 4\nCharacteristics of trustworthy AI systems. Valid & Reliable is a necessary\ncondition of trustworthiness and is shown as the base for other trustworthi-\nness characteristics. Accountable & Transparent is shown as a vertical box\nbecause it relates to all other characteristics.\n12\nFig. 5\nFunctions organize AI risk management activities at their highest level to\ngovern, map, measure, and manage AI risks. Governance is designed to be\na cross-cutting function to inform and be infused throughout the other three\nfunctions.\n20\nPage ii\n\n\nNIST AI 100-1\nAI RMF 1.0\nExecutive Summary\nArtificial intelligence (AI) technologies have significant potential to transform society and\npeople\u2019s lives \u2013 from commerce and health to transportation and cybersecurity to the envi-\nronment and our planet. AI technologies can drive inclusive economic growth and support\nscientific advancements that improve the conditions of our world. AI technologies, how-\never, also pose risks that can negatively impact individuals, groups, organizations, commu-\nnities, society, the environment, and the planet. Like risks for other types of technology, AI\nrisks can emerge in a variety of ways and can be characterized as long- or short-term, high-\nor low-probability, systemic or localized, and high- or low-impact.\n",
      "rephrase": "This document discusses the roles of AI actors throughout the AI lifecycle. For detailed descriptions of their tasks, including testing and validation, refer to Appendix A. It is important to note that in the AI Model dimension, those who build and use AI models are separated from those who verify and validate them as a best practice. \n\nThe characteristics of trustworthy AI systems are outlined, with 'Valid & Reliable' as the foundation of trustworthiness. 'Accountable & Transparent' is emphasized as it relates to all other characteristics. \n\nAdditionally, AI risk management activities are organized into functions that govern, map, measure, and manage AI risks. Governance is intended to be integrated across all functions. \n\nIn the Executive Summary of NIST AI 100-1 AI RMF 1.0, it highlights that AI technologies have the potential to greatly impact society positively, enhancing areas like commerce, health, transportation, cybersecurity, and environmental conditions. However, these technologies also carry risks that can adversely affect individuals, groups, organizations, and the environment. AI risks can vary in duration, probability, scope, and impact.",
      "entities": [
        {
          "name": "AI actors",
          "type": "category",
          "description": "Entities involved in the AI lifecycle stages, including those building, using, verifying, and validating AI models."
        },
        {
          "name": "trustworthy AI systems",
          "type": "system",
          "description": "AI systems characterized by necessary conditions of trustworthiness, including validity, reliability, accountability, and transparency."
        },
        {
          "name": "AI risk management activities",
          "type": "function",
          "description": "Functions that organize activities to govern, map, measure, and manage risks associated with AI technologies."
        },
        {
          "name": "NIST AI RMF 1.0",
          "type": "document",
          "description": "A framework published by NIST that outlines the risk management framework for artificial intelligence."
        },
        {
          "name": "AI technologies",
          "type": "technology",
          "description": "Technologies that have the potential to transform various sectors of society, including commerce, health, transportation, and cybersecurity."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "Potential negative impacts of AI technologies on individuals, groups, organizations, communities, society, and the environment."
        }
      ],
      "relations": [
        {
          "source": "AI actors",
          "target": "AI lifecycle stages",
          "type": "includes",
          "description": "AI actors are involved in various tasks throughout the AI lifecycle stages.",
          "strength": 8
        },
        {
          "source": "trustworthy AI systems",
          "target": "trustworthiness characteristics",
          "type": "composed_of",
          "description": "Trustworthy AI systems are characterized by validity, reliability, accountability, and transparency.",
          "strength": 9
        },
        {
          "source": "AI risk management activities",
          "target": "AI risks",
          "type": "manages",
          "description": "AI risk management activities are designed to govern and manage risks associated with AI technologies.",
          "strength": 8
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "AI technologies",
          "type": "provides",
          "description": "NIST AI RMF 1.0 provides a framework for managing risks related to AI technologies.",
          "strength": 7
        },
        {
          "source": "AI technologies",
          "target": "society",
          "type": "transforms",
          "description": "AI technologies have the potential to transform various aspects of society.",
          "strength": 9
        },
        {
          "source": "AI risks",
          "target": "individuals and communities",
          "type": "negatively impacts",
          "description": "AI risks can negatively impact individuals, groups, organizations, and communities.",
          "strength": 8
        }
      ],
      "chunk_time": 15.557139083000948,
      "chunk_tokens": 369,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 4,
      "text": " Like risks for other types of technology, AI\nrisks can emerge in a variety of ways and can be characterized as long- or short-term, high-\nor low-probability, systemic or localized, and high- or low-impact.\nThe AI RMF refers to an AI system as an engineered or machine-based system that\ncan, for a given set of objectives, generate outputs such as predictions, recommenda-\ntions, or decisions influencing real or virtual environments. AI systems are designed\nto operate with varying levels of autonomy (Adapted from: OECD Recommendation\non AI:2019; ISO/IEC 22989:2022).\nWhile there are myriad standards and best practices to help organizations mitigate the risks\nof traditional software or information-based systems, the risks posed by AI systems are in\nmany ways unique (See Appendix B). AI systems, for example, may be trained on data that\ncan change over time, sometimes significantly and unexpectedly, affecting system function-\nality and trustworthiness in ways that are hard to understand. AI systems and the contexts\nin which they are deployed are frequently complex, making it difficult to detect and respond\nto failures when they occur. AI systems are inherently socio-technical in nature, meaning\nthey are influenced by societal dynamics and human behavior. AI risks \u2013 and benefits \u2013\ncan emerge from the interplay of technical aspects combined with societal factors related\nto how a system is used, its interactions with other AI systems, who operates it, and the\nsocial context in which it is deployed.\nThese risks make AI a uniquely challenging technology to deploy and utilize both for orga-\nnizations and within society. Without proper controls, AI systems can amplify, perpetuate,\nor exacerbate inequitable or undesirable outcomes for individuals and communities. With\nproper controls, AI systems can mitigate and manage inequitable outcomes.\nAI risk management is a key component of responsible development and use of AI sys-\ntems.",
      "rephrase": "AI technology, like other technologies, carries various risks that can be categorized in different ways: they can be long-term or short-term, high-probability or low-probability, systemic or localized, and high-impact or low-impact. The AI Risk Management Framework (AI RMF) defines an AI system as a machine-based system designed to achieve specific goals by producing outputs such as predictions, recommendations, or decisions that affect real or virtual environments. These systems can operate with different levels of autonomy (Adapted from: OECD Recommendation on AI:2019; ISO/IEC 22989:2022). \n\nWhile there are many standards and best practices to help organizations manage risks associated with traditional software, AI systems present unique challenges (See Appendix B). For instance, AI systems can be trained on data that changes over time, sometimes in unexpected ways, which can impact their functionality and reliability. Additionally, the complexity of AI systems and their deployment contexts makes it hard to identify and address failures. \n\nAI systems are also socio-technical, meaning they are affected by social dynamics and human behavior. The risks and benefits of AI arise from the interaction between technical features and societal factors, including how the system is used, its interactions with other AI systems, who operates it, and the social context of its deployment. \n\nThese factors make AI a particularly challenging technology for organizations and society to implement and use. Without proper controls, AI systems can worsen inequities or lead to negative outcomes for individuals and communities. However, with the right controls in place, AI systems can help reduce and manage these inequities. \n\nEffective AI risk management is essential for the responsible development and use of AI systems.",
      "entities": [
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "The AI RMF refers to an AI system as an engineered or machine-based system that can generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments."
        },
        {
          "name": "AI systems",
          "type": "system",
          "description": "AI systems are designed to operate with varying levels of autonomy and can be influenced by societal dynamics and human behavior."
        },
        {
          "name": "OECD Recommendation on AI:2019",
          "type": "standard",
          "description": "A recommendation that provides guidelines for the responsible development and use of AI systems."
        },
        {
          "name": "ISO/IEC 22989:2022",
          "type": "standard",
          "description": "An international standard that addresses the characteristics and requirements of AI systems."
        },
        {
          "name": "AI risk management",
          "type": "function",
          "description": "A key component of responsible development and use of AI systems aimed at mitigating and managing risks associated with AI."
        },
        {
          "name": "societal dynamics",
          "type": "category",
          "description": "Factors related to human behavior and societal interactions that influence the deployment and functioning of AI systems."
        },
        {
          "name": "inequitable outcomes",
          "type": "outcome",
          "description": "Undesirable results that can be amplified or perpetuated by AI systems if not properly controlled."
        }
      ],
      "relations": [
        {
          "source": "AI RMF",
          "target": "AI systems",
          "type": "describes",
          "description": "The AI RMF describes the nature and functionality of AI systems.",
          "strength": 9
        },
        {
          "source": "AI systems",
          "target": "societal dynamics",
          "type": "influenced_by",
          "description": "AI systems are influenced by societal dynamics and human behavior.",
          "strength": 8
        },
        {
          "source": "AI risk management",
          "target": "AI systems",
          "type": "supports",
          "description": "AI risk management supports the responsible development and use of AI systems.",
          "strength": 9
        },
        {
          "source": "AI systems",
          "target": "inequitable outcomes",
          "type": "creates_opportunities_for",
          "description": "AI systems can create opportunities for inequitable outcomes if not properly managed.",
          "strength": 7
        },
        {
          "source": "OECD Recommendation on AI:2019",
          "target": "AI RMF",
          "type": "aligned_with",
          "description": "The OECD Recommendation on AI:2019 aligns with the principles outlined in the AI RMF.",
          "strength": 8
        },
        {
          "source": "ISO/IEC 22989:2022",
          "target": "AI RMF",
          "type": "aligned_with",
          "description": "ISO/IEC 22989:2022 provides standards that align with the AI RMF.",
          "strength": 8
        }
      ],
      "chunk_time": 16.62975862500025,
      "chunk_tokens": 400,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 5,
      "text": "uate,\nor exacerbate inequitable or undesirable outcomes for individuals and communities. With\nproper controls, AI systems can mitigate and manage inequitable outcomes.\nAI risk management is a key component of responsible development and use of AI sys-\ntems. Responsible AI practices can help align the decisions about AI system design, de-\nvelopment, and uses with intended aim and values. Core concepts in responsible AI em-\nphasize human centricity, social responsibility, and sustainability. AI risk management can\ndrive responsible uses and practices by prompting organizations and their internal teams\nwho design, develop, and deploy AI to think more critically about context and potential\nor unexpected negative and positive impacts. Understanding and managing the risks of AI\nsystems will help to enhance trustworthiness, and in turn, cultivate public trust.\nPage 1\n\n\nNIST AI 100-1\nAI RMF 1.0\nSocial responsibility can refer to the organization\u2019s responsibility \u201cfor the impacts\nof its decisions and activities on society and the environment through transparent\nand ethical behavior\u201d (ISO 26000:2010). Sustainability refers to the \u201cstate of the\nglobal system, including environmental, social, and economic aspects, in which the\nneeds of the present are met without compromising the ability of future generations\nto meet their own needs\u201d (ISO/IEC TR 24368:2022). Responsible AI is meant to\nresult in technology that is also equitable and accountable. The expectation is that\norganizational practices are carried out in accord with \u201cprofessional responsibility,\u201d\ndefined by ISO as an approach that \u201caims to ensure that professionals who design,\ndevelop, or deploy AI systems and applications or AI-based products or systems,\nrecognize their unique position to exert influence on people, society, and the future\nof AI\u201d (ISO/IEC TR 24368:2022).\nAs directed by the National Artificial Intelligence Initiative Act of 2020 (P.L.",
      "rephrase": "AI systems can sometimes create or worsen unfair outcomes for people and communities. However, with the right controls, these systems can help reduce and manage these issues. Managing AI risks is essential for the responsible development and use of AI. By following responsible AI practices, organizations can ensure that their decisions about designing, developing, and using AI align with their goals and values. Key principles of responsible AI focus on putting people first, being socially responsible, and promoting sustainability. Effective AI risk management encourages organizations and their teams to carefully consider the context and possible positive or negative effects of AI. By understanding and managing these risks, organizations can build trust and foster public confidence in AI technologies.\n\nSocial responsibility means that organizations must consider how their decisions and actions affect society and the environment, acting transparently and ethically. Sustainability refers to meeting current needs without compromising future generations' ability to meet theirs. Responsible AI aims to create technology that is fair and accountable. Organizations are expected to follow professional responsibility, which means that those who design, develop, or deploy AI systems should recognize their influence on society and the future of AI, as defined by ISO standards.",
      "entities": [
        {
          "name": "AI risk management",
          "type": "function",
          "description": "A key component of responsible development and use of AI systems, aimed at mitigating and managing inequitable outcomes."
        },
        {
          "name": "Responsible AI",
          "type": "framework",
          "description": "A framework that emphasizes human centricity, social responsibility, and sustainability in the design, development, and use of AI systems."
        },
        {
          "name": "ISO 26000:2010",
          "type": "standard",
          "description": "An international standard that outlines social responsibility and the impacts of organizational decisions on society and the environment."
        },
        {
          "name": "ISO/IEC TR 24368:2022",
          "type": "standard",
          "description": "A standard that defines professional responsibility in the context of AI, ensuring that professionals recognize their influence on society and the future of AI."
        },
        {
          "name": "National Artificial Intelligence Initiative Act of 2020",
          "type": "document",
          "description": "A legislative act that directs the development and implementation of AI initiatives in the United States."
        }
      ],
      "relations": [
        {
          "source": "AI risk management",
          "target": "Responsible AI",
          "type": "supports",
          "description": "AI risk management drives responsible uses and practices in AI development.",
          "strength": 9
        },
        {
          "source": "Responsible AI",
          "target": "ISO 26000:2010",
          "type": "aligned_with",
          "description": "Responsible AI practices align with the principles of social responsibility outlined in ISO 26000:2010.",
          "strength": 8
        },
        {
          "source": "Responsible AI",
          "target": "ISO/IEC TR 24368:2022",
          "type": "aligned_with",
          "description": "Responsible AI is in accord with the professional responsibility defined by ISO/IEC TR 24368:2022.",
          "strength": 8
        },
        {
          "source": "National Artificial Intelligence Initiative Act of 2020",
          "target": "AI risk management",
          "type": "directed_by",
          "description": "The act directs the development of AI initiatives, including risk management practices.",
          "strength": 7
        }
      ],
      "chunk_time": 12.403932875000464,
      "chunk_tokens": 396,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 6,
      "text": "recognize their unique position to exert influence on people, society, and the future\nof AI\u201d (ISO/IEC TR 24368:2022).\nAs directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283),\nthe goal of the AI RMF is to offer a resource to the organizations designing, developing,\ndeploying, or using AI systems to help manage the many risks of AI and promote trustwor-\nthy and responsible development and use of AI systems. The Framework is intended to be\nvoluntary, rights-preserving, non-sector-specific, and use-case agnostic, providing flexibil-\nity to organizations of all sizes and in all sectors and throughout society to implement the\napproaches in the Framework.\nThe Framework is designed to equip organizations and individuals \u2013 referred to here as\nAI actors \u2013 with approaches that increase the trustworthiness of AI systems, and to help\nfoster the responsible design, development, deployment, and use of AI systems over time.\nAI actors are defined by the Organisation for Economic Co-operation and Development\n(OECD) as \u201cthose who play an active role in the AI system lifecycle, including organiza-\ntions and individuals that deploy or operate AI\u201d [OECD (2019) Artificial Intelligence in\nSociety\u2014OECD iLibrary] (See Appendix A).\nThe AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies\ncontinue to develop, and to be operationalized by organizations in varying degrees and\ncapacities so society can benefit from AI while also being protected from its potential\nharms.\nThe Framework and supporting resources will be updated, expanded, and improved based\non evolving technology, the standards landscape around the world, and AI community ex-\nperience and feedback. NIST will continue to align the AI RMF and related guidance with\napplicable international standards, guidelines, and practices.",
      "rephrase": "The AI RMF (Artificial Intelligence Risk Management Framework) aims to help organizations that design, develop, deploy, or use AI systems manage risks and promote trustworthy AI practices. This initiative is guided by the National Artificial Intelligence Initiative Act of 2020. The Framework is voluntary, respects rights, and is applicable across various sectors and use cases, allowing organizations of all sizes to adopt its approaches. It is designed to help both organizations and individuals, known as AI actors, enhance the trustworthiness of AI systems and ensure responsible practices in their design, development, deployment, and use. AI actors are defined by the OECD as those actively involved in the AI system lifecycle, including organizations and individuals who deploy or operate AI. The AI RMF is practical and adaptable, aiming to evolve alongside AI technologies while ensuring societal benefits and protection from potential harms. The Framework and its resources will be regularly updated based on technological advancements, global standards, and feedback from the AI community. NIST will continue to align the AI RMF with relevant international standards and guidelines.",
      "entities": [
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "The AI RMF (Artificial Intelligence Risk Management Framework) is designed to help organizations manage the risks associated with AI systems and promote trustworthy development and use."
        },
        {
          "name": "National Artificial Intelligence Initiative Act of 2020",
          "type": "document",
          "description": "A legislative act that directs the establishment of frameworks and resources for managing AI risks and promoting responsible AI development."
        },
        {
          "name": "Organisation for Economic Co-operation and Development (OECD)",
          "type": "organization",
          "description": "An international organization that defines AI actors as those involved in the AI system lifecycle, including organizations and individuals that deploy or operate AI."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "The National Institute of Standards and Technology, which is responsible for aligning the AI RMF with international standards and guidelines."
        },
        {
          "name": "AI actors",
          "type": "category",
          "description": "Individuals and organizations that play an active role in the lifecycle of AI systems, including deployment and operation."
        }
      ],
      "relations": [
        {
          "source": "AI RMF",
          "target": "National Artificial Intelligence Initiative Act of 2020",
          "type": "develops",
          "description": "The AI RMF is developed as a resource directed by the National Artificial Intelligence Initiative Act.",
          "strength": 9
        },
        {
          "source": "AI RMF",
          "target": "AI actors",
          "type": "supports",
          "description": "The AI RMF supports AI actors by providing approaches to increase the trustworthiness of AI systems.",
          "strength": 8
        },
        {
          "source": "AI RMF",
          "target": "NIST",
          "type": "aligned_with",
          "description": "NIST aligns the AI RMF with applicable international standards and guidelines.",
          "strength": 8
        },
        {
          "source": "OECD",
          "target": "AI actors",
          "type": "defines",
          "description": "OECD defines AI actors as those involved in the AI system lifecycle.",
          "strength": 7
        }
      ],
      "chunk_time": 12.295422375005728,
      "chunk_tokens": 393,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 7,
      "text": ", and improved based\non evolving technology, the standards landscape around the world, and AI community ex-\nperience and feedback. NIST will continue to align the AI RMF and related guidance with\napplicable international standards, guidelines, and practices. As the AI RMF is put into\nuse, additional lessons will be learned to inform future updates and additional resources.\nThe Framework is divided into two parts. Part 1 discusses how organizations can frame\nthe risks related to AI and describes the intended audience. Next, AI risks and trustworthi-\nness are analyzed, outlining the characteristics of trustworthy AI systems, which include\nPage 2\n\n\nNIST AI 100-1\nAI RMF 1.0\nvalid and reliable, safe, secure and resilient, accountable and transparent, explainable and\ninterpretable, privacy enhanced, and fair with their harmful biases managed.\nPart 2 comprises the \u201cCore\u201d of the Framework. It describes four specific functions to help\norganizations address the risks of AI systems in practice. These functions \u2013 GOVERN,\nMAP, MEASURE, and MANAGE \u2013 are broken down further into categories and subcate-\ngories. While GOVERN applies to all stages of organizations\u2019 AI risk management pro-\ncesses and procedures, the MAP, MEASURE, and MANAGE functions can be applied in AI\nsystem-specific contexts and at specific stages of the AI lifecycle.\nAdditional resources related to the Framework are included in the AI RMF Playbook,\nwhich is available via the NIST AI RMF website:\nhttps://www.nist.gov/itl/ai-risk-management-framework.\nDevelopment of the AI RMF by NIST in collaboration with the private and public sec-\ntors is directed and consistent with its broader AI efforts called for by the National AI\nInitiative Act of 2020, the National Security Commission on Artificial Intelligence recom-\nmendations, and the Plan for Federal Engagement in Developing Technical Standards and\nRelated Tools.",
      "rephrase": "The AI Risk Management Framework (AI RMF) is being updated to keep pace with new technology, global standards, and feedback from the AI community. NIST will ensure that the AI RMF aligns with relevant international standards and guidelines. As organizations start using the AI RMF, they will gain insights that will help improve future versions and resources. \n\nThe Framework has two main parts. Part 1 explains how organizations can identify and manage AI-related risks and describes who should use the framework. It also analyzes AI risks and what makes AI systems trustworthy. Trustworthy AI systems should be valid, reliable, safe, secure, resilient, accountable, transparent, explainable, interpretable, privacy-enhancing, and fair, with efforts to manage harmful biases. \n\nPart 2 is the core of the Framework and outlines four key functions to help organizations manage AI risks: GOVERN, MAP, MEASURE, and MANAGE. These functions are further divided into categories and subcategories. GOVERN applies to all stages of AI risk management, while MAP, MEASURE, and MANAGE are specific to certain contexts and stages of the AI lifecycle. \n\nAdditional resources related to the Framework can be found in the AI RMF Playbook, available on the NIST AI RMF website: https://www.nist.gov/itl/ai-risk-management-framework. \n\nNIST developed the AI RMF in collaboration with both private and public sectors, in line with broader AI initiatives outlined in the National AI Initiative Act of 2020, recommendations from the National Security Commission on Artificial Intelligence, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools.",
      "entities": [
        {
          "name": "NIST AI RMF 1.0",
          "type": "document",
          "description": "The AI Risk Management Framework (AI RMF) developed by NIST, which provides guidance on managing risks associated with AI systems."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "The National Institute of Standards and Technology, a U.S. federal agency that develops standards and guidelines for various technologies, including AI."
        },
        {
          "name": "AI RMF Playbook",
          "type": "resource",
          "description": "A supplementary resource related to the AI RMF, providing additional guidance and tools for implementing the framework."
        },
        {
          "name": "National AI Initiative Act of 2020",
          "type": "standard",
          "description": "A legislative act that calls for the development of a national strategy for AI, including standards and guidelines."
        },
        {
          "name": "National Security Commission on Artificial Intelligence",
          "type": "organization",
          "description": "A commission that provides recommendations on how to advance AI technology while ensuring national security."
        },
        {
          "name": "AI lifecycle",
          "type": "category",
          "description": "The stages through which an AI system progresses, from development to deployment and management."
        },
        {
          "name": "GOVERN",
          "type": "function",
          "description": "One of the four specific functions in the AI RMF that applies to all stages of AI risk management processes."
        },
        {
          "name": "MAP",
          "type": "function",
          "description": "A function in the AI RMF that helps organizations identify and assess AI risks in specific contexts."
        },
        {
          "name": "MEASURE",
          "type": "function",
          "description": "A function in the AI RMF that focuses on evaluating the effectiveness of AI risk management strategies."
        },
        {
          "name": "MANAGE",
          "type": "function",
          "description": "A function in the AI RMF that involves implementing and overseeing AI risk management practices."
        }
      ],
      "relations": [
        {
          "source": "NIST AI RMF 1.0",
          "target": "NIST",
          "type": "authored_by",
          "description": "The AI RMF 1.0 document is authored by NIST.",
          "strength": 9
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "AI RMF Playbook",
          "type": "includes",
          "description": "The AI RMF 1.0 includes additional resources provided in the AI RMF Playbook.",
          "strength": 8
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "National AI Initiative Act of 2020",
          "type": "aligned_with",
          "description": "The development of the AI RMF is aligned with the objectives of the National AI Initiative Act of 2020.",
          "strength": 7
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "National Security Commission on Artificial Intelligence",
          "type": "aligned_with",
          "description": "The AI RMF is consistent with recommendations from the National Security Commission on Artificial Intelligence.",
          "strength": 7
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "GOVERN",
          "type": "composed_of",
          "description": "The AI RMF 1.0 framework is composed of the function GOVERN, among others.",
          "strength": 8
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "MAP",
          "type": "composed_of",
          "description": "The AI RMF 1.0 framework includes the function MAP as part of its structure.",
          "strength": 8
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "MEASURE",
          "type": "composed_of",
          "description": "The AI RMF 1.0 framework includes the function MEASURE to evaluate AI risks.",
          "strength": 8
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "MANAGE",
          "type": "composed_of",
          "description": "The AI RMF 1.0 framework includes the function MANAGE for overseeing AI risk management.",
          "strength": 8
        }
      ],
      "chunk_time": 23.296561082999688,
      "chunk_tokens": 398,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 8,
      "text": " and consistent with its broader AI efforts called for by the National AI\nInitiative Act of 2020, the National Security Commission on Artificial Intelligence recom-\nmendations, and the Plan for Federal Engagement in Developing Technical Standards and\nRelated Tools. Engagement with the AI community during this Framework\u2019s development\n\u2013 via responses to a formal Request for Information, three widely attended workshops,\npublic comments on a concept paper and two drafts of the Framework, discussions at mul-\ntiple public forums, and many small group meetings \u2013 has informed development of the AI\nRMF 1.0 as well as AI research and development and evaluation conducted by NIST and\nothers. Priority research and additional guidance that will enhance this Framework will be\ncaptured in an associated AI Risk Management Framework Roadmap to which NIST and\nthe broader community can contribute.\nPage 3\n\n\nNIST AI 100-1\nAI RMF 1.0\nPart 1: Foundational Information\n1.\nFraming Risk\nAI risk management offers a path to minimize potential negative impacts of AI systems,\nsuch as threats to civil liberties and rights, while also providing opportunities to maximize\npositive impacts. Addressing, documenting, and managing AI risks and potential negative\nimpacts effectively can lead to more trustworthy AI systems.\n1.1\nUnderstanding and Addressing Risks, Impacts, and Harms\nIn the context of the AI RMF, risk refers to the composite measure of an event\u2019s probability\nof occurring and the magnitude or degree of the consequences of the corresponding event.\nThe impacts, or consequences, of AI systems can be positive, negative, or both and can\nresult in opportunities or threats (Adapted from: ISO 31000:2018).",
      "rephrase": "The National Institute of Standards and Technology (NIST) is working on an AI Risk Management Framework (AI RMF 1.0) as part of its broader AI initiatives, which are guided by the National AI Initiative Act of 2020, recommendations from the National Security Commission on Artificial Intelligence, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools. To develop this framework, NIST engaged with the AI community through various means, including a formal Request for Information, three workshops, public comments on a concept paper and two drafts, discussions at public forums, and small group meetings. This engagement has shaped the AI RMF 1.0 and influenced AI research and development by NIST and others. Future research and guidance to improve this framework will be included in an AI Risk Management Framework Roadmap, which will allow contributions from NIST and the wider community.\n\nPart 1: Foundational Information\n\n1. Framing Risk\nAI risk management aims to reduce the potential negative effects of AI systems, such as threats to civil liberties, while also maximizing their positive impacts. Effectively addressing and managing AI risks can lead to more trustworthy AI systems.\n\n1.1 Understanding and Addressing Risks, Impacts, and Harms\nIn the AI RMF context, risk is defined as the likelihood of an event occurring combined with the severity of its consequences. The impacts of AI systems can be positive, negative, or both, leading to either opportunities or threats (Adapted from: ISO 31000:2018).",
      "entities": [
        {
          "name": "National AI Initiative Act of 2020",
          "type": "document",
          "description": "A legislative act aimed at promoting and coordinating AI research and development efforts in the United States."
        },
        {
          "name": "National Security Commission on Artificial Intelligence",
          "type": "organization",
          "description": "A commission established to provide recommendations on how to advance AI technology while ensuring national security."
        },
        {
          "name": "Plan for Federal Engagement in Developing Technical Standards and Related Tools",
          "type": "document",
          "description": "A strategic plan outlining how federal agencies will engage in the development of technical standards and tools for AI."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0, which provides guidelines for managing risks associated with AI systems."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "The National Institute of Standards and Technology, a federal agency that develops standards and guidelines for various technologies, including AI."
        },
        {
          "name": "AI Risk Management Framework Roadmap",
          "type": "document",
          "description": "A roadmap that outlines priority research and guidance to enhance the AI RMF."
        },
        {
          "name": "ISO 31000:2018",
          "type": "standard",
          "description": "An international standard for risk management that provides guidelines and principles for managing risks."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "NIST",
          "type": "develops",
          "description": "NIST develops the AI RMF 1.0 framework to guide AI risk management.",
          "strength": 9
        },
        {
          "source": "National AI Initiative Act of 2020",
          "target": "National Security Commission on Artificial Intelligence",
          "type": "aligned_with",
          "description": "The National AI Initiative Act of 2020 aligns with the recommendations of the National Security Commission on Artificial Intelligence.",
          "strength": 8
        },
        {
          "source": "AI RMF 1.0",
          "target": "AI Risk Management Framework Roadmap",
          "type": "supports",
          "description": "The AI RMF 1.0 supports the development of the AI Risk Management Framework Roadmap.",
          "strength": 7
        },
        {
          "source": "AI RMF 1.0",
          "target": "ISO 31000:2018",
          "type": "infused_throughout",
          "description": "The principles of ISO 31000:2018 are infused throughout the AI RMF 1.0 framework.",
          "strength": 8
        }
      ],
      "chunk_time": 14.895166417001747,
      "chunk_tokens": 356,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 9,
      "text": " magnitude or degree of the consequences of the corresponding event.\nThe impacts, or consequences, of AI systems can be positive, negative, or both and can\nresult in opportunities or threats (Adapted from: ISO 31000:2018). When considering the\nnegative impact of a potential event, risk is a function of 1) the negative impact, or magni-\ntude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of\noccurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be\nexperienced by individuals, groups, communities, organizations, society, the environment,\nand the planet.\n\u201cRisk management refers to coordinated activities to direct and control an organiza-\ntion with regard to risk\u201d (Source: ISO 31000:2018).\nWhile risk management processes generally address negative impacts, this Framework of-\nfers approaches to minimize anticipated negative impacts of AI systems and identify op-\nportunities to maximize positive impacts. Effectively managing the risk of potential harms\ncould lead to more trustworthy AI systems and unleash potential benefits to people (individ-\nuals, communities, and society), organizations, and systems/ecosystems. Risk management\ncan enable AI developers and users to understand impacts and account for the inherent lim-\nitations and uncertainties in their models and systems, which in turn can improve overall\nsystem performance and trustworthiness and the likelihood that AI technologies will be\nused in ways that are beneficial.\nThe AI RMF is designed to address new risks as they emerge. This flexibility is particularly\nimportant where impacts are not easily foreseeable and applications are evolving. While\nsome AI risks and benefits are well-known, it can be challenging to assess negative impacts\nand the degree of harms. Figure 1 provides examples of potential harms that can be related\nto AI systems.\n",
      "rephrase": "The consequences of AI systems can be positive, negative, or a mix of both, leading to opportunities or threats (Adapted from: ISO 31000:2018). When evaluating the negative effects of a potential event, risk is determined by two factors: 1) the severity of harm that could occur if the event happens, and 2) the likelihood of that event occurring (Adapted from: OMB Circular A-130:2016). Negative impacts can affect individuals, groups, communities, organizations, society, the environment, and the planet.\n\nRisk management involves organized efforts to guide and control an organization regarding risks (Source: ISO 31000:2018). While traditional risk management focuses on addressing negative impacts, this framework also aims to reduce expected negative effects of AI systems and identify ways to enhance positive impacts. By effectively managing potential harms, we can create more trustworthy AI systems that benefit individuals, communities, organizations, and ecosystems. Risk management helps AI developers and users understand the impacts of their systems and recognize the limitations and uncertainties involved, which can improve overall performance and trustworthiness, increasing the chances that AI technologies will be used beneficially.\n\nThe AI Risk Management Framework (AI RMF) is designed to adapt to new risks as they arise, which is crucial since some impacts may not be easily predicted and AI applications are constantly evolving. Although some risks and benefits of AI are well understood, assessing negative impacts and the extent of harm can be difficult. Figure 1 illustrates examples of potential harms associated with AI systems.",
      "entities": [
        {
          "name": "ISO 31000:2018",
          "type": "standard",
          "description": "A standard that provides guidelines for risk management processes, emphasizing coordinated activities to manage risks within organizations."
        },
        {
          "name": "OMB Circular A-130:2016",
          "type": "guideline",
          "description": "A guideline that outlines the management of information resources, including considerations for risk in the context of negative impacts."
        },
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "The AI Risk Management Framework designed to address emerging risks associated with AI systems and to maximize positive impacts while minimizing negative ones."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Systems that utilize artificial intelligence, which can have both positive and negative impacts on individuals, organizations, and society."
        },
        {
          "name": "risk management",
          "type": "function",
          "description": "Coordinated activities aimed at directing and controlling an organization with regard to risk, particularly focusing on minimizing negative impacts."
        },
        {
          "name": "individuals, communities, and society",
          "type": "category",
          "description": "Stakeholders that can experience the impacts of AI systems, both positive and negative."
        },
        {
          "name": "systems/ecosystems",
          "type": "domain",
          "description": "Broader contexts in which AI systems operate and can have impacts, including environmental and societal systems."
        }
      ],
      "relations": [
        {
          "source": "AI RMF",
          "target": "AI systems",
          "type": "supports",
          "description": "The AI RMF supports the management of risks associated with AI systems to enhance their trustworthiness.",
          "strength": 9
        },
        {
          "source": "risk management",
          "target": "ISO 31000:2018",
          "type": "aligned_with",
          "description": "Risk management processes are aligned with the guidelines provided by ISO 31000:2018.",
          "strength": 8
        },
        {
          "source": "risk management",
          "target": "individuals, communities, and society",
          "type": "provides",
          "description": "Risk management provides frameworks to understand and mitigate impacts on individuals, communities, and society.",
          "strength": 7
        },
        {
          "source": "AI RMF",
          "target": "risk management",
          "type": "develops",
          "description": "The AI RMF develops approaches to enhance risk management specifically for AI technologies.",
          "strength": 8
        }
      ],
      "chunk_time": 13.877703541998926,
      "chunk_tokens": 383,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 10,
      "text": " easily foreseeable and applications are evolving. While\nsome AI risks and benefits are well-known, it can be challenging to assess negative impacts\nand the degree of harms. Figure 1 provides examples of potential harms that can be related\nto AI systems.\nAI risk management efforts should consider that humans may assume that AI systems work\n\u2013 and work well \u2013 in all settings. For example, whether correct or not, AI systems are\noften perceived as being more objective than humans or as offering greater capabilities\nthan general software.\nPage 4\n\n\nNIST AI 100-1\nAI RMF 1.0\nFig. 1. Examples of potential harms related to AI systems. Trustworthy AI systems and their\nresponsible use can mitigate negative risks and contribute to benefits for people, organizations, and\necosystems.\n1.2\nChallenges for AI Risk Management\nSeveral challenges are described below. They should be taken into account when managing\nrisks in pursuit of AI trustworthiness.\n1.2.1\nRisk Measurement\nAI risks or failures that are not well-defined or adequately understood are difficult to mea-\nsure quantitatively or qualitatively. The inability to appropriately measure AI risks does not\nimply that an AI system necessarily poses either a high or low risk. Some risk measurement\nchallenges include:\nRisks related to third-party software, hardware, and data: Third-party data or systems\ncan accelerate research and development and facilitate technology transition. They also\nmay complicate risk measurement. Risk can emerge both from third-party data, software or\nhardware itself and how it is used. Risk metrics or methodologies used by the organization\ndeveloping the AI system may not align with the risk metrics or methodologies uses by\nthe organization deploying or operating the system. Also, the organization developing\nthe AI system may not be transparent about the risk metrics or methodologies it used.",
      "rephrase": "AI applications are rapidly evolving, and while some risks and benefits of AI are well understood, it can be difficult to evaluate the negative impacts and extent of harm. Figure 1 shows examples of potential harms associated with AI systems. \n\nWhen managing AI risks, it's important to recognize that people often assume AI systems function effectively in all situations. Many believe AI is more objective than humans or has greater capabilities than traditional software, regardless of whether this is true. \n\nTrustworthy AI systems, when used responsibly, can help reduce risks and provide benefits to individuals, organizations, and ecosystems. \n\n### Challenges in AI Risk Management \n\nThere are several challenges to consider when managing AI risks to ensure trustworthiness: \n\n1. **Risk Measurement**: It is hard to measure AI risks or failures that are not clearly defined or understood. Just because a risk is difficult to measure does not mean it is necessarily high or low. Some challenges in risk measurement include: \n   - **Third-party Software, Hardware, and Data**: Using third-party data or systems can speed up research and development but can also complicate risk measurement. Risks can arise from the third-party data, software, or hardware itself, as well as from how they are used. The risk metrics used by the organization developing the AI may not match those used by the organization deploying or operating it. Additionally, the developers may not be transparent about the risk metrics or methods they employed.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) 1.0, providing guidelines for managing risks associated with AI systems."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework 1.0 developed by NIST, aimed at fostering trustworthiness in AI systems through effective risk management."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Technological systems that utilize artificial intelligence to perform tasks, often perceived as more objective than human capabilities."
        },
        {
          "name": "third-party software, hardware, and data",
          "type": "resource",
          "description": "External resources that can influence the development and deployment of AI systems, potentially complicating risk measurement."
        },
        {
          "name": "risk measurement challenges",
          "type": "category",
          "description": "Challenges associated with quantifying and qualifying risks related to AI systems, particularly when risks are not well-defined."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 outlines the AI RMF 1.0 framework for managing AI risks.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "AI systems",
          "type": "supports",
          "description": "The AI RMF 1.0 framework supports the responsible use of AI systems to mitigate risks.",
          "strength": 8
        },
        {
          "source": "third-party software, hardware, and data",
          "target": "risk measurement challenges",
          "type": "contributes_to",
          "description": "Third-party resources contribute to the challenges faced in measuring AI risks effectively.",
          "strength": 7
        },
        {
          "source": "AI systems",
          "target": "risk measurement challenges",
          "type": "illustrates",
          "description": "The complexities of AI systems illustrate the challenges in defining and measuring associated risks.",
          "strength": 8
        }
      ],
      "chunk_time": 12.546150000001944,
      "chunk_tokens": 386,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 11,
      "text": " the organization\ndeveloping the AI system may not align with the risk metrics or methodologies uses by\nthe organization deploying or operating the system. Also, the organization developing\nthe AI system may not be transparent about the risk metrics or methodologies it used. Risk\nmeasurement and management can be complicated by how customers use or integrate third-\nparty data or systems into AI products or services, particularly without sufficient internal\ngovernance structures and technical safeguards. Regardless, all parties and AI actors should\nmanage risk in the AI systems they develop, deploy, or use as standalone or integrated\ncomponents.\nTracking emergent risks: Organizations\u2019 risk management efforts will be enhanced by\nidentifying and tracking emergent risks and considering techniques for measuring them.\nPage 5\n\n\nNIST AI 100-1\nAI RMF 1.0\nAI system impact assessment approaches can help AI actors understand potential impacts\nor harms within specific contexts.\nAvailability of reliable metrics: The current lack of consensus on robust and verifiable\nmeasurement methods for risk and trustworthiness, and applicability to different AI use\ncases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure\nnegative risk or harms include the reality that development of metrics is often an institu-\ntional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In\naddition, measurement approaches can be oversimplified, gamed, lack critical nuance, be-\ncome relied upon in unexpected ways, or fail to account for differences in affected groups\nand contexts.\nApproaches for measuring impacts on a population work best if they recognize that contexts\nmatter, that harms may affect varied groups or sub-groups differently, and that communities\nor other sub-groups who may be harmed are not always direct users of a system.\n",
      "rephrase": "The organization that creates the AI system might not use the same risk assessment methods as the organization that implements or operates it. Additionally, the developers may not clearly share the risk assessment methods they used. Managing risks can become more complex when customers incorporate third-party data or systems into AI products, especially if there are no strong internal governance or technical safeguards in place. Nonetheless, all parties involved in developing, deploying, or using AI systems\u2014whether as standalone products or integrated components\u2014should actively manage risks. \n\nTo improve risk management, organizations should identify and monitor emerging risks and explore ways to measure them. AI system impact assessments can help stakeholders understand potential harms in specific situations. \n\nA major challenge in measuring AI risks is the lack of agreement on effective and reliable methods for assessing risk and trustworthiness across different AI applications. When trying to measure negative risks or harms, there are potential issues, such as the fact that creating metrics is often a collective effort and may unintentionally include irrelevant factors. Furthermore, measurement methods can be overly simplistic, manipulated, lack important details, be misused, or fail to consider the differences among affected groups and contexts. \n\nEffective approaches to measuring impacts on populations should acknowledge that context is important, that harms can affect different groups in various ways, and that communities or sub-groups at risk may not always be direct users of the AI system.",
      "entities": [
        {
          "name": "AI system",
          "type": "system",
          "description": "A technological system that utilizes artificial intelligence to perform tasks or provide services."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document published by NIST that outlines guidelines and frameworks for AI risk management."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0, which provides a structured approach to managing risks associated with AI systems."
        },
        {
          "name": "risk metrics",
          "type": "category",
          "description": "Quantitative measures used to assess the risks associated with AI systems."
        },
        {
          "name": "methodologies",
          "type": "methodology",
          "description": "Structured approaches or techniques used to measure and manage risks in AI systems."
        },
        {
          "name": "third-party data",
          "type": "resource",
          "description": "Data sourced from external entities that can be integrated into AI products or services."
        },
        {
          "name": "internal governance structures",
          "type": "system",
          "description": "Organizational frameworks that ensure compliance and risk management within AI development and deployment."
        },
        {
          "name": "emergent risks",
          "type": "risk",
          "description": "Newly arising risks that may not have been previously identified or considered in risk management efforts."
        },
        {
          "name": "reliable metrics",
          "type": "category",
          "description": "Metrics that are consistent, accurate, and trustworthy for measuring risk and trustworthiness in AI."
        },
        {
          "name": "impacts on a population",
          "type": "outcome",
          "description": "The effects or consequences that AI systems may have on different groups within a population."
        }
      ],
      "relations": [
        {
          "source": "AI system",
          "target": "risk metrics",
          "type": "aligned_with",
          "description": "The AI system's risk metrics may not align with those used by organizations deploying the system.",
          "strength": 7
        },
        {
          "source": "organization developing the AI system",
          "target": "methodologies",
          "type": "operates_under",
          "description": "The organization developing the AI system may use specific methodologies for risk measurement.",
          "strength": 6
        },
        {
          "source": "third-party data",
          "target": "AI products or services",
          "type": "integrated_into",
          "description": "Third-party data can be integrated into AI products or services, affecting their risk profile.",
          "strength": 8
        },
        {
          "source": "emergent risks",
          "target": "risk management efforts",
          "type": "supports",
          "description": "Identifying and tracking emergent risks enhances organizations' risk management efforts.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "composed_of",
          "description": "The NIST AI 100-1 document includes the AI RMF 1.0 framework.",
          "strength": 8
        },
        {
          "source": "reliable metrics",
          "target": "risk measurement challenge",
          "type": "contributes_to",
          "description": "The lack of reliable metrics contributes to challenges in measuring risk and trustworthiness in AI.",
          "strength": 7
        },
        {
          "source": "impacts on a population",
          "target": "methodologies",
          "type": "applicable_to",
          "description": "Methodologies for measuring impacts on a population are applicable to understanding AI harms.",
          "strength": 8
        }
      ],
      "chunk_time": 19.81715254199662,
      "chunk_tokens": 358,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 12,
      "text": "Approaches for measuring impacts on a population work best if they recognize that contexts\nmatter, that harms may affect varied groups or sub-groups differently, and that communities\nor other sub-groups who may be harmed are not always direct users of a system.\nRisk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI\nlifecycle may yield different results than measuring risk at a later stage; some risks may\nbe latent at a given point in time and may increase as AI systems adapt and evolve. Fur-\nthermore, different AI actors across the AI lifecycle can have different risk perspectives.\nFor example, an AI developer who makes AI software available, such as pre-trained mod-\nels, can have a different risk perspective than an AI actor who is responsible for deploying\nthat pre-trained model in a specific use case. Such deployers may not recognize that their\nparticular uses could entail risks which differ from those perceived by the initial developer.\nAll involved AI actors share responsibilities for designing, developing, and deploying a\ntrustworthy AI system that is fit for purpose.\nRisk in real-world settings: While measuring AI risks in a laboratory or a controlled\nenvironment may yield important insights pre-deployment, these measurements may differ\nfrom risks that emerge in operational, real-world settings.\nInscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability\ncan be a result of the opaque nature of AI systems (limited explainability or interpretabil-\nity), lack of transparency or documentation in AI system development or deployment, or\ninherent uncertainties in AI systems.\nHuman baseline: Risk management of AI systems that are intended to augment or replace\nhuman activity, for example decision making, requires some form of baseline metrics for\ncomparison. This is difficult to systematize since AI systems carry out different tasks \u2013 and\nperform tasks differently \u2013 than humans.\n",
      "rephrase": "To effectively measure the impact of AI on a population, it's important to consider the context, as different groups may be affected in various ways. Not all communities that might be harmed are direct users of the AI system. Risks can vary at different stages of the AI lifecycle; assessing risks early may yield different insights than evaluating them later, as some risks may not be apparent initially but can grow as AI systems change. Different stakeholders in the AI lifecycle, such as developers and deployers, may have different views on risks. For instance, a developer providing pre-trained AI models may not see the same risks as someone using those models in a specific application. All parties involved share the responsibility to create trustworthy AI systems that meet their intended purposes. Additionally, measuring AI risks in controlled environments can provide useful information, but these findings may not fully reflect the risks that arise in real-world situations. The complexity of AI systems can make risk measurement challenging, especially when the systems are difficult to understand or lack transparency. Finally, managing risks for AI systems that are meant to assist or replace human tasks requires a baseline for comparison, which is complicated because AI performs tasks differently than humans.",
      "entities": [
        {
          "name": "AI lifecycle",
          "type": "framework",
          "description": "The AI lifecycle refers to the stages involved in the development and deployment of AI systems, including design, development, and deployment."
        },
        {
          "name": "AI developer",
          "type": "person",
          "description": "An AI developer is an individual or organization that creates AI software, such as pre-trained models, and has a specific risk perspective regarding their use."
        },
        {
          "name": "AI deployer",
          "type": "person",
          "description": "An AI deployer is an individual or organization responsible for implementing AI models in specific use cases, potentially facing different risks than the developers."
        },
        {
          "name": "trustworthy AI system",
          "type": "system",
          "description": "A trustworthy AI system is designed to be reliable and fit for purpose, ensuring that all AI actors share responsibilities in its development and deployment."
        },
        {
          "name": "inscrutability",
          "type": "risk",
          "description": "Inscrutability refers to the challenges in understanding and measuring risks associated with AI systems due to their opaque nature and lack of transparency."
        },
        {
          "name": "human baseline",
          "type": "category",
          "description": "Human baseline refers to the metrics used for comparison when assessing AI systems that augment or replace human activities, particularly in decision-making."
        }
      ],
      "relations": [
        {
          "source": "AI developer",
          "target": "AI deployer",
          "type": "contributes_to",
          "description": "AI developers contribute to the deployment of AI systems by providing pre-trained models, which are then utilized by AI deployers.",
          "strength": 8
        },
        {
          "source": "AI lifecycle",
          "target": "trustworthy AI system",
          "type": "supports",
          "description": "The AI lifecycle supports the development of trustworthy AI systems by outlining the necessary stages and responsibilities involved.",
          "strength": 9
        },
        {
          "source": "inscrutability",
          "target": "risk management",
          "type": "identifies",
          "description": "Inscrutability identifies challenges in risk management for AI systems due to their opaque nature and limited explainability.",
          "strength": 7
        },
        {
          "source": "human baseline",
          "target": "risk management",
          "type": "provides",
          "description": "The human baseline provides a framework for comparing AI systems against human performance in decision-making tasks.",
          "strength": 8
        }
      ],
      "chunk_time": 12.12265325000044,
      "chunk_tokens": 384,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 13,
      "text": " that are intended to augment or replace\nhuman activity, for example decision making, requires some form of baseline metrics for\ncomparison. This is difficult to systematize since AI systems carry out different tasks \u2013 and\nperform tasks differently \u2013 than humans.\nPage 6\n\n\nNIST AI 100-1\nAI RMF 1.0\n1.2.2\nRisk Tolerance\nWhile the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk\ntolerance refers to the organization\u2019s or AI actor\u2019s (see Appendix A) readiness to bear the\nrisk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula-\ntory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that\nis acceptable to organizations or society are highly contextual and application and use-case\nspecific. Risk tolerances can be influenced by policies and norms established by AI sys-\ntem owners, organizations, industries, communities, or policy makers. Risk tolerances are\nlikely to change over time as AI systems, policies, and norms evolve. Different organiza-\ntions may have varied risk tolerances due to their particular organizational priorities and\nresource considerations.\nEmerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con-\ntinue to be developed and debated by businesses, governments, academia, and civil society.\nTo the extent that challenges for specifying AI risk tolerances remain unresolved, there may\nbe contexts where a risk management framework is not yet readily applicable for mitigating\nnegative AI risks.\nThe Framework is intended to be flexible and to augment existing risk practices\nwhich should align with applicable laws, regulations, and norms. Organizations\nshould follow existing regulations and guidelines for risk criteria, tolerance, and\nresponse established by organizational, domain, discipline, sector, or professional\nrequirements.",
      "rephrase": "AI systems that aim to enhance or replace human tasks, like decision-making, need baseline metrics for comparison. However, this is challenging because AI performs tasks differently than humans. \n\nThe NIST AI RMF (Artificial Intelligence Risk Management Framework) can help prioritize risks but does not define what risk tolerance is. Risk tolerance is the willingness of an organization or AI user to accept risk to achieve their goals. This tolerance can be shaped by legal and regulatory requirements. It varies based on the specific context and use case, and can be influenced by policies set by AI system owners, organizations, industries, communities, or policymakers. Risk tolerance may change over time as AI systems and related policies evolve. Different organizations may have different risk tolerances based on their unique priorities and resources. \n\nNew knowledge and methods for evaluating harm versus benefits will continue to be developed and discussed among businesses, governments, academia, and civil society. If challenges in defining AI risk tolerances remain, there may be situations where a risk management framework is not easily applicable to reduce negative AI risks. \n\nThe Framework is designed to be adaptable and should complement existing risk management practices that comply with relevant laws, regulations, and norms. Organizations should adhere to current regulations and guidelines regarding risk criteria, tolerance, and responses as established by their specific sector or professional standards.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) and its components."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0, which provides guidelines for managing risks associated with AI systems."
        },
        {
          "name": "Risk Tolerance",
          "type": "category",
          "description": "The readiness of an organization or AI actor to bear risks in order to achieve objectives, influenced by various contextual factors."
        },
        {
          "name": "ISO GUIDE 73",
          "type": "standard",
          "description": "A standard that provides guidelines for risk management, referenced in the context of risk tolerance."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Systems that utilize artificial intelligence to perform tasks, which may differ from human capabilities."
        },
        {
          "name": "organizations",
          "type": "category",
          "description": "Entities that may have varied risk tolerances based on their priorities and resource considerations."
        },
        {
          "name": "policies and norms",
          "type": "guideline",
          "description": "Established rules and standards that influence risk tolerances for AI systems."
        },
        {
          "name": "risk management framework",
          "type": "framework",
          "description": "A structured approach to identifying, assessing, and mitigating risks associated with AI."
        },
        {
          "name": "civil society",
          "type": "sector",
          "description": "A sector that contributes to the development and debate of methods for informing harm/cost-benefit tradeoffs in AI."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "Risk Tolerance",
          "type": "supports",
          "description": "The AI RMF supports the prioritization of risk tolerance in AI systems.",
          "strength": 8
        },
        {
          "source": "Risk Tolerance",
          "target": "ISO GUIDE 73",
          "type": "aligned_with",
          "description": "Risk tolerance is influenced by guidelines established in ISO GUIDE 73.",
          "strength": 7
        },
        {
          "source": "AI systems",
          "target": "organizations",
          "type": "applicable_to",
          "description": "AI systems are applicable to various organizations that may have different risk tolerances.",
          "strength": 9
        },
        {
          "source": "policies and norms",
          "target": "Risk Tolerance",
          "type": "influences",
          "description": "Policies and norms established by organizations influence their risk tolerances.",
          "strength": 8
        },
        {
          "source": "risk management framework",
          "target": "AI RMF 1.0",
          "type": "develops",
          "description": "The risk management framework is developed to augment existing practices like the AI RMF.",
          "strength": 7
        },
        {
          "source": "civil society",
          "target": "methods",
          "type": "contributes_to",
          "description": "Civil society contributes to the development of methods for harm/cost-benefit tradeoffs in AI.",
          "strength": 6
        }
      ],
      "chunk_time": 18.065462083002785,
      "chunk_tokens": 384,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 14,
      "text": " to augment existing risk practices\nwhich should align with applicable laws, regulations, and norms. Organizations\nshould follow existing regulations and guidelines for risk criteria, tolerance, and\nresponse established by organizational, domain, discipline, sector, or professional\nrequirements. Some sectors or industries may have established definitions of harm or\nestablished documentation, reporting, and disclosure requirements. Within sectors,\nrisk management may depend on existing guidelines for specific applications and\nuse case settings. Where established guidelines do not exist, organizations should\ndefine reasonable risk tolerance. Once tolerance is defined, this AI RMF can be used\nto manage risks and to document risk management processes.\n1.2.3\nRisk Prioritization\nAttempting to eliminate negative risk entirely can be counterproductive in practice because\nnot all incidents and failures can be eliminated. Unrealistic expectations about risk may\nlead organizations to allocate resources in a manner that makes risk triage inefficient or\nimpractical or wastes scarce resources. A risk management culture can help organizations\nrecognize that not all AI risks are the same, and resources can be allocated purposefully.\nActionable risk management efforts lay out clear guidelines for assessing trustworthiness\nof each AI system an organization develops or deploys. Policies and resources should be\nprioritized based on the assessed risk level and potential impact of an AI system. The extent\nto which an AI system may be customized or tailored to the specific context of use by the\nAI deployer can be a contributing factor.\nPage 7\n\n\nNIST AI 100-1\nAI RMF 1.0\nWhen applying the AI RMF, risks which the organization determines to be highest for the\nAI systems within a given context of use call for the most urgent prioritization and most\nthorough risk management process.",
      "rephrase": "Organizations should enhance their risk management practices to comply with relevant laws and regulations. They need to follow established guidelines for risk criteria, tolerance, and response based on their specific sector or professional standards. Some industries may have clear definitions of harm and specific requirements for documentation and reporting. In cases where no guidelines exist, organizations should set reasonable risk tolerance levels. Once these levels are established, the AI Risk Management Framework (AI RMF) can be used to manage and document risk management processes.\n\nWhen it comes to prioritizing risks, trying to eliminate all negative risks is often impractical, as not every incident can be avoided. Having unrealistic expectations about risk can lead to inefficient use of resources. A strong risk management culture helps organizations understand that not all AI risks are equal, allowing for better resource allocation. Effective risk management should provide clear guidelines for evaluating the trustworthiness of each AI system. Policies and resources should be prioritized based on the assessed risk level and the potential impact of the AI system. The ability to customize an AI system for its specific use context can also influence risk management priorities.\n\nWhen using the AI RMF, organizations should focus on the highest risks associated with their AI systems in a specific context, ensuring these risks receive urgent attention and thorough management.",
      "entities": [
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI RMF 1.0 is a framework designed to manage risks associated with AI systems, providing guidelines for organizations to prioritize and document their risk management processes."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "NIST AI 100-1 is a document that outlines standards and guidelines related to artificial intelligence risk management."
        },
        {
          "name": "risk management culture",
          "type": "category",
          "description": "A risk management culture refers to the organizational mindset that recognizes and addresses various AI risks, promoting purposeful allocation of resources."
        },
        {
          "name": "risk criteria",
          "type": "guideline",
          "description": "Risk criteria are established guidelines that help organizations assess and manage risks, including tolerance and response strategies."
        },
        {
          "name": "risk tolerance",
          "type": "standard",
          "description": "Risk tolerance is the level of risk that an organization is willing to accept while pursuing its objectives."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "AI systems are technological solutions that utilize artificial intelligence to perform tasks and make decisions."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "risk management culture",
          "type": "supports",
          "description": "The AI RMF 1.0 supports the development of a risk management culture within organizations by providing guidelines for risk assessment.",
          "strength": 8
        },
        {
          "source": "risk criteria",
          "target": "risk tolerance",
          "type": "aligned_with",
          "description": "Risk criteria are aligned with the concept of risk tolerance, helping organizations define acceptable levels of risk.",
          "strength": 7
        },
        {
          "source": "AI systems",
          "target": "risk management culture",
          "type": "applicable_to",
          "description": "AI systems are applicable to the risk management culture as they require specific risk assessments and management strategies.",
          "strength": 6
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document that outlines the standards and guidelines for the AI RMF 1.0 framework.",
          "strength": 9
        }
      ],
      "chunk_time": 13.6285782499981,
      "chunk_tokens": 362,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 15,
      "text": "AI RMF 1.0\nWhen applying the AI RMF, risks which the organization determines to be highest for the\nAI systems within a given context of use call for the most urgent prioritization and most\nthorough risk management process. In cases where an AI system presents unacceptable\nnegative risk levels \u2013 such as where significant negative impacts are imminent, severe harms\nare actually occurring, or catastrophic risks are present \u2013 development and deployment\nshould cease in a safe manner until risks can be sufficiently managed. If an AI system\u2019s\ndevelopment, deployment, and use cases are found to be low-risk in a specific context, that\nmay suggest potentially lower prioritization.\nRisk prioritization may differ between AI systems that are designed or deployed to directly\ninteract with humans as compared to AI systems that are not. Higher initial prioritization\nmay be called for in settings where the AI system is trained on large datasets comprised of\nsensitive or protected data such as personally identifiable information, or where the outputs\nof the AI systems have direct or indirect impact on humans. AI systems designed to interact\nonly with computational systems and trained on non-sensitive datasets (for example, data\ncollected from the physical environment) may call for lower initial prioritization. Nonethe-\nless, regularly assessing and prioritizing risk based on context remains important because\nnon-human-facing AI systems can have downstream safety or social implications.\nResidual risk \u2013 defined as risk remaining after risk treatment (Source: ISO GUIDE 73) \u2013\ndirectly impacts end users or affected individuals and communities. Documenting residual\nrisks will call for the system provider to fully consider the risks of deploying the AI product\nand will inform end users about potential negative impacts of interacting with the system.\n1.2.4\nOrganizational Integration and Management of Risk\nAI risks should not be considered in isolation. Different AI actors have different responsi-\nbilities and awareness depending on their roles in the lifecycle.",
      "rephrase": "AI RMF 1.0\nWhen using the AI RMF, organizations should focus on the highest risks associated with their AI systems based on how they will be used. If an AI system poses serious risks\u2014like imminent severe harm or catastrophic dangers\u2014its development and use should be paused safely until those risks are managed. Conversely, if an AI system is deemed low-risk in a specific context, it may require less immediate attention.\nRisk prioritization can vary between AI systems that interact directly with people and those that do not. Systems trained on sensitive data, such as personal information, or those that affect humans directly or indirectly should be prioritized more highly. In contrast, AI systems that only interact with other computer systems and use non-sensitive data may require less urgent attention. However, it is still crucial to regularly assess and prioritize risks based on context, as even non-human-facing AI can have safety or social consequences.\nResidual risk, which is the risk that remains after treatment (according to ISO GUIDE 73), affects end users and communities. Documenting these residual risks is essential for the system provider to understand the implications of deploying the AI product and to inform users about potential negative effects of using the system.\nOrganizational Integration and Management of Risk\nAI risks should not be viewed in isolation. Different stakeholders in the AI lifecycle have varying responsibilities and levels of awareness regarding these risks.",
      "entities": [
        {
          "name": "AI RMF 1.0",
          "type": "document",
          "description": "AI RMF 1.0 is a framework for managing risks associated with AI systems, emphasizing the need for prioritization and thorough risk management processes."
        },
        {
          "name": "ISO GUIDE 73",
          "type": "standard",
          "description": "ISO GUIDE 73 provides definitions and guidelines for risk management, including the concept of residual risk."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "AI systems refer to artificial intelligence technologies that can interact with humans or computational systems, with varying levels of risk based on their context of use."
        },
        {
          "name": "residual risk",
          "type": "risk",
          "description": "Residual risk is the risk that remains after risk treatment has been applied, impacting end users and communities."
        },
        {
          "name": "end users",
          "type": "person",
          "description": "End users are individuals or communities that interact with AI systems and are affected by the risks associated with their deployment."
        },
        {
          "name": "AI actors",
          "type": "organization",
          "description": "AI actors are various stakeholders involved in the lifecycle of AI systems, each with different responsibilities and levels of awareness regarding risks."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "AI systems",
          "type": "supports",
          "description": "AI RMF 1.0 supports the management of risks associated with AI systems by providing a structured approach to risk prioritization.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "residual risk",
          "type": "includes",
          "description": "AI RMF 1.0 includes the concept of residual risk, emphasizing its importance in understanding the impacts on end users.",
          "strength": 8
        },
        {
          "source": "ISO GUIDE 73",
          "target": "residual risk",
          "type": "defines",
          "description": "ISO GUIDE 73 defines residual risk and its implications for risk management practices.",
          "strength": 9
        },
        {
          "source": "AI actors",
          "target": "AI systems",
          "type": "manages",
          "description": "AI actors manage the risks associated with AI systems throughout their lifecycle, ensuring responsibilities are clearly defined.",
          "strength": 8
        },
        {
          "source": "end users",
          "target": "AI systems",
          "type": "affected_by",
          "description": "End users are affected by the risks associated with AI systems, particularly in terms of potential negative impacts.",
          "strength": 10
        }
      ],
      "chunk_time": 14.664312249995419,
      "chunk_tokens": 396,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 16,
      "text": " impacts of interacting with the system.\n1.2.4\nOrganizational Integration and Management of Risk\nAI risks should not be considered in isolation. Different AI actors have different responsi-\nbilities and awareness depending on their roles in the lifecycle. For example, organizations\ndeveloping an AI system often will not have information about how the system may be\nused. AI risk management should be integrated and incorporated into broader enterprise\nrisk management strategies and processes. Treating AI risks along with other critical risks,\nsuch as cybersecurity and privacy, will yield a more integrated outcome and organizational\nefficiencies.\nThe AI RMF may be utilized along with related guidance and frameworks for managing\nAI system risks or broader enterprise risks. Some risks related to AI systems are common\nacross other types of software development and deployment. Examples of overlapping risks\ninclude: privacy concerns related to the use of underlying data to train AI systems; the en-\nergy and environmental implications associated with resource-heavy computing demands;\nsecurity concerns related to the confidentiality, integrity, and availability of the system and\nits training and output data; and general security of the underlying software and hardware\nfor AI systems.\nPage 8\n\n\nNIST AI 100-1\nAI RMF 1.0\nOrganizations need to establish and maintain the appropriate accountability mechanisms,\nroles and responsibilities, culture, and incentive structures for risk management to be ef-\nfective. Use of the AI RMF alone will not lead to these changes or provide the appropriate\nincentives. Effective risk management is realized through organizational commitment at\nsenior levels and may require cultural change within an organization or industry. In addi-\ntion, small to medium-sized organizations managing AI risks or implementing the AI RMF\nmay face different challenges than large organizations, depending on their capabilities and\nresources.\n2.\n",
      "rephrase": "1.2.4 Organizational Integration and Management of Risk\n\nAI risks should be viewed in the context of the entire organization. Different people involved in AI development have varying responsibilities and levels of awareness based on their roles. For instance, organizations that create AI systems may not know how those systems will be used. Therefore, managing AI risks should be part of the overall risk management strategies of the organization. By addressing AI risks alongside other important risks, like cybersecurity and privacy, organizations can achieve better integration and efficiency.\n\nThe AI Risk Management Framework (AI RMF) can be used alongside other guidelines for managing AI and enterprise risks. Some risks associated with AI systems are similar to those found in other software development processes. Common risks include: privacy issues from using data to train AI; environmental impacts from high energy consumption; security risks related to the confidentiality and integrity of the system and its data; and the overall security of the software and hardware used in AI systems.\n\nOrganizations must create and uphold clear accountability, roles, responsibilities, culture, and incentives for effective risk management. Simply using the AI RMF will not bring about these changes or provide the necessary incentives. Successful risk management requires commitment from senior leadership and may involve cultural shifts within the organization or industry. Additionally, small to medium-sized organizations may face different challenges in managing AI risks compared to larger organizations, based on their resources and capabilities.",
      "entities": [
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI RMF 1.0 is a framework designed to manage risks associated with AI systems, integrating these risks into broader enterprise risk management strategies."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "NIST AI 100-1 is a document that outlines guidelines and standards for managing AI risks and integrating them into organizational practices."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "AI risks refer to the potential negative impacts and uncertainties associated with the development and deployment of AI systems."
        },
        {
          "name": "cybersecurity",
          "type": "category",
          "description": "Cybersecurity encompasses the practices and technologies used to protect systems, networks, and data from cyber threats."
        },
        {
          "name": "privacy",
          "type": "category",
          "description": "Privacy concerns relate to the protection of personal data and the ethical use of information in AI systems."
        },
        {
          "name": "small to medium-sized organizations",
          "type": "organization",
          "description": "These organizations may face unique challenges in managing AI risks compared to larger organizations due to their limited resources and capabilities."
        },
        {
          "name": "large organizations",
          "type": "organization",
          "description": "Large organizations typically have more resources and capabilities to manage AI risks effectively compared to smaller counterparts."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "AI risks",
          "type": "supports",
          "description": "The AI RMF 1.0 supports the management of AI risks by providing a structured framework for integration into enterprise risk management.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "provides",
          "description": "NIST AI 100-1 provides guidelines and standards that inform the implementation of the AI RMF 1.0.",
          "strength": 8
        },
        {
          "source": "AI risks",
          "target": "cybersecurity",
          "type": "aligned_with",
          "description": "AI risks are aligned with cybersecurity risks, as both involve critical concerns regarding data protection and system integrity.",
          "strength": 7
        },
        {
          "source": "AI risks",
          "target": "privacy",
          "type": "aligned_with",
          "description": "AI risks are aligned with privacy concerns, particularly regarding the use of data in AI systems.",
          "strength": 7
        },
        {
          "source": "small to medium-sized organizations",
          "target": "AI risks",
          "type": "faces",
          "description": "Small to medium-sized organizations face unique challenges in managing AI risks due to their limited resources.",
          "strength": 6
        },
        {
          "source": "large organizations",
          "target": "AI risks",
          "type": "manages",
          "description": "Large organizations typically have more resources to manage AI risks effectively compared to smaller organizations.",
          "strength": 8
        }
      ],
      "chunk_time": 15.975374458997976,
      "chunk_tokens": 370,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 17,
      "text": " and may require cultural change within an organization or industry. In addi-\ntion, small to medium-sized organizations managing AI risks or implementing the AI RMF\nmay face different challenges than large organizations, depending on their capabilities and\nresources.\n2.\nAudience\nIdentifying and managing AI risks and potential impacts \u2013 both positive and negative \u2013 re-\nquires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will\nrepresent a diversity of experience, expertise, and backgrounds and comprise demograph-\nically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors\nacross the AI lifecycle and dimensions.\nThe OECD has developed a framework for classifying AI lifecycle activities according to\nfive key socio-technical dimensions, each with properties relevant for AI policy and gover-\nnance, including risk management [OECD (2022) OECD Framework for the Classification\nof AI systems \u2014 OECD Digital Economy Papers]. Figure 2 shows these dimensions,\nslightly modified by NIST for purposes of this framework. The NIST modification high-\nlights the importance of test, evaluation, verification, and validation (TEVV) processes\nthroughout an AI lifecycle and generalizes the operational context of an AI system.\nAI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI\nModel, and Task and Output. AI actors involved in these dimensions who perform or\nmanage the design, development, deployment, evaluation, and use of AI systems and drive\nAI risk management efforts are the primary AI RMF audience.\nRepresentative AI actors across the lifecycle dimensions are listed in Figure 3 and described\nin detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks\nand achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific\nexpertise are integrated throughout the AI lifecycle and are especially likely to benefit from\nthe Framework.",
      "rephrase": "Managing AI risks may require changes in culture within organizations or industries. Small to medium-sized organizations may face different challenges than large ones when dealing with AI risks or implementing the AI Risk Management Framework (AI RMF), based on their resources and capabilities.\n\nThe process of identifying and managing AI risks, both positive and negative, involves a variety of perspectives and participants throughout the AI lifecycle. It is important for these participants to come from diverse backgrounds and have different experiences and expertise. The AI RMF is designed for use by all actors involved in the AI lifecycle.\n\nThe OECD has created a framework that classifies AI lifecycle activities into five key socio-technical dimensions relevant for AI policy and governance, including risk management. NIST has slightly modified this framework to emphasize the importance of testing, evaluation, verification, and validation (TEVV) processes throughout the AI lifecycle and to generalize the operational context of AI systems. \n\nThe dimensions highlighted include Application Context, Data and Input, AI Model, and Task and Output. The main audience for the AI RMF consists of AI actors who are involved in designing, developing, deploying, evaluating, and using AI systems, and who are responsible for managing AI risks. \n\nFigure 3 lists representative AI actors across these lifecycle dimensions, with detailed descriptions in Appendix A. All AI actors within the AI RMF collaborate to manage risks and promote trustworthy and responsible AI. Those with TEVV expertise are integrated throughout the AI lifecycle and will particularly benefit from this framework.",
      "entities": [
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "The AI RMF (Artificial Intelligence Risk Management Framework) is intended to be used by AI actors across the AI lifecycle and dimensions to manage risks and achieve the goals of trustworthy and responsible AI."
        },
        {
          "name": "OECD",
          "type": "organization",
          "description": "The OECD (Organisation for Economic Co-operation and Development) has developed a framework for classifying AI lifecycle activities according to five key socio-technical dimensions relevant for AI policy and governance."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "NIST (National Institute of Standards and Technology) modified the OECD framework to highlight the importance of test, evaluation, verification, and validation (TEVV) processes throughout the AI lifecycle."
        },
        {
          "name": "AI actors",
          "type": "category",
          "description": "AI actors represent a diversity of experience, expertise, and backgrounds and are involved in the design, development, deployment, evaluation, and use of AI systems."
        },
        {
          "name": "TEVV processes",
          "type": "function",
          "description": "TEVV (test, evaluation, verification, and validation) processes are crucial throughout the AI lifecycle to ensure the reliability and safety of AI systems."
        },
        {
          "name": "AI lifecycle",
          "type": "domain",
          "description": "The AI lifecycle encompasses all stages of AI system development and use, including design, development, deployment, evaluation, and management of AI risks."
        }
      ],
      "relations": [
        {
          "source": "AI RMF",
          "target": "AI actors",
          "type": "supports",
          "description": "The AI RMF supports AI actors in managing risks and achieving trustworthy AI.",
          "strength": 9
        },
        {
          "source": "OECD",
          "target": "AI lifecycle activities",
          "type": "develops",
          "description": "The OECD developed a framework for classifying AI lifecycle activities according to socio-technical dimensions.",
          "strength": 8
        },
        {
          "source": "NIST",
          "target": "OECD framework",
          "type": "updated_by",
          "description": "NIST modified the OECD framework to emphasize TEVV processes in the AI lifecycle.",
          "strength": 7
        },
        {
          "source": "AI actors",
          "target": "AI lifecycle",
          "type": "applicable_to",
          "description": "AI actors are involved in various stages of the AI lifecycle.",
          "strength": 8
        },
        {
          "source": "TEVV processes",
          "target": "AI lifecycle",
          "type": "infused_throughout",
          "description": "TEVV processes are integrated throughout the AI lifecycle to ensure effective risk management.",
          "strength": 9
        }
      ],
      "chunk_time": 13.98073087499506,
      "chunk_tokens": 391,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 18,
      "text": " the AI RMF, all AI actors work together to manage risks\nand achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific\nexpertise are integrated throughout the AI lifecycle and are especially likely to benefit from\nthe Framework. Performed regularly, TEVV tasks can provide insights relative to technical,\nsocietal, legal, and ethical standards or norms, and can assist with anticipating impacts and\nassessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV\nallows for both mid-course remediation and post-hoc risk management.\nThe People & Planet dimension at the center of Figure 2 represents human rights and the\nbroader well-being of society and the planet. The AI actors in this dimension comprise\na separate AI RMF audience who informs the primary audience. These AI actors may in-\nclude trade associations, standards developing organizations, researchers, advocacy groups,\nPage 9\n\n\nNIST AI 100-1\nAI RMF 1.0\nFig. 2. Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD\nFramework for the Classification of AI systems \u2014 OECD Digital Economy Papers. The two inner\ncircles show AI systems\u2019 key dimensions and the outer circle shows AI lifecycle stages. Ideally,\nrisk management efforts start with the Plan and Design function in the application context and are\nperformed throughout the AI system lifecycle. See Figure 3 for representative AI actors.\nenvironmental groups, civil society organizations, end users, and potentially impacted in-\ndividuals and communities.",
      "rephrase": "The AI Risk Management Framework (AI RMF) encourages collaboration among all AI stakeholders to manage risks and promote trustworthy and responsible AI. Experts in Technology, Ethics, Values, and Verification (TEVV) are involved throughout the AI lifecycle and can greatly benefit from this Framework. Regular TEVV tasks help provide insights into technical, societal, legal, and ethical standards, allowing for better anticipation of impacts and assessment of emerging risks. By incorporating TEVV as a regular part of the AI lifecycle, organizations can address issues during development and manage risks after deployment.\n\nThe 'People & Planet' aspect, shown in Figure 2, focuses on human rights and the overall well-being of society and the environment. The AI actors in this area form a distinct audience within the AI RMF, providing input to the main audience. These actors may include trade associations, organizations that develop standards, researchers, advocacy groups, environmental organizations, civil society groups, end users, and individuals or communities that may be affected by AI systems.\n\nFigure 2 illustrates the lifecycle and key dimensions of an AI system, based on the OECD Framework for the Classification of AI systems. The inner circles represent the main dimensions of AI systems, while the outer circle outlines the stages of the AI lifecycle. Ideally, risk management should begin with the planning and design phase and continue throughout the entire lifecycle of the AI system. Figure 3 provides examples of relevant AI actors.",
      "entities": [
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "The AI RMF is a framework designed to manage risks and achieve the goals of trustworthy and responsible AI."
        },
        {
          "name": "TEVV",
          "type": "function",
          "description": "TEVV refers to tasks that provide insights into technical, societal, legal, and ethical standards, assisting in risk management throughout the AI lifecycle."
        },
        {
          "name": "People & Planet dimension",
          "type": "category",
          "description": "This dimension represents human rights and the broader well-being of society and the planet within the AI RMF context."
        },
        {
          "name": "OECD Framework for the Classification of AI systems",
          "type": "standard",
          "description": "A framework developed by OECD for classifying AI systems, referenced in the context of AI lifecycle and dimensions."
        },
        {
          "name": "AI actors",
          "type": "organization",
          "description": "AI actors include various stakeholders such as trade associations, standards developing organizations, researchers, and advocacy groups involved in the AI lifecycle."
        }
      ],
      "relations": [
        {
          "source": "AI RMF",
          "target": "TEVV",
          "type": "supports",
          "description": "The AI RMF supports the integration of TEVV tasks throughout the AI lifecycle.",
          "strength": 9
        },
        {
          "source": "TEVV",
          "target": "AI lifecycle",
          "type": "infused_throughout",
          "description": "TEVV tasks are integrated throughout the AI lifecycle to manage risks effectively.",
          "strength": 8
        },
        {
          "source": "People & Planet dimension",
          "target": "AI actors",
          "type": "includes",
          "description": "The People & Planet dimension includes AI actors who inform the primary audience regarding societal impacts.",
          "strength": 7
        },
        {
          "source": "OECD Framework for the Classification of AI systems",
          "target": "AI RMF",
          "type": "aligned_with",
          "description": "The OECD framework is aligned with the principles outlined in the AI RMF.",
          "strength": 8
        }
      ],
      "chunk_time": 10.746756874999846,
      "chunk_tokens": 321,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 19,
      "text": " Plan and Design function in the application context and are\nperformed throughout the AI system lifecycle. See Figure 3 for representative AI actors.\nenvironmental groups, civil society organizations, end users, and potentially impacted in-\ndividuals and communities. These actors can:\n\u2022 assist in providing context and understanding potential and actual impacts;\n\u2022 be a source of formal or quasi-formal norms and guidance for AI risk management;\n\u2022 designate boundaries for AI operation (technical, societal, legal, and ethical); and\n\u2022 promote discussion of the tradeoffs needed to balance societal values and priorities\nrelated to civil liberties and rights, equity, the environment and the planet, and the\neconomy.\nSuccessful risk management depends upon a sense of collective responsibility among AI\nactors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse\nperspectives, disciplines, professions, and experiences. Diverse teams contribute to more\nopen sharing of ideas and assumptions about the purposes and functions of technology \u2013\nmaking these implicit aspects more explicit. This broader collective perspective creates\nopportunities for surfacing problems and identifying existing and emergent risks.\nPage 10\n\n\nNIST AI 100-1\nAI RMF 1.0\nFig. 3. AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing,\nevaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with\nthose building and using the models separated from those verifying and validating the models.\nPage 11\n\n\nNIST AI 100-1\nAI RMF 1.0\n3.\nAI Risks and Trustworthiness\nFor AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-\nteria that are of value to interested parties.",
      "rephrase": "The Plan and Design functions are important in the context of AI applications and are carried out throughout the entire lifecycle of an AI system. Figure 3 illustrates the key AI actors involved, which include environmental groups, civil society organizations, end users, and individuals or communities that may be affected. These actors can: \n- Provide context and insights on the potential and actual impacts of AI; \n- Offer formal or informal guidelines for managing AI risks; \n- Set boundaries for AI operations in terms of technical, societal, legal, and ethical aspects; \n- Encourage discussions about the trade-offs necessary to balance societal values, such as civil liberties, equity, environmental concerns, and economic priorities. \n\nEffective risk management relies on a shared sense of responsibility among the AI actors shown in Figure 3. The AI Risk Management Framework (AI RMF) functions, detailed in Section 5, benefit from a variety of perspectives, disciplines, and experiences. Diverse teams foster open discussions about the goals and functions of technology, making previously unspoken assumptions clear. This collective viewpoint helps identify problems and recognize both existing and emerging risks. \n\nFigure 3 also shows AI actors at different stages of the AI lifecycle. For more information on the tasks of these actors, including testing, evaluation, verification, and validation, refer to Appendix A. Note that in the AI Model dimension (Figure 2), those who build and use the models are separated from those who verify and validate them. \n\nFor AI systems to be considered trustworthy, they must often meet various criteria that are important to stakeholders.",
      "entities": [
        {
          "name": "Plan and Design function",
          "type": "function",
          "description": "A function performed throughout the AI system lifecycle, focusing on planning and designing AI applications."
        },
        {
          "name": "AI system lifecycle",
          "type": "system",
          "description": "The stages through which an AI system progresses, from conception to deployment and beyond."
        },
        {
          "name": "environmental groups",
          "type": "organization",
          "description": "Organizations focused on environmental issues that can provide context and understanding of AI impacts."
        },
        {
          "name": "civil society organizations",
          "type": "organization",
          "description": "Organizations that represent the interests of the public and can contribute to AI risk management."
        },
        {
          "name": "end users",
          "type": "person",
          "description": "Individuals who use AI systems and can provide insights into their impacts and risks."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework that outlines functions and perspectives necessary for effective AI risk management."
        },
        {
          "name": "AI actors",
          "type": "category",
          "description": "Various stakeholders involved in the AI lifecycle, including those who build, use, verify, and validate AI models."
        },
        {
          "name": "AI Risks and Trustworthiness",
          "type": "category",
          "description": "A focus area that addresses the criteria necessary for AI systems to be considered trustworthy."
        }
      ],
      "relations": [
        {
          "source": "Plan and Design function",
          "target": "AI system lifecycle",
          "type": "supports",
          "description": "The Plan and Design function supports the various stages of the AI system lifecycle.",
          "strength": 8
        },
        {
          "source": "environmental groups",
          "target": "AI risk management",
          "type": "contributes_to",
          "description": "Environmental groups contribute to AI risk management by providing context and understanding of potential impacts.",
          "strength": 7
        },
        {
          "source": "civil society organizations",
          "target": "AI risk management",
          "type": "contributes_to",
          "description": "Civil society organizations contribute to AI risk management by offering norms and guidance.",
          "strength": 7
        },
        {
          "source": "end users",
          "target": "AI risk management",
          "type": "contributes_to",
          "description": "End users contribute to AI risk management by sharing their experiences and insights.",
          "strength": 6
        },
        {
          "source": "AI RMF 1.0",
          "target": "AI actors",
          "type": "includes",
          "description": "The AI RMF 1.0 includes diverse AI actors necessary for effective risk management.",
          "strength": 9
        },
        {
          "source": "AI Risks and Trustworthiness",
          "target": "AI RMF 1.0",
          "type": "aligned_with",
          "description": "AI Risks and Trustworthiness aligns with the principles outlined in the AI RMF 1.0.",
          "strength": 8
        }
      ],
      "chunk_time": 16.224092500000552,
      "chunk_tokens": 387,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 20,
      "text": "1\nAI RMF 1.0\n3.\nAI Risks and Trustworthiness\nFor AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-\nteria that are of value to interested parties. Approaches which enhance AI trustworthiness\ncan reduce negative AI risks. This Framework articulates the following characteristics of\ntrustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI\nsystems include: valid and reliable, safe, secure and resilient, accountable and trans-\nparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias\nmanaged. Creating trustworthy AI requires balancing each of these characteristics based\non the AI system\u2019s context of use. While all characteristics are socio-technical system at-\ntributes, accountability and transparency also relate to the processes and activities internal\nto an AI system and its external setting. Neglecting these characteristics can increase the\nprobability and magnitude of negative consequences.\nFig. 4. Characteristics of trustworthy AI systems. Valid & Reliable is a necessary condition of\ntrustworthiness and is shown as the base for other trustworthiness characteristics. Accountable &\nTransparent is shown as a vertical box because it relates to all other characteristics.\nTrustworthiness characteristics (shown in Figure 4) are inextricably tied to social and orga-\nnizational behavior, the datasets used by AI systems, selection of AI models and algorithms\nand the decisions made by those who build them, and the interactions with the humans who\nprovide insight from and oversight of such systems. Human judgment should be employed\nwhen deciding on the specific metrics related to AI trustworthiness characteristics and the\nprecise threshold values for those metrics.\nAddressing AI trustworthiness characteristics individually will not ensure AI system trust-\nworthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-\nting, and some will be more or less important in any given situation.",
      "rephrase": "AI RMF 1.0 outlines the importance of trustworthiness in AI systems. For AI to be considered trustworthy, it must meet various criteria valued by stakeholders. Enhancing trustworthiness can help minimize negative risks associated with AI. The framework identifies key characteristics of trustworthy AI, which include: being valid and reliable, safe, secure, resilient, accountable, transparent, explainable, interpretable, privacy-focused, and fair while managing harmful biases. Achieving trustworthy AI requires balancing these characteristics based on how the AI system will be used. While all these traits are related to social and technical aspects, accountability and transparency are particularly important for both the internal processes of the AI system and its external environment. Ignoring these traits can lead to increased risks and negative outcomes. The characteristics of trustworthy AI are interconnected with social behavior, the data used, the choice of AI models and algorithms, and the decisions made by developers, as well as the interactions with users who provide oversight. Human judgment is essential in determining the specific metrics for trustworthiness and their acceptable levels. Addressing each characteristic separately won't guarantee overall trustworthiness, as trade-offs are often necessary, and not all characteristics will be relevant in every situation.",
      "entities": [
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "A framework that articulates characteristics of trustworthy AI and offers guidance for addressing them."
        },
        {
          "name": "trustworthy AI",
          "type": "category",
          "description": "AI systems that are valid, reliable, safe, secure, resilient, accountable, transparent, explainable, interpretable, privacy-enhanced, and fair with harmful bias managed."
        },
        {
          "name": "AI trustworthiness characteristics",
          "type": "function",
          "description": "Characteristics that define the trustworthiness of AI systems, including accountability and transparency."
        },
        {
          "name": "social and organizational behavior",
          "type": "domain",
          "description": "The behaviors and practices within social and organizational contexts that influence AI trustworthiness."
        },
        {
          "name": "datasets",
          "type": "resource",
          "description": "Data collections used by AI systems that impact their trustworthiness."
        },
        {
          "name": "AI models and algorithms",
          "type": "technology",
          "description": "The computational methods and structures that underpin AI systems and affect their trustworthiness."
        },
        {
          "name": "human judgment",
          "type": "function",
          "description": "The decision-making process employed by humans to determine metrics related to AI trustworthiness."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "trustworthy AI",
          "type": "supports",
          "description": "AI RMF 1.0 provides guidance for achieving trustworthy AI.",
          "strength": 9
        },
        {
          "source": "trustworthy AI",
          "target": "AI trustworthiness characteristics",
          "type": "includes",
          "description": "Trustworthy AI encompasses various characteristics that define its trustworthiness.",
          "strength": 8
        },
        {
          "source": "AI trustworthiness characteristics",
          "target": "social and organizational behavior",
          "type": "reflects",
          "description": "AI trustworthiness characteristics are influenced by social and organizational behavior.",
          "strength": 7
        },
        {
          "source": "AI trustworthiness characteristics",
          "target": "datasets",
          "type": "applicable_to",
          "description": "Datasets used by AI systems are relevant to the evaluation of AI trustworthiness characteristics.",
          "strength": 8
        },
        {
          "source": "AI trustworthiness characteristics",
          "target": "AI models and algorithms",
          "type": "influences",
          "description": "The selection of AI models and algorithms affects the trustworthiness characteristics of AI systems.",
          "strength": 8
        },
        {
          "source": "human judgment",
          "target": "AI trustworthiness characteristics",
          "type": "contributes_to",
          "description": "Human judgment is essential in determining the metrics related to AI trustworthiness characteristics.",
          "strength": 9
        }
      ],
      "chunk_time": 14.6874239999961,
      "chunk_tokens": 395,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 21,
      "text": " metrics.\nAddressing AI trustworthiness characteristics individually will not ensure AI system trust-\nworthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-\nting, and some will be more or less important in any given situation. Ultimately, trustwor-\nthiness is a social concept that ranges across a spectrum and is only as strong as its weakest\ncharacteristics.\nWhen managing AI risks, organizations can face difficult decisions in balancing these char-\nacteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for\ninterpretability and achieving privacy. In other cases, organizations might face a tradeoff\nbetween predictive accuracy and interpretability. Or, under certain conditions such as data\nsparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions\nPage 12\n\n\nNIST AI 100-1\nAI RMF 1.0\nabout fairness and other values in certain domains. Dealing with tradeoffs requires tak-\ning into account the decision-making context. These analyses can highlight the existence\nand extent of tradeoffs between different measures, but they do not answer questions about\nhow to navigate the tradeoff. Those depend on the values at play in the relevant context and\nshould be resolved in a manner that is both transparent and appropriately justifiable.\nThere are multiple approaches for enhancing contextual awareness in the AI lifecycle. For\nexample, subject matter experts can assist in the evaluation of TEVV findings and work\nwith product and deployment teams to align TEVV parameters to requirements and de-\nployment conditions. When properly resourced, increasing the breadth and diversity of\ninput from interested parties and relevant AI actors throughout the AI lifecycle can en-\nhance opportunities for informing contextually sensitive evaluations, and for identifying\nAI system benefits and positive impacts. These practices can increase the likelihood that\nrisks arising in social contexts are managed appropriately.\n",
      "rephrase": "Addressing the individual characteristics of AI trustworthiness alone won't guarantee that an AI system is trustworthy. There are often trade-offs involved, and not all characteristics will be relevant in every situation. Some characteristics may be more important than others depending on the context. Trustworthiness is a social concept that varies and is only as strong as its weakest aspect.\n\nOrganizations managing AI risks often face tough choices when balancing these characteristics. For instance, they may need to choose between making an AI system easy to understand (interpretability) and protecting user privacy. In other cases, they might have to decide between accuracy in predictions and interpretability. Additionally, using privacy-enhancing techniques can sometimes reduce accuracy, which can impact fairness and other important values, especially when data is limited.\n\nTo navigate these trade-offs, organizations must consider the specific decision-making context. While analyses can reveal the existence and extent of trade-offs, they do not provide solutions on how to manage them. The resolution depends on the values relevant to the situation and should be handled transparently and justifiably.\n\nThere are various ways to improve contextual awareness throughout the AI lifecycle. For example, subject matter experts can help evaluate findings and collaborate with product and deployment teams to ensure that parameters align with requirements and conditions. By involving a diverse range of stakeholders throughout the AI lifecycle, organizations can better assess context-sensitive evaluations and identify the benefits and positive impacts of AI systems. This approach can enhance the management of risks that arise in social contexts.",
      "entities": [
        {
          "name": "AI trustworthiness",
          "type": "concept",
          "description": "A social concept that encompasses various characteristics of AI systems, emphasizing that trustworthiness is influenced by tradeoffs among these characteristics."
        },
        {
          "name": "organizations",
          "type": "category",
          "description": "Entities that manage AI risks and face decisions regarding the balance of AI trustworthiness characteristics."
        },
        {
          "name": "interpretability",
          "type": "characteristic",
          "description": "The degree to which an AI system's decisions can be understood by humans, which may conflict with other characteristics like privacy."
        },
        {
          "name": "privacy",
          "type": "characteristic",
          "description": "The protection of personal data, which can sometimes be at odds with the interpretability of AI systems."
        },
        {
          "name": "predictive accuracy",
          "type": "characteristic",
          "description": "The ability of an AI system to make correct predictions, which may be compromised in favor of interpretability."
        },
        {
          "name": "data sparsity",
          "type": "condition",
          "description": "A situation where there is insufficient data, which can lead to tradeoffs in AI system performance."
        },
        {
          "name": "TEVV findings",
          "type": "resource",
          "description": "Findings related to Trustworthiness, Explainability, Verifiability, and Validity that can be evaluated by subject matter experts."
        },
        {
          "name": "AI lifecycle",
          "type": "process",
          "description": "The stages through which an AI system progresses, from development to deployment, where contextual awareness can be enhanced."
        },
        {
          "name": "contextually sensitive evaluations",
          "type": "practice",
          "description": "Evaluations that take into account the specific context in which an AI system operates, aiming to identify benefits and risks."
        }
      ],
      "relations": [
        {
          "source": "organizations",
          "target": "AI trustworthiness",
          "type": "manages",
          "description": "Organizations manage the characteristics of AI trustworthiness while facing tradeoffs.",
          "strength": 8
        },
        {
          "source": "interpretability",
          "target": "privacy",
          "type": "tradeoff",
          "description": "There is a tradeoff between optimizing for interpretability and achieving privacy in AI systems.",
          "strength": 7
        },
        {
          "source": "predictive accuracy",
          "target": "interpretability",
          "type": "tradeoff",
          "description": "A tradeoff exists between predictive accuracy and interpretability in certain scenarios.",
          "strength": 7
        },
        {
          "source": "data sparsity",
          "target": "predictive accuracy",
          "type": "affects",
          "description": "Data sparsity can negatively affect predictive accuracy when using privacy-enhancing techniques.",
          "strength": 6
        },
        {
          "source": "subject matter experts",
          "target": "TEVV findings",
          "type": "evaluates",
          "description": "Subject matter experts evaluate TEVV findings to align them with deployment conditions.",
          "strength": 8
        },
        {
          "source": "AI lifecycle",
          "target": "contextually sensitive evaluations",
          "type": "supports",
          "description": "Enhancing contextual awareness in the AI lifecycle supports contextually sensitive evaluations.",
          "strength": 9
        }
      ],
      "chunk_time": 17.77071375000378,
      "chunk_tokens": 381,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 22,
      "text": " and relevant AI actors throughout the AI lifecycle can en-\nhance opportunities for informing contextually sensitive evaluations, and for identifying\nAI system benefits and positive impacts. These practices can increase the likelihood that\nrisks arising in social contexts are managed appropriately.\nUnderstanding and treatment of trustworthiness characteristics depends on an AI actor\u2019s\nparticular role within the AI lifecycle. For any given AI system, an AI designer or developer\nmay have a different perception of the characteristics than the deployer.\nTrustworthiness characteristics explained in this document influence each other.\nHighly secure but unfair systems, accurate but opaque and uninterpretable systems,\nand inaccurate but secure, privacy-enhanced, and transparent systems are all unde-\nsirable. A comprehensive approach to risk management calls for balancing tradeoffs\namong the trustworthiness characteristics. It is the joint responsibility of all AI ac-\ntors to determine whether AI technology is an appropriate or necessary tool for a\ngiven context or purpose, and how to use it responsibly. The decision to commission\nor deploy an AI system should be based on a contextual assessment of trustworthi-\nness characteristics and the relative risks, impacts, costs, and benefits, and informed\nby a broad set of interested parties.\n3.1\nValid and Reliable\nValidation is the \u201cconfirmation, through the provision of objective evidence, that the re-\nquirements for a specific intended use or application have been fulfilled\u201d (Source: ISO\n9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener-\nalized to data and settings beyond their training creates and increases negative AI risks and\nreduces trustworthiness.\nReliability is defined in the same standard as the \u201cability of an item to perform as required,\nwithout failure, for a given time interval, under given conditions\u201d (Source: ISO/IEC TS\n5723:2022).",
      "rephrase": "Involving relevant AI stakeholders throughout the AI lifecycle can improve evaluations that consider the specific context and help identify the benefits and positive impacts of AI systems. This approach can also better manage risks that arise in social situations. The understanding of trustworthiness in AI depends on the role of each stakeholder in the AI lifecycle. For example, an AI designer or developer may view trustworthiness differently than the person deploying the AI system. The characteristics of trustworthiness discussed in this document are interconnected. Systems that are secure but unfair, accurate but difficult to understand, or inaccurate yet secure and transparent are all problematic. A thorough risk management strategy must balance these trustworthiness characteristics. It is the shared responsibility of all AI stakeholders to assess whether AI technology is suitable for a specific context and how to use it responsibly. Decisions to create or implement an AI system should be based on a careful evaluation of trustworthiness characteristics, along with the associated risks, impacts, costs, and benefits, and should involve a wide range of interested parties. Validation is the process of confirming, with objective evidence, that an AI system meets the requirements for its intended use (Source: ISO 9000:2015). Deploying AI systems that are inaccurate, unreliable, or not well-suited to new data and settings increases negative risks and undermines trustworthiness. Reliability, as defined by the same standard, is the ability of a system to perform as expected without failure for a specified time and under certain conditions (Source: ISO/IEC TS 5723:2022).",
      "entities": [
        {
          "name": "AI lifecycle",
          "type": "framework",
          "description": "The AI lifecycle refers to the stages and processes involved in the development, deployment, and evaluation of AI systems."
        },
        {
          "name": "trustworthiness characteristics",
          "type": "category",
          "description": "Trustworthiness characteristics are attributes that determine the reliability, fairness, and transparency of AI systems."
        },
        {
          "name": "AI designer",
          "type": "person",
          "description": "An AI designer is an individual responsible for creating the design and specifications of an AI system."
        },
        {
          "name": "AI developer",
          "type": "person",
          "description": "An AI developer is a person who builds and implements AI systems based on the designs provided."
        },
        {
          "name": "AI deployer",
          "type": "person",
          "description": "An AI deployer is the individual or organization that implements and operates the AI system in a real-world context."
        },
        {
          "name": "ISO 9000:2015",
          "type": "standard",
          "description": "ISO 9000:2015 is an international standard that specifies requirements for a quality management system."
        },
        {
          "name": "ISO/IEC TS 5723:2022",
          "type": "standard",
          "description": "ISO/IEC TS 5723:2022 is a technical specification that defines reliability in the context of systems and products."
        },
        {
          "name": "AI technology",
          "type": "technology",
          "description": "AI technology encompasses the tools and systems that utilize artificial intelligence to perform tasks that typically require human intelligence."
        },
        {
          "name": "negative AI risks",
          "type": "risk",
          "description": "Negative AI risks refer to the potential adverse effects and consequences that arise from the deployment of AI systems."
        }
      ],
      "relations": [
        {
          "source": "trustworthiness characteristics",
          "target": "AI lifecycle",
          "type": "influences",
          "description": "Trustworthiness characteristics influence the evaluation and management of AI systems throughout the AI lifecycle.",
          "strength": 9
        },
        {
          "source": "AI designer",
          "target": "trustworthiness characteristics",
          "type": "perceives",
          "description": "AI designers may have a different perception of trustworthiness characteristics compared to other AI actors.",
          "strength": 7
        },
        {
          "source": "AI developer",
          "target": "trustworthiness characteristics",
          "type": "perceives",
          "description": "AI developers may interpret trustworthiness characteristics differently based on their role in the AI lifecycle.",
          "strength": 7
        },
        {
          "source": "AI deployer",
          "target": "trustworthiness characteristics",
          "type": "perceives",
          "description": "AI deployers assess trustworthiness characteristics based on the operational context of the AI system.",
          "strength": 7
        },
        {
          "source": "ISO 9000:2015",
          "target": "validation",
          "type": "provides",
          "description": "ISO 9000:2015 provides guidelines for validating the requirements for specific intended uses of AI systems.",
          "strength": 8
        },
        {
          "source": "ISO/IEC TS 5723:2022",
          "target": "reliability",
          "type": "defines",
          "description": "ISO/IEC TS 5723:2022 defines the concept of reliability in the context of systems and products.",
          "strength": 8
        },
        {
          "source": "AI technology",
          "target": "negative AI risks",
          "type": "creates_opportunities_for",
          "description": "The deployment of AI technology can create opportunities for negative AI risks if not managed properly.",
          "strength": 9
        }
      ],
      "chunk_time": 20.391272540997306,
      "chunk_tokens": 383,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 23,
      "text": "iness.\nReliability is defined in the same standard as the \u201cability of an item to perform as required,\nwithout failure, for a given time interval, under given conditions\u201d (Source: ISO/IEC TS\n5723:2022). Reliability is a goal for overall correctness of AI system operation under the\nconditions of expected use and over a given period of time, including the entire lifetime of\nthe system.\nPage 13\n\n\nNIST AI 100-1\nAI RMF 1.0\nAccuracy and robustness contribute to the validity and trustworthiness of AI systems, and\ncan be in tension with one another in AI systems.\nAccuracy is defined by ISO/IEC TS 5723:2022 as \u201ccloseness of results of observations,\ncomputations, or estimates to the true values or the values accepted as being true.\u201d Mea-\nsures of accuracy should consider computational-centric measures (e.g., false positive and\nfalse negative rates), human-AI teaming, and demonstrate external validity (generalizable\nbeyond the training conditions). Accuracy measurements should always be paired with\nclearly defined and realistic test sets \u2013 that are representative of conditions of expected use\n\u2013 and details about test methodology; these should be included in associated documen-\ntation. Accuracy measurements may include disaggregation of results for different data\nsegments.\nRobustness or generalizability is defined as the \u201cability of a system to maintain its level\nof performance under a variety of circumstances\u201d (Source: ISO/IEC TS 5723:2022). Ro-\nbustness is a goal for appropriate system functionality in a broad set of conditions and\ncircumstances, including uses of AI systems not initially anticipated. Robustness requires\nnot only that the system perform exactly as it does under expected uses, but also that it\nshould perform in ways that minimize potential harms to people if it is operating in an\nunexpected setting.\n",
      "rephrase": "Reliability refers to an item's ability to function as intended without failure for a specific time and under certain conditions, according to ISO/IEC TS 5723:2022. For AI systems, reliability means they should operate correctly during expected use over their entire lifespan.\n\nAccuracy and robustness are important for the validity and trustworthiness of AI systems, but they can sometimes conflict with each other. Accuracy, as defined by ISO/IEC TS 5723:2022, is how close the results of observations or calculations are to the true values. To measure accuracy, it's essential to consider factors like false positives and false negatives, human-AI collaboration, and ensure that results can be generalized beyond the training conditions. Accuracy assessments should be based on well-defined and realistic test sets that reflect expected use conditions, and the methodology used should be documented. Additionally, accuracy results may be broken down by different data segments.\n\nRobustness, also defined by ISO/IEC TS 5723:2022, is the ability of a system to maintain its performance across various situations. This means that an AI system should not only work well under expected conditions but also minimize potential harm if it operates in unexpected scenarios.",
      "entities": [
        {
          "name": "ISO/IEC TS 5723:2022",
          "type": "standard",
          "description": "A standard that defines reliability, accuracy, and robustness in the context of AI systems."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) 1.0."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "A framework developed by NIST to manage risks associated with AI systems."
        },
        {
          "name": "AI systems",
          "type": "system",
          "description": "Systems that utilize artificial intelligence to perform tasks and make decisions."
        },
        {
          "name": "accuracy",
          "type": "function",
          "description": "The closeness of results of observations, computations, or estimates to the true values."
        },
        {
          "name": "robustness",
          "type": "function",
          "description": "The ability of a system to maintain its level of performance under a variety of circumstances."
        }
      ],
      "relations": [
        {
          "source": "ISO/IEC TS 5723:2022",
          "target": "reliability",
          "type": "defines",
          "description": "Defines reliability as the ability of an item to perform as required without failure.",
          "strength": 9
        },
        {
          "source": "ISO/IEC TS 5723:2022",
          "target": "accuracy",
          "type": "defines",
          "description": "Defines accuracy as the closeness of results to true values.",
          "strength": 9
        },
        {
          "source": "ISO/IEC TS 5723:2022",
          "target": "robustness",
          "type": "defines",
          "description": "Defines robustness as the ability to maintain performance under various circumstances.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "includes",
          "description": "Includes the AI Risk Management Framework 1.0 as part of its content.",
          "strength": 8
        },
        {
          "source": "accuracy",
          "target": "AI systems",
          "type": "contributes_to",
          "description": "Contributes to the validity and trustworthiness of AI systems.",
          "strength": 8
        },
        {
          "source": "robustness",
          "target": "AI systems",
          "type": "contributes_to",
          "description": "Contributes to appropriate system functionality in a broad set of conditions.",
          "strength": 8
        }
      ],
      "chunk_time": 19.39259170799778,
      "chunk_tokens": 400,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 24,
      "text": " AI systems not initially anticipated. Robustness requires\nnot only that the system perform exactly as it does under expected uses, but also that it\nshould perform in ways that minimize potential harms to people if it is operating in an\nunexpected setting.\nValidity and reliability for deployed AI systems are often assessed by ongoing testing or\nmonitoring that confirms a system is performing as intended. Measurement of validity,\naccuracy, robustness, and reliability contribute to trustworthiness and should take into con-\nsideration that certain types of failures can cause greater harm. AI risk management efforts\nshould prioritize the minimization of potential negative impacts, and may need to include\nhuman intervention in cases where the AI system cannot detect or correct errors.\n3.2\nSafe\nAI systems should \u201cnot under defined conditions, lead to a state in which human life,\nhealth, property, or the environment is endangered\u201d (Source: ISO/IEC TS 5723:2022). Safe\noperation of AI systems is improved through:\n\u2022 responsible design, development, and deployment practices;\n\u2022 clear information to deployers on responsible use of the system;\n\u2022 responsible decision-making by deployers and end users; and\n\u2022 explanations and documentation of risks based on empirical evidence of incidents.\nDifferent types of safety risks may require tailored AI risk management approaches based\non context and the severity of potential risks presented. Safety risks that pose a potential\nrisk of serious injury or death call for the most urgent prioritization and most thorough risk\nmanagement process.\nPage 14\n\n\nNIST AI 100-1\nAI RMF 1.0\nEmploying safety considerations during the lifecycle and starting as early as possible with\nplanning and design can prevent failures or conditions that can render a system dangerous.\n",
      "rephrase": "AI systems can behave in unexpected ways, so they need to be robust. This means they should not only work well in expected situations but also minimize harm if they operate in unforeseen circumstances. To ensure AI systems are valid and reliable, they should undergo continuous testing and monitoring to confirm they perform as intended. Assessing their validity, accuracy, robustness, and reliability builds trust, especially since some failures can cause significant harm. AI risk management should focus on reducing negative impacts and may require human intervention if the AI cannot identify or fix its own errors.\n\nAI systems must not endanger human life, health, property, or the environment under any defined conditions (Source: ISO/IEC TS 5723:2022). To ensure safe operation, the following practices are essential:\n- Responsible design, development, and deployment.\n- Clear guidance for users on how to use the system responsibly.\n- Thoughtful decision-making by both deployers and end users.\n- Documentation of risks based on real incidents.\n\nDifferent safety risks may need specific risk management strategies depending on the context and severity. Risks that could lead to serious injury or death should be prioritized and managed thoroughly. Incorporating safety considerations from the planning and design stages can help prevent dangerous failures.",
      "entities": [
        {
          "name": "AI systems",
          "type": "system",
          "description": "AI systems are designed to perform tasks that typically require human intelligence, but their robustness must be ensured under both expected and unexpected conditions."
        },
        {
          "name": "ISO/IEC TS 5723:2022",
          "type": "standard",
          "description": "A standard that outlines safety requirements for AI systems, emphasizing that they should not endanger human life, health, property, or the environment."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document published by NIST that provides guidelines and frameworks for AI risk management."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "AI RMF 1.0 is a framework developed to guide organizations in managing risks associated with AI systems."
        },
        {
          "name": "AI risk management",
          "type": "practice",
          "description": "A practice focused on minimizing potential negative impacts of AI systems, including the need for human intervention in certain cases."
        },
        {
          "name": "safety risks",
          "type": "risk",
          "description": "Risks associated with the operation of AI systems that may lead to serious injury or death, requiring tailored management approaches."
        }
      ],
      "relations": [
        {
          "source": "AI systems",
          "target": "ISO/IEC TS 5723:2022",
          "type": "aligned_with",
          "description": "AI systems should comply with the safety requirements outlined in ISO/IEC TS 5723:2022 to avoid endangering human life and health.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "composed_of",
          "description": "NIST AI 100-1 includes the AI RMF 1.0 framework as part of its guidelines for AI risk management.",
          "strength": 8
        },
        {
          "source": "AI risk management",
          "target": "safety risks",
          "type": "evaluates",
          "description": "AI risk management practices evaluate and prioritize safety risks to minimize potential harms.",
          "strength": 9
        }
      ],
      "chunk_time": 13.10430500000075,
      "chunk_tokens": 357,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 25,
      "text": " 14\n\n\nNIST AI 100-1\nAI RMF 1.0\nEmploying safety considerations during the lifecycle and starting as early as possible with\nplanning and design can prevent failures or conditions that can render a system dangerous.\nOther practical approaches for AI safety often relate to rigorous simulation and in-domain\ntesting, real-time monitoring, and the ability to shut down, modify, or have human inter-\nvention into systems that deviate from intended or expected functionality.\nAI safety risk management approaches should take cues from efforts and guidelines for\nsafety in fields such as transportation and healthcare, and align with existing sector- or\napplication-specific guidelines or standards.\n3.3\nSecure and Resilient\nAI systems, as well as the ecosystems in which they are deployed, may be said to be re-\nsilient if they can withstand unexpected adverse events or unexpected changes in their envi-\nronment or use \u2013 or if they can maintain their functions and structure in the face of internal\nand external change and degrade safely and gracefully when this is necessary (Adapted\nfrom: ISO/IEC TS 5723:2022). Common security concerns relate to adversarial examples,\ndata poisoning, and the exfiltration of models, training data, or other intellectual property\nthrough AI system endpoints. AI systems that can maintain confidentiality, integrity, and\navailability through protection mechanisms that prevent unauthorized access and use may\nbe said to be secure. Guidelines in the NIST Cybersecurity Framework and Risk Manage-\nment Framework are among those which are applicable here.\nSecurity and resilience are related but distinct characteristics. While resilience is the abil-\nity to return to normal function after an unexpected adverse event, security includes re-\nsilience but also encompasses protocols to avoid, protect against, respond to, or recover\nfrom attacks.",
      "rephrase": "NIST AI 100-1 AI RMF 1.0 emphasizes the importance of considering safety throughout the entire lifecycle of AI systems, starting from the planning and design stages. This proactive approach can help prevent dangerous failures. Effective AI safety measures include thorough simulations, testing within the relevant domain, real-time monitoring, and having the capability to shut down or modify systems, as well as allowing for human intervention if the system behaves unexpectedly. AI safety risk management should draw from safety practices in fields like transportation and healthcare and align with existing guidelines specific to different sectors or applications.\n\nAI systems and their environments are considered resilient if they can handle unexpected events or changes while maintaining their functions. They should also be able to safely degrade when necessary. Common security issues include adversarial attacks, data poisoning, and unauthorized access to models or training data. Secure AI systems protect confidentiality, integrity, and availability by preventing unauthorized access. The NIST Cybersecurity Framework and Risk Management Framework provide relevant guidelines for ensuring security.\n\nWhile security and resilience are related, they are not the same. Resilience refers to the ability to return to normal operations after an unexpected event, whereas security includes resilience but also involves measures to prevent, protect against, respond to, and recover from attacks.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document related to AI risk management frameworks, focusing on safety considerations during the lifecycle of AI systems."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The first version of the AI Risk Management Framework, which emphasizes safety and risk management in AI systems."
        },
        {
          "name": "ISO/IEC TS 5723:2022",
          "type": "standard",
          "description": "A standard that provides guidelines for the resilience of AI systems, adapted for the context of AI safety."
        },
        {
          "name": "NIST Cybersecurity Framework",
          "type": "guideline",
          "description": "A framework that provides guidelines for managing cybersecurity risks, applicable to AI systems."
        },
        {
          "name": "NIST Risk Management Framework",
          "type": "guideline",
          "description": "A framework that outlines a structured approach to risk management, relevant to AI safety and security."
        },
        {
          "name": "AI safety risk management approaches",
          "type": "category",
          "description": "Approaches that focus on managing risks associated with AI systems to ensure safety and reliability."
        },
        {
          "name": "security and resilience",
          "type": "category",
          "description": "Characteristics of AI systems that relate to their ability to withstand adverse events and maintain functionality."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document that discusses the AI RMF 1.0 framework.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "AI safety risk management approaches",
          "type": "supports",
          "description": "AI RMF 1.0 supports the development of AI safety risk management approaches.",
          "strength": 8
        },
        {
          "source": "ISO/IEC TS 5723:2022",
          "target": "AI safety risk management approaches",
          "type": "aligned_with",
          "description": "ISO/IEC TS 5723:2022 provides guidelines that align with AI safety risk management approaches.",
          "strength": 7
        },
        {
          "source": "NIST Cybersecurity Framework",
          "target": "AI safety risk management approaches",
          "type": "applicable_to",
          "description": "The NIST Cybersecurity Framework is applicable to AI safety risk management approaches.",
          "strength": 8
        },
        {
          "source": "NIST Risk Management Framework",
          "target": "AI safety risk management approaches",
          "type": "applicable_to",
          "description": "The NIST Risk Management Framework is applicable to AI safety risk management approaches.",
          "strength": 8
        },
        {
          "source": "security and resilience",
          "target": "AI safety risk management approaches",
          "type": "includes",
          "description": "Security and resilience are included as key considerations in AI safety risk management approaches.",
          "strength": 9
        }
      ],
      "chunk_time": 17.9503232499992,
      "chunk_tokens": 370,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 26,
      "text": " are related but distinct characteristics. While resilience is the abil-\nity to return to normal function after an unexpected adverse event, security includes re-\nsilience but also encompasses protocols to avoid, protect against, respond to, or recover\nfrom attacks. Resilience relates to robustness and goes beyond the provenance of the data\nto encompass unexpected or adversarial use (or abuse or misuse) of the model or data.\n3.4\nAccountable and Transparent\nTrustworthy AI depends upon accountability. Accountability presupposes transparency.\nTransparency reflects the extent to which information about an AI system and its outputs is\navailable to individuals interacting with such a system \u2013 regardless of whether they are even\naware that they are doing so. Meaningful transparency provides access to appropriate levels\nof information based on the stage of the AI lifecycle and tailored to the role or knowledge\nof AI actors or individuals interacting with or using the AI system. By promoting higher\nlevels of understanding, transparency increases confidence in the AI system.\nThis characteristic\u2019s scope spans from design decisions and training data to model train-\ning, the structure of the model, its intended use cases, and how and when deployment,\npost-deployment, or end user decisions were made and by whom. Transparency is often\nnecessary for actionable redress related to AI system outputs that are incorrect or otherwise\nlead to negative impacts. Transparency should consider human-AI interaction: for exam-\nPage 15\n\n\nNIST AI 100-1\nAI RMF 1.0\nple, how a human operator or user is notified when a potential or actual adverse outcome\ncaused by an AI system is detected. A transparent system is not necessarily an accurate,\nprivacy-enhanced, secure, or fair system. However, it is difficult to determine whether an\nopaque system possesses such characteristics, and to do so over time as complex systems\nevolve.\n",
      "rephrase": "Resilience and security are related but different concepts. Resilience is the ability to bounce back to normal after an unexpected negative event, while security includes resilience and also involves measures to prevent, protect against, respond to, or recover from attacks. Resilience focuses on strength and goes beyond just the source of the data; it also considers unexpected or harmful uses of the model or data.\n\nAccountability and transparency are essential for trustworthy AI. Accountability requires transparency, which means providing information about an AI system and its outputs to users, even if they are not aware they are interacting with it. Meaningful transparency offers the right level of information based on the AI's lifecycle stage and the knowledge of the users. This helps users understand the system better and builds their confidence in it.\n\nTransparency covers everything from design choices and training data to how the model is trained, its structure, intended uses, and decisions made during deployment and afterward. It is crucial for addressing issues when AI outputs are incorrect or harmful. Transparency should also consider how users are informed about potential or actual negative outcomes caused by the AI. While a transparent system is not automatically accurate, secure, or fair, it is challenging to assess whether a non-transparent system has these qualities, especially as complex systems change over time.",
      "entities": [
        {
          "name": "Resilience",
          "type": "concept",
          "description": "The ability to return to normal function after an unexpected adverse event, relating to robustness and the unexpected or adversarial use of data."
        },
        {
          "name": "Security",
          "type": "concept",
          "description": "Encompasses resilience and includes protocols to avoid, protect against, respond to, or recover from attacks."
        },
        {
          "name": "Accountability",
          "type": "concept",
          "description": "A characteristic of trustworthy AI that presupposes transparency."
        },
        {
          "name": "Transparency",
          "type": "concept",
          "description": "The extent to which information about an AI system and its outputs is available to individuals interacting with the system."
        },
        {
          "name": "AI Lifecycle",
          "type": "framework",
          "description": "The stages through which an AI system progresses, from design decisions to deployment and post-deployment."
        },
        {
          "name": "NIST AI RMF 1.0",
          "type": "document",
          "description": "A document that outlines a framework for managing risks associated with AI systems."
        }
      ],
      "relations": [
        {
          "source": "Resilience",
          "target": "Security",
          "type": "aligned_with",
          "description": "Resilience is a component of security, which also includes additional protocols.",
          "strength": 9
        },
        {
          "source": "Accountability",
          "target": "Transparency",
          "type": "supports",
          "description": "Accountability in AI systems is supported by transparency.",
          "strength": 8
        },
        {
          "source": "Transparency",
          "target": "AI Lifecycle",
          "type": "applicable_to",
          "description": "Transparency is relevant at various stages of the AI lifecycle.",
          "strength": 7
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "Transparency",
          "type": "provides",
          "description": "The NIST AI RMF 1.0 document provides guidelines that promote transparency in AI systems.",
          "strength": 8
        }
      ],
      "chunk_time": 12.405337250005687,
      "chunk_tokens": 382,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 27,
      "text": " detected. A transparent system is not necessarily an accurate,\nprivacy-enhanced, secure, or fair system. However, it is difficult to determine whether an\nopaque system possesses such characteristics, and to do so over time as complex systems\nevolve.\nThe role of AI actors should be considered when seeking accountability for the outcomes of\nAI systems. The relationship between risk and accountability associated with AI and tech-\nnological systems more broadly differs across cultural, legal, sectoral, and societal contexts.\nWhen consequences are severe, such as when life and liberty are at stake, AI developers\nand deployers should consider proportionally and proactively adjusting their transparency\nand accountability practices. Maintaining organizational practices and governing structures\nfor harm reduction, like risk management, can help lead to more accountable systems.\nMeasures to enhance transparency and accountability should also consider the impact of\nthese efforts on the implementing entity, including the level of necessary resources and the\nneed to safeguard proprietary information.\nMaintaining the provenance of training data and supporting attribution of the AI system\u2019s\ndecisions to subsets of training data can assist with both transparency and accountability.\nTraining data may also be subject to copyright and should follow applicable intellectual\nproperty rights laws.\nAs transparency tools for AI systems and related documentation continue to evolve, devel-\nopers of AI systems are encouraged to test different types of transparency tools in cooper-\nation with AI deployers to ensure that AI systems are used as intended.\n3.5\nExplainable and Interpretable\nExplainability refers to a representation of the mechanisms underlying AI systems\u2019 oper-\nation, whereas interpretability refers to the meaning of AI systems\u2019 output in the context\nof their designed functional purposes. Together, explainability and interpretability assist\nthose operating or overseeing an AI system, as well as users of an AI system, to gain\ndeeper insights into the functionality and trustworthiness of the system, including its out-\nputs.",
      "rephrase": "A transparent system does not automatically mean it is accurate, secure, fair, or protects privacy. It can be challenging to assess whether a non-transparent system has these qualities, especially as complex systems change over time. When looking for accountability in AI systems, it's important to consider the role of AI developers and users. The relationship between risk and accountability varies based on cultural, legal, and societal factors. In situations where the stakes are high, such as when lives are at risk, AI developers should actively improve their transparency and accountability practices. Implementing risk management and harm reduction strategies can lead to more responsible systems. Efforts to increase transparency and accountability should also take into account the resources needed and the protection of proprietary information. Keeping track of the training data used for AI and linking AI decisions back to specific data can help with transparency and accountability. Training data may be protected by copyright and must comply with intellectual property laws. As tools for transparency in AI systems develop, AI developers should work with users to test these tools to ensure AI systems function as intended. Explainability means understanding how AI systems work, while interpretability refers to understanding the meaning of their outputs based on their intended purpose. Together, these concepts help users and overseers of AI systems gain insights into how the systems operate and how trustworthy they are.",
      "entities": [
        {
          "name": "AI systems",
          "type": "system",
          "description": "AI systems are technological systems that utilize artificial intelligence to perform tasks, make decisions, and provide outputs based on data."
        },
        {
          "name": "transparency",
          "type": "category",
          "description": "Transparency in AI refers to the clarity and openness regarding the mechanisms and processes involved in the operation of AI systems."
        },
        {
          "name": "accountability",
          "type": "category",
          "description": "Accountability in the context of AI systems involves the responsibility of developers and deployers to ensure that the systems operate fairly and ethically."
        },
        {
          "name": "risk management",
          "type": "function",
          "description": "Risk management is a practice aimed at identifying, assessing, and mitigating risks associated with the deployment and operation of AI systems."
        },
        {
          "name": "training data",
          "type": "resource",
          "description": "Training data refers to the datasets used to train AI systems, which can influence their performance and decision-making capabilities."
        },
        {
          "name": "explainability",
          "type": "framework",
          "description": "Explainability refers to the representation of the mechanisms underlying the operation of AI systems, helping users understand how decisions are made."
        },
        {
          "name": "interpretability",
          "type": "framework",
          "description": "Interpretability refers to the meaning of the outputs generated by AI systems in the context of their designed functional purposes."
        }
      ],
      "relations": [
        {
          "source": "AI systems",
          "target": "transparency",
          "type": "supports",
          "description": "AI systems benefit from transparency to enhance their accountability and trustworthiness.",
          "strength": 9
        },
        {
          "source": "AI systems",
          "target": "accountability",
          "type": "supports",
          "description": "AI systems require accountability measures to ensure ethical and fair operation.",
          "strength": 9
        },
        {
          "source": "risk management",
          "target": "accountability",
          "type": "fosters",
          "description": "Risk management practices help foster accountability in the deployment of AI systems.",
          "strength": 8
        },
        {
          "source": "training data",
          "target": "transparency",
          "type": "supports",
          "description": "Maintaining the provenance of training data supports transparency and accountability in AI systems.",
          "strength": 8
        },
        {
          "source": "explainability",
          "target": "AI systems",
          "type": "contributes_to",
          "description": "Explainability contributes to a better understanding of AI systems' operations and outputs.",
          "strength": 9
        },
        {
          "source": "interpretability",
          "target": "AI systems",
          "type": "contributes_to",
          "description": "Interpretability helps users understand the meaning of AI systems' outputs in their functional context.",
          "strength": 9
        }
      ],
      "chunk_time": 16.73584891599603,
      "chunk_tokens": 392,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 28,
      "text": ". Together, explainability and interpretability assist\nthose operating or overseeing an AI system, as well as users of an AI system, to gain\ndeeper insights into the functionality and trustworthiness of the system, including its out-\nputs. The underlying assumption is that perceptions of negative risk stem from a lack of\nability to make sense of, or contextualize, system output appropriately. Explainable and\ninterpretable AI systems offer information that will help end users understand the purposes\nand potential impact of an AI system.\nRisk from lack of explainability may be managed by describing how AI systems function,\nwith descriptions tailored to individual differences such as the user\u2019s role, knowledge, and\nskill level. Explainable systems can be debugged and monitored more easily, and they lend\nthemselves to more thorough documentation, audit, and governance.\nPage 16\n\n\nNIST AI 100-1\nAI RMF 1.0\nRisks to interpretability often can be addressed by communicating a description of why\nan AI system made a particular prediction or recommendation. (See \u201cFour Principles of\nExplainable Artificial Intelligence\u201d and \u201cPsychological Foundations of Explainability and\nInterpretability in Artificial Intelligence\u201d found here.)\nTransparency, explainability, and interpretability are distinct characteristics that support\neach other. Transparency can answer the question of \u201cwhat happened\u201d in the system. Ex-\nplainability can answer the question of \u201chow\u201d a decision was made in the system. Inter-\npretability can answer the question of \u201cwhy\u201d a decision was made by the system and its\nmeaning or context to the user.\n3.6\nPrivacy-Enhanced\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy,\nidentity, and dignity. These norms and practices typically address freedom from intrusion,\nlimiting observation, or individuals\u2019 agency to consent to disclosure or control of facets of\ntheir identities (e.g., body, data, reputation).",
      "rephrase": "Explainability and interpretability are important for both those managing AI systems and the users of these systems. They help everyone understand how the AI works and how trustworthy it is, including its results. The main idea is that people often feel uncertain about AI because they can't easily understand its outputs. AI systems that are explainable and interpretable provide information that helps users grasp their purpose and potential effects. To reduce risks from a lack of explainability, it's helpful to explain how AI systems operate, adjusting the explanations based on the user's role, knowledge, and skills. Explainable systems are easier to debug, monitor, and document, which supports better governance. Risks related to interpretability can be managed by explaining why an AI system made a specific prediction or recommendation. Transparency, explainability, and interpretability are different but interconnected features. Transparency answers 'what happened,' explainability answers 'how' a decision was made, and interpretability answers 'why' a decision was made and its significance to the user. Privacy involves protecting individuals' autonomy, identity, and dignity by limiting intrusion and allowing people to control their personal information.",
      "entities": [
        {
          "name": "Explainable AI",
          "type": "framework",
          "description": "A framework that focuses on making AI systems understandable to users by providing insights into their functionality and trustworthiness."
        },
        {
          "name": "Interpretability",
          "type": "category",
          "description": "A characteristic of AI systems that allows users to understand the meaning or context of decisions made by the system."
        },
        {
          "name": "Transparency",
          "type": "category",
          "description": "A characteristic of AI systems that provides clarity on what happened during the decision-making process."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document published by NIST that outlines guidelines for managing risks associated with AI systems."
        },
        {
          "name": "AI RMF 1.0",
          "type": "document",
          "description": "A document that provides a risk management framework for AI systems."
        },
        {
          "name": "Privacy",
          "type": "category",
          "description": "Norms and practices that safeguard human autonomy, identity, and dignity, addressing issues of consent and control over personal information."
        }
      ],
      "relations": [
        {
          "source": "Explainable AI",
          "target": "Interpretability",
          "type": "supports",
          "description": "Explainable AI frameworks support the development of interpretability in AI systems.",
          "strength": 9
        },
        {
          "source": "Explainable AI",
          "target": "Transparency",
          "type": "supports",
          "description": "Explainable AI frameworks promote transparency in AI systems by clarifying decision-making processes.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "composed_of",
          "description": "NIST AI 100-1 is part of the broader AI RMF 1.0 framework for managing AI risks.",
          "strength": 8
        },
        {
          "source": "Privacy",
          "target": "Explainable AI",
          "type": "aligned_with",
          "description": "Privacy norms align with the principles of explainable AI by emphasizing user autonomy and control over personal data.",
          "strength": 7
        }
      ],
      "chunk_time": 13.133689084002981,
      "chunk_tokens": 400,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 29,
      "text": " to safeguard human autonomy,\nidentity, and dignity. These norms and practices typically address freedom from intrusion,\nlimiting observation, or individuals\u2019 agency to consent to disclosure or control of facets of\ntheir identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool\nfor Improving Privacy through Enterprise Risk Management.)\nPrivacy values such as anonymity, confidentiality, and control generally should guide choices\nfor AI system design, development, and deployment. Privacy-related risks may influence\nsecurity, bias, and transparency and come with tradeoffs with these other characteristics.\nLike safety and security, specific technical features of an AI system may promote or reduce\nprivacy. AI systems can also present new risks to privacy by allowing inference to identify\nindividuals or previously private information about individuals.\nPrivacy-enhancing technologies (\u201cPETs\u201d) for AI, as well as data minimizing methods such\nas de-identification and aggregation for certain model outputs, can support design for\nprivacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-\nenhancing techniques can result in a loss in accuracy, affecting decisions about fairness\nand other values in certain domains.\n3.7\nFair \u2013 with Harmful Bias Managed\nFairness in AI includes concerns for equality and equity by addressing issues such as harm-\nful bias and discrimination. Standards of fairness can be complex and difficult to define be-\ncause perceptions of fairness differ among cultures and may shift depending on application.\nOrganizations\u2019 risk management efforts will be enhanced by recognizing and considering\nthese differences. Systems in which harmful biases are mitigated are not necessarily fair.\nFor example, systems in which predictions are somewhat balanced across demographic\ngroups may still be inaccessible to individuals with disabilities or affected by the digital\ndivide or may exacerbate existing disparities or systemic biases.\nPage 17\n\n\nNIST AI 100-1\nAI RMF 1.0\nBias is broader than demographic balance and data representativeness.",
      "rephrase": "To protect human autonomy, identity, and dignity, we need to follow certain norms and practices that ensure individuals have control over their personal information, such as their body, data, and reputation. The NIST Privacy Framework provides guidance on improving privacy through enterprise risk management. When designing, developing, and deploying AI systems, we should prioritize privacy values like anonymity, confidentiality, and control. Privacy risks can affect security, bias, and transparency, and there may be trade-offs between these aspects. Specific technical features of AI systems can either enhance or compromise privacy. Additionally, AI can introduce new privacy risks by enabling the identification of individuals or revealing private information. Privacy-enhancing technologies (PETs) and methods like de-identification and aggregation can help create AI systems that prioritize privacy. However, using these techniques may sometimes reduce accuracy, which can impact fairness and other important values. Fairness in AI involves addressing harmful bias and discrimination, but defining fairness can be complex due to cultural differences and varying perceptions. Organizations should consider these differences in their risk management efforts. Even if harmful biases are reduced, a system may still not be fair if it is inaccessible to people with disabilities or if it worsens existing inequalities. Bias in AI is more than just achieving demographic balance or data representativeness.",
      "entities": [
        {
          "name": "NIST Privacy Framework",
          "type": "document",
          "description": "A tool for improving privacy through enterprise risk management, focusing on safeguarding human autonomy, identity, and dignity."
        },
        {
          "name": "Privacy-enhancing technologies (PETs)",
          "type": "technology",
          "description": "Technologies designed to protect privacy in AI systems, including methods like de-identification and aggregation."
        },
        {
          "name": "Fairness in AI",
          "type": "framework",
          "description": "A concept that includes concerns for equality and equity, addressing issues such as harmful bias and discrimination in AI systems."
        },
        {
          "name": "AI RMF 1.0",
          "type": "document",
          "description": "A framework related to AI risk management, emphasizing the importance of fairness and bias management in AI systems."
        },
        {
          "name": "Bias",
          "type": "risk",
          "description": "A broader concept than demographic balance and data representativeness, which can affect fairness in AI systems."
        }
      ],
      "relations": [
        {
          "source": "NIST Privacy Framework",
          "target": "Privacy-enhancing technologies (PETs)",
          "type": "supports",
          "description": "The NIST Privacy Framework supports the use of privacy-enhancing technologies in AI system design.",
          "strength": 8
        },
        {
          "source": "Fairness in AI",
          "target": "Bias",
          "type": "includes",
          "description": "Fairness in AI includes the management of harmful bias and discrimination.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "Fairness in AI",
          "type": "aligned_with",
          "description": "AI RMF 1.0 is aligned with the principles of fairness in AI, emphasizing the need to manage bias.",
          "strength": 7
        }
      ],
      "chunk_time": 9.974252500003786,
      "chunk_tokens": 396,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 30,
      "text": " disabilities or affected by the digital\ndivide or may exacerbate existing disparities or systemic biases.\nPage 17\n\n\nNIST AI 100-1\nAI RMF 1.0\nBias is broader than demographic balance and data representativeness. NIST has identified\nthree major categories of AI bias to be considered and managed: systemic, computational\nand statistical, and human-cognitive. Each of these can occur in the absence of prejudice,\npartiality, or discriminatory intent. Systemic bias can be present in AI datasets, the orga-\nnizational norms, practices, and processes across the AI lifecycle, and the broader society\nthat uses AI systems. Computational and statistical biases can be present in AI datasets\nand algorithmic processes, and often stem from systematic errors due to non-representative\nsamples. Human-cognitive biases relate to how an individual or group perceives AI sys-\ntem information to make a decision or fill in missing information, or how humans think\nabout purposes and functions of an AI system. Human-cognitive biases are omnipresent\nin decision-making processes across the AI lifecycle and system use, including the design,\nimplementation, operation, and maintenance of AI.\nBias exists in many forms and can become ingrained in the automated systems that help\nmake decisions about our lives. While bias is not always a negative phenomenon, AI sys-\ntems can potentially increase the speed and scale of biases and perpetuate and amplify\nharms to individuals, groups, communities, organizations, and society. Bias is tightly asso-\nciated with the concepts of transparency as well as fairness in society. (For more informa-\ntion about bias, including the three categories, see NIST Special Publication 1270, Towards\na Standard for Identifying and Managing Bias in Artificial Intelligence.)\nPage 18\n\n\nNIST AI 100-1\nAI RMF 1.0\n4.\n",
      "rephrase": "Bias in AI goes beyond just having a fair representation of different demographics. The National Institute of Standards and Technology (NIST) has identified three main types of AI bias that need to be addressed: systemic bias, computational and statistical bias, and human-cognitive bias. These biases can occur even without any intention to discriminate. \n\nSystemic bias can be found in AI datasets, the practices and norms of organizations, and in society as a whole. Computational and statistical biases often arise from errors in AI datasets and algorithms, particularly when the data is not representative. Human-cognitive biases involve how people interpret AI information to make decisions or fill in gaps, and these biases are common throughout the AI lifecycle, including in design, implementation, and maintenance.\n\nBias can take many forms and may become embedded in automated systems that influence our lives. While bias isn't always negative, AI can speed up and amplify existing biases, potentially causing harm to individuals and communities. Addressing bias is closely linked to ensuring transparency and fairness in society. For more details on bias and its categories, refer to NIST Special Publication 1270, which focuses on identifying and managing bias in AI.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) and discusses various aspects of AI bias."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The first version of the AI Risk Management Framework developed by NIST, focusing on identifying and managing risks associated with AI."
        },
        {
          "name": "NIST Special Publication 1270",
          "type": "document",
          "description": "A publication that provides guidance on identifying and managing bias in artificial intelligence."
        },
        {
          "name": "systemic bias",
          "type": "category",
          "description": "A type of bias that can be present in AI datasets and organizational practices, reflecting broader societal norms."
        },
        {
          "name": "computational and statistical bias",
          "type": "category",
          "description": "Biases that arise from systematic errors in AI datasets and algorithmic processes, often due to non-representative samples."
        },
        {
          "name": "human-cognitive bias",
          "type": "category",
          "description": "Biases related to individual or group perceptions of AI system information, influencing decision-making processes."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Automated systems that assist in decision-making and can perpetuate biases present in their design and implementation."
        },
        {
          "name": "transparency",
          "type": "concept",
          "description": "A principle associated with fairness in society, closely related to the discussion of bias in AI."
        },
        {
          "name": "fairness",
          "type": "concept",
          "description": "A principle that aims to ensure equitable treatment and outcomes in the context of AI systems."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document that outlines the AI RMF 1.0 framework.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "NIST Special Publication 1270",
          "type": "includes",
          "description": "NIST AI 100-1 includes references to NIST Special Publication 1270 for further information on bias.",
          "strength": 8
        },
        {
          "source": "AI RMF 1.0",
          "target": "systemic bias",
          "type": "identifies",
          "description": "AI RMF 1.0 identifies systemic bias as a major category of AI bias.",
          "strength": 8
        },
        {
          "source": "AI RMF 1.0",
          "target": "computational and statistical bias",
          "type": "identifies",
          "description": "AI RMF 1.0 identifies computational and statistical bias as a major category of AI bias.",
          "strength": 8
        },
        {
          "source": "AI RMF 1.0",
          "target": "human-cognitive bias",
          "type": "identifies",
          "description": "AI RMF 1.0 identifies human-cognitive bias as a major category of AI bias.",
          "strength": 8
        },
        {
          "source": "AI systems",
          "target": "transparency",
          "type": "supports",
          "description": "AI systems are closely associated with the concept of transparency in relation to bias.",
          "strength": 7
        },
        {
          "source": "AI systems",
          "target": "fairness",
          "type": "supports",
          "description": "AI systems are closely associated with the concept of fairness in relation to bias.",
          "strength": 7
        }
      ],
      "chunk_time": 18.45019945799868,
      "chunk_tokens": 385,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 31,
      "text": " including the three categories, see NIST Special Publication 1270, Towards\na Standard for Identifying and Managing Bias in Artificial Intelligence.)\nPage 18\n\n\nNIST AI 100-1\nAI RMF 1.0\n4.\nEffectiveness of the AI RMF\nEvaluations of AI RMF effectiveness \u2013 including ways to measure bottom-line improve-\nments in the trustworthiness of AI systems \u2013 will be part of future NIST activities, in\nconjunction with the AI community.\nOrganizations and other users of the Framework are encouraged to periodically evaluate\nwhether the AI RMF has improved their ability to manage AI risks, including but not lim-\nited to their policies, processes, practices, implementation plans, indicators, measurements,\nand expected outcomes. NIST intends to work collaboratively with others to develop met-\nrics, methodologies, and goals for evaluating the AI RMF\u2019s effectiveness, and to broadly\nshare results and supporting information. Framework users are expected to benefit from:\n\u2022 enhanced processes for governing, mapping, measuring, and managing AI risk, and\nclearly documenting outcomes;\n\u2022 improved awareness of the relationships and tradeoffs among trustworthiness char-\nacteristics, socio-technical approaches, and AI risks;\n\u2022 explicit processes for making go/no-go system commissioning and deployment deci-\nsions;\n\u2022 established policies, processes, practices, and procedures for improving organiza-\ntional accountability efforts related to AI system risks;\n\u2022 enhanced organizational culture which prioritizes the identification and management\nof AI system risks and potential impacts to individuals, communities, organizations,\nand society;\n\u2022 better information sharing within and across organizations about risks, decision-\nmaking processes, responsibilities, common pitfalls, TEVV practices, and approaches\nfor continuous improvement;\n\u2022 greater contextual knowledge for increased awareness of downstream risks;\n\u2022 strengthened engagement with interested parties and relevant AI actors; and\n\u2022 augmented capacity for TEVV of AI systems and associated risks.\n",
      "rephrase": "NIST Special Publication 1270 outlines three categories for identifying and managing bias in Artificial Intelligence (AI). In the NIST AI RMF 1.0 document, the effectiveness of the AI Risk Management Framework (AI RMF) will be evaluated in future NIST activities, focusing on how to measure improvements in the trustworthiness of AI systems. Organizations using the Framework are encouraged to regularly assess whether it has helped them manage AI risks better, including their policies, processes, and expected outcomes. NIST plans to collaborate with the AI community to create metrics and methodologies for evaluating the AI RMF's effectiveness and to share the results widely. Users of the Framework can expect benefits such as: \n- Improved processes for governing and managing AI risks, with clear documentation of outcomes. \n- Better understanding of the relationships and trade-offs between trustworthiness, socio-technical approaches, and AI risks. \n- Clear procedures for making decisions on whether to deploy AI systems. \n- Established practices for enhancing accountability regarding AI system risks. \n- A stronger organizational culture that focuses on identifying and managing AI risks and their impacts on individuals and society. \n- Improved information sharing about risks, decision-making, and continuous improvement practices. \n- Greater awareness of downstream risks. \n- Enhanced engagement with stakeholders and relevant AI participants. \n- Increased capacity for testing and evaluating AI systems and their associated risks.",
      "entities": [
        {
          "name": "NIST Special Publication 1270",
          "type": "document",
          "description": "A publication by NIST that outlines a standard for identifying and managing bias in artificial intelligence."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework (AI RMF) developed by NIST to help organizations manage AI risks."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "The National Institute of Standards and Technology, a U.S. federal agency that develops standards and guidelines for various technologies."
        },
        {
          "name": "AI community",
          "type": "category",
          "description": "A collective term for organizations and individuals involved in the development and management of artificial intelligence technologies."
        },
        {
          "name": "TEVV practices",
          "type": "practice",
          "description": "Practices related to Testing, Evaluation, Validation, and Verification of AI systems."
        }
      ],
      "relations": [
        {
          "source": "NIST Special Publication 1270",
          "target": "AI RMF 1.0",
          "type": "aligned_with",
          "description": "NIST Special Publication 1270 aligns with the AI RMF 1.0 framework for managing AI risks.",
          "strength": 8
        },
        {
          "source": "NIST",
          "target": "AI RMF 1.0",
          "type": "develops",
          "description": "NIST develops the AI RMF 1.0 framework to assist organizations in managing AI risks.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "AI community",
          "type": "supports",
          "description": "The AI RMF 1.0 framework supports the AI community by providing guidelines for risk management.",
          "strength": 7
        },
        {
          "source": "NIST",
          "target": "AI community",
          "type": "collaborates_with",
          "description": "NIST collaborates with the AI community to evaluate the effectiveness of the AI RMF.",
          "strength": 8
        },
        {
          "source": "AI RMF 1.0",
          "target": "TEVV practices",
          "type": "includes",
          "description": "The AI RMF 1.0 framework includes TEVV practices for evaluating AI systems.",
          "strength": 7
        }
      ],
      "chunk_time": 13.170454541002982,
      "chunk_tokens": 393,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 32,
      "text": " pitfalls, TEVV practices, and approaches\nfor continuous improvement;\n\u2022 greater contextual knowledge for increased awareness of downstream risks;\n\u2022 strengthened engagement with interested parties and relevant AI actors; and\n\u2022 augmented capacity for TEVV of AI systems and associated risks.\nPage 19\n\n\nNIST AI 100-1\nAI RMF 1.0\nPart 2: Core and Profiles\n5.\nAI RMF Core\nThe AI RMF Core provides outcomes and actions that enable dialogue, understanding, and\nactivities to manage AI risks and responsibly develop trustworthy AI systems. As illus-\ntrated in Figure 5, the Core is composed of four functions: GOVERN, MAP, MEASURE,\nand MANAGE. Each of these high-level functions is broken down into categories and sub-\ncategories. Categories and subcategories are subdivided into specific actions and outcomes.\nActions do not constitute a checklist, nor are they necessarily an ordered set of steps.\nFig. 5. Functions organize AI risk management activities at their highest level to govern, map,\nmeasure, and manage AI risks. Governance is designed to be a cross-cutting function to inform\nand be infused throughout the other three functions.\nRisk management should be continuous, timely, and performed throughout the AI system\nlifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects\ndiverse and multidisciplinary perspectives, potentially including the views of AI actors out-\nside the organization. Having a diverse team contributes to more open sharing of ideas and\nassumptions about purposes and functions of the technology being designed, developed,\nPage 20\n\n\nNIST AI 100-1\nAI RMF 1.0\ndeployed, or evaluated \u2013 which can create opportunities to surface problems and identify\nexisting and emergent risks.\n",
      "rephrase": "The text discusses the challenges and practices related to TEVV (Trustworthy, Ethical, Verifiable, and Validated) in AI systems, emphasizing the need for: \n- Better understanding of potential risks that may arise later on. \n- Improved collaboration with stakeholders and relevant AI professionals. \n- Enhanced ability to evaluate the TEVV of AI systems and their associated risks.\n\nThe AI RMF (Risk Management Framework) Core outlines key outcomes and actions to facilitate discussions and manage AI risks effectively, ensuring the development of trustworthy AI systems. It consists of four main functions: GOVERN, MAP, MEASURE, and MANAGE. Each function is divided into categories and subcategories, which further break down into specific actions and outcomes. These actions are not a strict checklist or a sequential process.\n\nRisk management should be an ongoing process throughout the entire lifecycle of AI systems. The AI RMF Core functions should incorporate diverse perspectives, including input from AI professionals outside the organization. A diverse team fosters open discussions about the technology's purpose and functions, helping to identify potential problems and emerging risks.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) 1.0, detailing core functions and profiles for managing AI risks."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0 that provides guidelines for managing risks associated with AI systems."
        },
        {
          "name": "AI RMF Core",
          "type": "system",
          "description": "The core component of the AI RMF that includes actions and outcomes to manage AI risks and develop trustworthy AI systems."
        },
        {
          "name": "GOVERN",
          "type": "function",
          "description": "One of the four high-level functions of the AI RMF Core focused on governance of AI risks."
        },
        {
          "name": "MAP",
          "type": "function",
          "description": "A high-level function of the AI RMF Core that involves mapping AI risks."
        },
        {
          "name": "MEASURE",
          "type": "function",
          "description": "A high-level function of the AI RMF Core that focuses on measuring AI risks."
        },
        {
          "name": "MANAGE",
          "type": "function",
          "description": "A high-level function of the AI RMF Core that deals with managing AI risks."
        },
        {
          "name": "AI actors",
          "type": "organization",
          "description": "Entities or individuals involved in the development and deployment of AI technologies."
        },
        {
          "name": "downstream risks",
          "type": "risk",
          "description": "Risks that arise as a consequence of AI system deployment and operation."
        },
        {
          "name": "diverse team",
          "type": "category",
          "description": "A team composed of individuals with varied backgrounds and perspectives contributing to AI development."
        }
      ],
      "relations": [
        {
          "source": "AI RMF Core",
          "target": "GOVERN",
          "type": "composed_of",
          "description": "The AI RMF Core is composed of the function GOVERN.",
          "strength": 9
        },
        {
          "source": "AI RMF Core",
          "target": "MAP",
          "type": "composed_of",
          "description": "The AI RMF Core is composed of the function MAP.",
          "strength": 9
        },
        {
          "source": "AI RMF Core",
          "target": "MEASURE",
          "type": "composed_of",
          "description": "The AI RMF Core is composed of the function MEASURE.",
          "strength": 9
        },
        {
          "source": "AI RMF Core",
          "target": "MANAGE",
          "type": "composed_of",
          "description": "The AI RMF Core is composed of the function MANAGE.",
          "strength": 9
        },
        {
          "source": "AI RMF Core",
          "target": "AI actors",
          "type": "supports",
          "description": "The AI RMF Core supports the engagement with AI actors for diverse perspectives.",
          "strength": 8
        },
        {
          "source": "diverse team",
          "target": "AI RMF Core",
          "type": "contributes_to",
          "description": "A diverse team contributes to the AI RMF Core by providing varied perspectives.",
          "strength": 7
        },
        {
          "source": "AI RMF Core",
          "target": "downstream risks",
          "type": "identifies",
          "description": "The AI RMF Core identifies downstream risks associated with AI systems.",
          "strength": 8
        }
      ],
      "chunk_time": 19.961742042003607,
      "chunk_tokens": 366,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 33,
      "text": " of the technology being designed, developed,\nPage 20\n\n\nNIST AI 100-1\nAI RMF 1.0\ndeployed, or evaluated \u2013 which can create opportunities to surface problems and identify\nexisting and emergent risks.\nAn online companion resource to the AI RMF, the NIST AI RMF Playbook, is available\nto help organizations navigate the AI RMF and achieve its outcomes through suggested\ntactical actions they can apply within their own contexts. Like the AI RMF, the Playbook\nis voluntary and organizations can utilize the suggestions according to their needs and\ninterests. Playbook users can create tailored guidance selected from suggested material\nfor their own use and contribute their suggestions for sharing with the broader community.\nAlong with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI\nResource Center.\nFramework users may apply these functions as best suits their needs for managing\nAI risks based on their resources and capabilities. Some organizations may choose\nto select from among the categories and subcategories; others may choose and have\nthe capacity to apply all categories and subcategories. Assuming a governance struc-\nture is in place, functions may be performed in any order across the AI lifecycle as\ndeemed to add value by a user of the framework. After instituting the outcomes in\nGOVERN, most users of the AI RMF would start with the MAP function and con-\ntinue to MEASURE or MANAGE. However users integrate the functions, the process\nshould be iterative, with cross-referencing between functions as necessary. Simi-\nlarly, there are categories and subcategories with elements that apply to multiple\nfunctions, or that logically should take place before certain subcategory decisions.\n5.",
      "rephrase": "The technology being designed, developed, deployed, or evaluated can reveal problems and risks. To assist organizations in using the AI Risk Management Framework (AI RMF), NIST has created an online resource called the NIST AI RMF Playbook. This Playbook offers practical actions that organizations can take based on their specific needs and interests. It is voluntary, allowing users to customize guidance from the suggested materials and share their own ideas with the community. Both the AI RMF and the Playbook are part of the NIST Trustworthy and Responsible AI Resource Center.\n\nOrganizations can use the framework functions in a way that best fits their resources and capabilities. Some may choose specific categories and subcategories, while others may apply all of them. With a governance structure in place, users can perform functions in any order that adds value throughout the AI lifecycle. Typically, after establishing outcomes in the GOVERN phase, users would begin with the MAP function and then proceed to MEASURE or MANAGE. The process should be iterative, allowing for cross-referencing between functions as needed. Additionally, some categories and subcategories may apply to multiple functions or should logically occur before certain decisions are made.",
      "entities": [
        {
          "name": "NIST AI RMF 1.0",
          "type": "framework",
          "description": "A framework designed to manage AI risks and guide organizations in the responsible use of AI technologies."
        },
        {
          "name": "NIST AI RMF Playbook",
          "type": "resource",
          "description": "An online companion resource to the AI RMF that provides tactical actions for organizations to achieve outcomes related to AI risk management."
        },
        {
          "name": "NIST Trustworthy and Responsible AI Resource Center",
          "type": "organization",
          "description": "A center that encompasses the AI RMF and the Playbook, focusing on promoting trustworthy and responsible AI practices."
        },
        {
          "name": "GOVERN",
          "type": "function",
          "description": "A function within the AI RMF framework that focuses on governance structures for managing AI risks."
        },
        {
          "name": "MAP",
          "type": "function",
          "description": "A function in the AI RMF that users typically start with after instituting outcomes in GOVERN."
        },
        {
          "name": "MEASURE",
          "type": "function",
          "description": "A function in the AI RMF that follows the MAP function, focusing on assessing AI risks and outcomes."
        },
        {
          "name": "MANAGE",
          "type": "function",
          "description": "A function in the AI RMF that involves ongoing management of AI risks after the MEASURE function."
        },
        {
          "name": "AI lifecycle",
          "type": "category",
          "description": "The series of stages through which AI systems progress, from design and development to deployment and evaluation."
        }
      ],
      "relations": [
        {
          "source": "NIST AI RMF 1.0",
          "target": "NIST AI RMF Playbook",
          "type": "composed_of",
          "description": "The NIST AI RMF Playbook is an online companion resource that is part of the NIST AI RMF framework.",
          "strength": 9
        },
        {
          "source": "NIST AI RMF Playbook",
          "target": "NIST Trustworthy and Responsible AI Resource Center",
          "type": "available_at",
          "description": "The NIST AI RMF Playbook is available through the NIST Trustworthy and Responsible AI Resource Center.",
          "strength": 8
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "GOVERN",
          "type": "includes",
          "description": "The AI RMF framework includes the GOVERN function as part of its risk management process.",
          "strength": 8
        },
        {
          "source": "GOVERN",
          "target": "MAP",
          "type": "supports",
          "description": "After instituting outcomes in GOVERN, users typically start with the MAP function.",
          "strength": 7
        },
        {
          "source": "MAP",
          "target": "MEASURE",
          "type": "fosters",
          "description": "The MAP function fosters the subsequent MEASURE function in the AI RMF process.",
          "strength": 7
        },
        {
          "source": "MEASURE",
          "target": "MANAGE",
          "type": "fosters",
          "description": "The MEASURE function fosters the ongoing MANAGE function in the AI RMF process.",
          "strength": 7
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "AI lifecycle",
          "type": "applicable_to",
          "description": "The NIST AI RMF framework is applicable to various stages of the AI lifecycle.",
          "strength": 8
        }
      ],
      "chunk_time": 19.560460000000603,
      "chunk_tokens": 360,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 34,
      "text": "\nshould be iterative, with cross-referencing between functions as necessary. Simi-\nlarly, there are categories and subcategories with elements that apply to multiple\nfunctions, or that logically should take place before certain subcategory decisions.\n5.1\nGovern\nThe GOVERN function:\n\u2022 cultivates and implements a culture of risk management within organizations design-\ning, developing, deploying, evaluating, or acquiring AI systems;\n\u2022 outlines processes, documents, and organizational schemes that anticipate, identify,\nand manage the risks a system can pose, including to users and others across society\n\u2013 and procedures to achieve those outcomes;\n\u2022 incorporates processes to assess potential impacts;\n\u2022 provides a structure by which AI risk management functions can align with organi-\nzational principles, policies, and strategic priorities;\n\u2022 connects technical aspects of AI system design and development to organizational\nvalues and principles, and enables organizational practices and competencies for the\nindividuals involved in acquiring, training, deploying, and monitoring such systems;\nand\n\u2022 addresses full product lifecycle and associated processes, including legal and other\nissues concerning use of third-party software or hardware systems and data.\nPage 21\n\n\nNIST AI 100-1\nAI RMF 1.0\nGOVERN is a cross-cutting function that is infused throughout AI risk management and\nenables the other functions of the process. Aspects of GOVERN, especially those related to\ncompliance or evaluation, should be integrated into each of the other functions. Attention\nto governance is a continual and intrinsic requirement for effective AI risk management\nover an AI system\u2019s lifespan and the organization\u2019s hierarchy.\nStrong governance can drive and enhance internal practices and norms to facilitate orga-\nnizational risk culture. Governing authorities can determine the overarching policies that\ndirect an organization\u2019s mission, goals, values, culture, and risk tolerance. Senior leader-\nship sets the tone for risk management within an organization, and with it, organizational\nculture.",
      "rephrase": "The process should be iterative, with functions referencing each other as needed. There are categories and subcategories that include elements relevant to multiple functions, or that should logically occur before certain decisions in subcategories.\n\n**Govern**\nThe GOVERN function:\n- Promotes and implements a culture of risk management in organizations that design, develop, deploy, evaluate, or acquire AI systems.\n- Establishes processes, documents, and organizational structures to anticipate, identify, and manage risks posed by the system, including risks to users and society, along with procedures to achieve these goals.\n- Includes processes to assess potential impacts.\n- Provides a framework for aligning AI risk management with the organization\u2019s principles, policies, and strategic priorities.\n- Links the technical aspects of AI system design and development to the organization\u2019s values and principles, and supports the practices and skills of individuals involved in acquiring, training, deploying, and monitoring these systems.\n- Addresses the entire product lifecycle and related processes, including legal issues concerning the use of third-party software, hardware, or data.\n\nGOVERN is a fundamental function that runs throughout AI risk management and supports other functions in the process. Elements of GOVERN, particularly those related to compliance or evaluation, should be integrated into all other functions. Ongoing attention to governance is essential for effective AI risk management throughout the lifespan of an AI system and within the organization\u2019s structure.\n\nStrong governance can enhance internal practices and norms, fostering a culture of risk awareness within the organization. Governing authorities set the main policies that guide the organization\u2019s mission, goals, values, culture, and risk tolerance. Senior leadership plays a crucial role in establishing the approach to risk management and shaping the organizational culture.",
      "entities": [
        {
          "name": "GOVERN",
          "type": "function",
          "description": "The GOVERN function cultivates and implements a culture of risk management within organizations involved in AI systems, outlining processes to manage risks and align with organizational principles."
        },
        {
          "name": "AI RMF 1.0",
          "type": "document",
          "description": "AI RMF 1.0 is a framework that outlines risk management practices for AI systems, emphasizing the importance of governance throughout the AI lifecycle."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "NIST AI 100-1 is a publication that provides guidelines and standards for managing risks associated with AI technologies."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "AI systems refer to technologies that utilize artificial intelligence to perform tasks that typically require human intelligence, such as decision-making and problem-solving."
        },
        {
          "name": "organizational culture",
          "type": "category",
          "description": "Organizational culture encompasses the values, beliefs, and behaviors that shape how an organization operates and manages risk."
        },
        {
          "name": "risk management",
          "type": "function",
          "description": "Risk management involves identifying, assessing, and prioritizing risks followed by coordinated efforts to minimize, monitor, and control the probability or impact of unfortunate events."
        },
        {
          "name": "governing authorities",
          "type": "organization",
          "description": "Governing authorities are entities that establish policies and frameworks to guide an organization's mission, goals, and risk tolerance."
        },
        {
          "name": "senior leadership",
          "type": "person",
          "description": "Senior leadership refers to the top executives in an organization who set the tone for risk management and influence organizational culture."
        }
      ],
      "relations": [
        {
          "source": "GOVERN",
          "target": "AI RMF 1.0",
          "type": "supports",
          "description": "The GOVERN function supports the implementation of AI RMF 1.0 by integrating governance into AI risk management practices.",
          "strength": 8
        },
        {
          "source": "GOVERN",
          "target": "AI systems",
          "type": "applicable_to",
          "description": "The GOVERN function is applicable to AI systems, ensuring that risk management practices are implemented throughout their lifecycle.",
          "strength": 9
        },
        {
          "source": "organizational culture",
          "target": "risk management",
          "type": "fosters",
          "description": "A strong organizational culture fosters effective risk management practices within an organization.",
          "strength": 7
        },
        {
          "source": "governing authorities",
          "target": "organizational culture",
          "type": "directed_by",
          "description": "Governing authorities direct the organizational culture by establishing overarching policies and values.",
          "strength": 8
        },
        {
          "source": "senior leadership",
          "target": "risk management",
          "type": "influences",
          "description": "Senior leadership influences the approach to risk management within an organization by setting the tone and expectations.",
          "strength": 9
        }
      ],
      "chunk_time": 20.894811500002106,
      "chunk_tokens": 398,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 35,
      "text": " culture. Governing authorities can determine the overarching policies that\ndirect an organization\u2019s mission, goals, values, culture, and risk tolerance. Senior leader-\nship sets the tone for risk management within an organization, and with it, organizational\nculture. Management aligns the technical aspects of AI risk management to policies and\noperations. Documentation can enhance transparency, improve human review processes,\nand bolster accountability in AI system teams.\nAfter putting in place the structures, systems, processes, and teams described in the GOV-\nERN function, organizations should benefit from a purpose-driven culture focused on risk\nunderstanding and management. It is incumbent on Framework users to continue to ex-\necute the GOVERN function as knowledge, cultures, and needs or expectations from AI\nactors evolve over time.\nPractices related to governing AI risks are described in the NIST AI RMF Playbook. Table\n1 lists the GOVERN function\u2019s categories and subcategories.\nTable 1: Categories and subcategories for the GOVERN function.\nGOVERN 1:\nPolicies, processes,\nprocedures, and\npractices across the\norganization related\nto the mapping,\nmeasuring, and\nmanaging of AI\nrisks are in place,\ntransparent, and\nimplemented\neffectively.\nGOVERN 1.1: Legal and regulatory requirements involving AI\nare understood, managed, and documented.\nGOVERN 1.2: The characteristics of trustworthy AI are inte-\ngrated into organizational policies, processes, procedures, and\npractices.\nGOVERN 1.3: Processes, procedures, and practices are in place\nto determine the needed level of risk management activities based\non the organization\u2019s risk tolerance.\nGOVERN 1.4: The risk management process and its outcomes are\nestablished through transparent policies, procedures, and other\ncontrols based on organizational risk priorities.\n",
      "rephrase": "Governing authorities set the main policies that guide an organization's mission, goals, values, culture, and approach to risk. Senior leadership plays a crucial role in shaping the organization's attitude towards risk management and its overall culture. Management ensures that the technical aspects of AI risk management align with these policies and operations. Proper documentation can improve transparency, enhance human review processes, and increase accountability within AI teams.\n\nOnce organizations establish the structures, systems, processes, and teams outlined in the GOVERN function, they can foster a culture focused on understanding and managing risks. It is essential for users of the Framework to keep implementing the GOVERN function as knowledge, culture, and expectations from AI stakeholders change over time.\n\nThe NIST AI RMF Playbook details practices for governing AI risks. Below is a summary of the categories and subcategories of the GOVERN function:\n\n**GOVERN 1:** Policies, processes, procedures, and practices for identifying, measuring, and managing AI risks are established, clear, and effectively implemented.\n- **GOVERN 1.1:** Legal and regulatory requirements related to AI are understood, managed, and documented.\n- **GOVERN 1.2:** The principles of trustworthy AI are incorporated into the organization's policies, processes, procedures, and practices.\n- **GOVERN 1.3:** There are processes and procedures to determine the appropriate level of risk management based on the organization's risk tolerance.\n- **GOVERN 1.4:** The risk management process and its results are defined through clear policies, procedures, and controls that reflect the organization's risk priorities.",
      "entities": [
        {
          "name": "GOVERN function",
          "type": "function",
          "description": "A framework that outlines the policies, processes, procedures, and practices for mapping, measuring, and managing AI risks within an organization."
        },
        {
          "name": "NIST AI RMF Playbook",
          "type": "document",
          "description": "A document that describes practices related to governing AI risks, including categories and subcategories for the GOVERN function."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "Potential threats and vulnerabilities associated with the deployment and management of artificial intelligence systems."
        },
        {
          "name": "organizational culture",
          "type": "category",
          "description": "The shared values, beliefs, and practices that shape how an organization operates, particularly in relation to risk management."
        },
        {
          "name": "risk tolerance",
          "type": "category",
          "description": "The level of risk that an organization is willing to accept in pursuit of its objectives."
        }
      ],
      "relations": [
        {
          "source": "GOVERN function",
          "target": "NIST AI RMF Playbook",
          "type": "supports",
          "description": "The GOVERN function is supported by the practices described in the NIST AI RMF Playbook.",
          "strength": 8
        },
        {
          "source": "organizational culture",
          "target": "risk management",
          "type": "aligned_with",
          "description": "Organizational culture is aligned with the principles of risk management as set by senior leadership.",
          "strength": 7
        },
        {
          "source": "risk tolerance",
          "target": "GOVERN function",
          "type": "applicable_to",
          "description": "The risk tolerance of an organization is applicable to the activities outlined in the GOVERN function.",
          "strength": 9
        },
        {
          "source": "AI risks",
          "target": "GOVERN function",
          "type": "managed_by",
          "description": "AI risks are managed through the processes and practices established in the GOVERN function.",
          "strength": 10
        }
      ],
      "chunk_time": 10.45752658299898,
      "chunk_tokens": 376,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 36,
      "text": " the needed level of risk management activities based\non the organization\u2019s risk tolerance.\nGOVERN 1.4: The risk management process and its outcomes are\nestablished through transparent policies, procedures, and other\ncontrols based on organizational risk priorities.\nCategories\nSubcategories\nContinued on next page\nPage 22\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 1: Categories and subcategories for the GOVERN function. (Continued)\nGOVERN 1.5: Ongoing monitoring and periodic review of the\nrisk management process and its outcomes are planned and or-\nganizational roles and responsibilities clearly defined, including\ndetermining the frequency of periodic review.\nGOVERN 1.6: Mechanisms are in place to inventory AI systems\nand are resourced according to organizational risk priorities.\nGOVERN 1.7: Processes and procedures are in place for decom-\nmissioning and phasing out AI systems safely and in a man-\nner that does not increase risks or decrease the organization\u2019s\ntrustworthiness.\nGOVERN 2:\nAccountability\nstructures are in\nplace so that the\nappropriate teams\nand individuals are\nempowered,\nresponsible, and\ntrained for mapping,\nmeasuring, and\nmanaging AI risks.\nGOVERN 2.1: Roles and responsibilities and lines of communi-\ncation related to mapping, measuring, and managing AI risks are\ndocumented and are clear to individuals and teams throughout\nthe organization.\nGOVERN 2.2: The organization\u2019s personnel and partners receive\nAI risk management training to enable them to perform their du-\nties and responsibilities consistent with related policies, proce-\ndures, and agreements.\nGOVERN 2.3: Executive leadership of the organization takes re-\nsponsibility for decisions about risks associated with AI system\ndevelopment and deployment.\n",
      "rephrase": "Organizations must manage risks according to their risk tolerance. The risk management process and its results should be guided by clear policies and procedures that reflect the organization's risk priorities. \n\n1. Ongoing monitoring and regular reviews of the risk management process should be planned, with defined roles and responsibilities, including how often these reviews occur. \n2. There should be systems in place to keep track of AI systems, with resources allocated based on the organization's risk priorities. \n3. Safe procedures must be established for decommissioning AI systems to avoid increasing risks or harming the organization's trustworthiness. \n\nAccountability structures must ensure that the right teams and individuals are empowered, responsible, and trained to identify, measure, and manage AI risks. \n\n1. Roles, responsibilities, and communication lines regarding AI risk management should be documented and understood by everyone in the organization. \n2. Staff and partners should receive training on AI risk management to fulfill their roles in line with relevant policies and agreements. \n3. Executive leadership must take responsibility for decisions related to the risks of developing and deploying AI systems.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines categories and subcategories for the governance of AI risk management."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "A framework that provides guidelines for managing risks associated with AI systems."
        },
        {
          "name": "GOVERN function",
          "type": "function",
          "description": "A function that encompasses the processes and policies for managing AI risks within an organization."
        },
        {
          "name": "organizational risk priorities",
          "type": "category",
          "description": "The priorities set by an organization to manage risks associated with its operations, particularly in AI."
        },
        {
          "name": "AI risk management training",
          "type": "practice",
          "description": "Training provided to personnel and partners to equip them with the skills needed to manage AI risks effectively."
        },
        {
          "name": "executive leadership",
          "type": "person",
          "description": "The individuals in leadership positions within the organization responsible for decision-making regarding AI risks."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "aligned_with",
          "description": "NIST AI 100-1 aligns with the AI RMF 1.0 framework for risk management.",
          "strength": 8
        },
        {
          "source": "GOVERN function",
          "target": "organizational risk priorities",
          "type": "supports",
          "description": "The GOVERN function supports the establishment of organizational risk priorities.",
          "strength": 9
        },
        {
          "source": "AI risk management training",
          "target": "organizational risk priorities",
          "type": "contributes_to",
          "description": "AI risk management training contributes to achieving the organization's risk priorities.",
          "strength": 7
        },
        {
          "source": "executive leadership",
          "target": "AI risk management training",
          "type": "manages",
          "description": "Executive leadership manages the implementation of AI risk management training within the organization.",
          "strength": 8
        }
      ],
      "chunk_time": 12.161534000006213,
      "chunk_tokens": 382,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 37,
      "text": " du-\nties and responsibilities consistent with related policies, proce-\ndures, and agreements.\nGOVERN 2.3: Executive leadership of the organization takes re-\nsponsibility for decisions about risks associated with AI system\ndevelopment and deployment.\nGOVERN 3:\nWorkforce diversity,\nequity, inclusion,\nand accessibility\nprocesses are\nprioritized in the\nmapping,\nmeasuring, and\nmanaging of AI\nrisks throughout the\nlifecycle.\nGOVERN 3.1: Decision-making related to mapping, measuring,\nand managing AI risks throughout the lifecycle is informed by a\ndiverse team (e.g., diversity of demographics, disciplines, expe-\nrience, expertise, and backgrounds).\nGOVERN 3.2: Policies and procedures are in place to define and\ndifferentiate roles and responsibilities for human-AI configura-\ntions and oversight of AI systems.\nGOVERN 4:\nOrganizational\nteams are committed\nto a culture\nGOVERN 4.1: Organizational policies and practices are in place\nto foster a critical thinking and safety-first mindset in the design,\ndevelopment, deployment, and uses of AI systems to minimize\npotential negative impacts.\nCategories\nSubcategories\nContinued on next page\nPage 23\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 1: Categories and subcategories for the GOVERN function. (Continued)\nthat considers and\ncommunicates AI\nrisk.\nGOVERN 4.2: Organizational teams document the risks and po-\ntential impacts of the AI technology they design, develop, deploy,\nevaluate, and use, and they communicate about the impacts more\nbroadly.\nGOVERN 4.3: Organizational practices are in place to enable AI\ntesting, identification of incidents, and information sharing.\nGOVERN 5:\nProcesses are in\nplace for robust\nengagement with\nrelevant AI actors.\n",
      "rephrase": "The organization has specific duties and responsibilities that align with its policies, procedures, and agreements. \n\n**GOVERN 2.3:** The executive leadership is responsible for making decisions about the risks involved in developing and deploying AI systems. \n\n**GOVERN 3:** The organization prioritizes diversity, equity, inclusion, and accessibility when assessing AI risks throughout its lifecycle. \n\n**GOVERN 3.1:** A diverse team, representing various demographics, disciplines, experiences, and expertise, informs decisions related to assessing AI risks. \n\n**GOVERN 3.2:** There are clear policies and procedures that define roles and responsibilities for managing human-AI interactions and overseeing AI systems. \n\n**GOVERN 4:** Teams within the organization are dedicated to fostering a culture that acknowledges and communicates AI risks. \n\n**GOVERN 4.1:** Policies and practices encourage critical thinking and a safety-first approach in the design, development, deployment, and use of AI systems to reduce potential negative impacts. \n\n**GOVERN 4.2:** Teams document the risks and potential impacts of the AI technologies they work with and share this information widely. \n\n**GOVERN 4.3:** The organization has practices in place for testing AI, identifying incidents, and sharing information. \n\n**GOVERN 5:** There are processes for engaging effectively with relevant stakeholders in the AI field.",
      "entities": [],
      "relations": [],
      "chunk_time": 24.334286915996927,
      "chunk_tokens": 400,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 38,
      "text": "ly.\nGOVERN 4.3: Organizational practices are in place to enable AI\ntesting, identification of incidents, and information sharing.\nGOVERN 5:\nProcesses are in\nplace for robust\nengagement with\nrelevant AI actors.\nGOVERN 5.1: Organizational policies and practices are in place\nto collect, consider, prioritize, and integrate feedback from those\nexternal to the team that developed or deployed the AI system\nregarding the potential individual and societal impacts related to\nAI risks.\nGOVERN 5.2: Mechanisms are established to enable the team\nthat developed or deployed AI systems to regularly incorporate\nadjudicated feedback from relevant AI actors into system design\nand implementation.\nGOVERN 6: Policies\nand procedures are\nin place to address\nAI risks and benefits\narising from\nthird-party software\nand data and other\nsupply chain issues.\nGOVERN 6.1: Policies and procedures are in place that address\nAI risks associated with third-party entities, including risks of in-\nfringement of a third-party\u2019s intellectual property or other rights.\nGOVERN 6.2: Contingency processes are in place to handle\nfailures or incidents in third-party data or AI systems deemed to\nbe high-risk.\nCategories\nSubcategories\n5.2\nMap\nThe MAP function establishes the context to frame risks related to an AI system. The AI\nlifecycle consists of many interdependent activities involving a diverse set of actors (See\nFigure 3). In practice, AI actors in charge of one part of the process often do not have full\nvisibility or control over other parts and their associated contexts. The interdependencies\nbetween these activities, and among the relevant AI actors, can make it difficult to reliably\nanticipate impacts of AI systems.",
      "rephrase": "GOVERN 4.3: There are organizational practices in place for testing AI, identifying incidents, and sharing information. \nGOVERN 5: There are processes for effective engagement with relevant AI stakeholders. \nGOVERN 5.1: The organization has policies to gather, consider, prioritize, and integrate feedback from people outside the team that developed or deployed the AI system, focusing on potential individual and societal impacts related to AI risks. \nGOVERN 5.2: Mechanisms are in place for the team that developed or deployed AI systems to regularly include validated feedback from relevant AI stakeholders into the design and implementation of the system. \nGOVERN 6: There are policies and procedures to manage AI risks and benefits that arise from third-party software, data, and supply chain issues. \nGOVERN 6.1: Policies are established to address AI risks linked to third-party entities, including potential infringements of their intellectual property or other rights. \nGOVERN 6.2: Contingency plans are in place to manage failures or incidents involving high-risk third-party data or AI systems. \nThe MAP function helps to frame risks related to an AI system. The AI lifecycle includes many interconnected activities involving various actors. Often, those responsible for one part of the process do not have complete visibility or control over other parts and their contexts. These interdependencies can make it challenging to predict the impacts of AI systems reliably.",
      "entities": [
        {
          "name": "GOVERN 4.3",
          "type": "document",
          "description": "A guideline outlining organizational practices for enabling AI testing, incident identification, and information sharing."
        },
        {
          "name": "GOVERN 5",
          "type": "document",
          "description": "A guideline that emphasizes the importance of robust engagement with relevant AI actors."
        },
        {
          "name": "GOVERN 5.1",
          "type": "document",
          "description": "A guideline detailing the need for organizational policies to collect and integrate feedback from external sources regarding AI risks."
        },
        {
          "name": "GOVERN 5.2",
          "type": "document",
          "description": "A guideline that establishes mechanisms for incorporating feedback from relevant AI actors into system design."
        },
        {
          "name": "GOVERN 6",
          "type": "document",
          "description": "A guideline that addresses AI risks and benefits arising from third-party software and data."
        },
        {
          "name": "GOVERN 6.1",
          "type": "document",
          "description": "A guideline focused on policies addressing AI risks associated with third-party entities."
        },
        {
          "name": "GOVERN 6.2",
          "type": "document",
          "description": "A guideline that outlines contingency processes for handling failures in high-risk third-party data or AI systems."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function that establishes the context to frame risks related to an AI system."
        },
        {
          "name": "AI lifecycle",
          "type": "category",
          "description": "A series of interdependent activities involving various actors in the development and deployment of AI systems."
        },
        {
          "name": "AI actors",
          "type": "person",
          "description": "Individuals or entities responsible for different parts of the AI lifecycle."
        }
      ],
      "relations": [
        {
          "source": "GOVERN 5",
          "target": "AI actors",
          "type": "supports",
          "description": "GOVERN 5 supports robust engagement with relevant AI actors.",
          "strength": 8
        },
        {
          "source": "GOVERN 5.1",
          "target": "AI risks",
          "type": "addresses",
          "description": "GOVERN 5.1 addresses AI risks by integrating feedback from external sources.",
          "strength": 7
        },
        {
          "source": "GOVERN 5.2",
          "target": "system design",
          "type": "contributes_to",
          "description": "GOVERN 5.2 contributes to system design by incorporating feedback from relevant AI actors.",
          "strength": 8
        },
        {
          "source": "GOVERN 6",
          "target": "third-party software",
          "type": "addresses",
          "description": "GOVERN 6 addresses AI risks and benefits arising from third-party software.",
          "strength": 9
        },
        {
          "source": "GOVERN 6.1",
          "target": "third-party entities",
          "type": "addresses",
          "description": "GOVERN 6.1 addresses risks associated with third-party entities.",
          "strength": 8
        },
        {
          "source": "GOVERN 6.2",
          "target": "high-risk systems",
          "type": "provides",
          "description": "GOVERN 6.2 provides contingency processes for handling failures in high-risk systems.",
          "strength": 9
        },
        {
          "source": "MAP function",
          "target": "AI lifecycle",
          "type": "illustrates",
          "description": "The MAP function illustrates the interdependencies within the AI lifecycle.",
          "strength": 7
        },
        {
          "source": "AI actors",
          "target": "AI lifecycle",
          "type": "operates_under",
          "description": "AI actors operate under the framework of the AI lifecycle.",
          "strength": 8
        }
      ],
      "chunk_time": 21.802272916997026,
      "chunk_tokens": 372,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 39,
      "text": " of the process often do not have full\nvisibility or control over other parts and their associated contexts. The interdependencies\nbetween these activities, and among the relevant AI actors, can make it difficult to reliably\nanticipate impacts of AI systems. For example, early decisions in identifying purposes and\nobjectives of an AI system can alter its behavior and capabilities, and the dynamics of de-\nployment setting (such as end users or impacted individuals) can shape the impacts of AI\nsystem decisions. As a result, the best intentions within one dimension of the AI lifecycle\ncan be undermined via interactions with decisions and conditions in other, later activities.\nPage 24\n\n\nNIST AI 100-1\nAI RMF 1.0\nThis complexity and varying levels of visibility can introduce uncertainty into risk man-\nagement practices. Anticipating, assessing, and otherwise addressing potential sources of\nnegative risk can mitigate this uncertainty and enhance the integrity of the decision process.\nThe information gathered while carrying out the MAP function enables negative risk pre-\nvention and informs decisions for processes such as model management, as well as an\ninitial decision about appropriateness or the need for an AI solution. Outcomes in the\nMAP function are the basis for the MEASURE and MANAGE functions. Without contex-\ntual knowledge, and awareness of risks within the identified contexts, risk management is\ndifficult to perform. The MAP function is intended to enhance an organization\u2019s ability to\nidentify risks and broader contributing factors.\nImplementation of this function is enhanced by incorporating perspectives from a diverse\ninternal team and engagement with those external to the team that developed or deployed\nthe AI system. Engagement with external collaborators, end users, potentially impacted\ncommunities, and others may vary based on the risk level of a particular AI system, the\nmakeup of the internal team, and organizational policies.",
      "rephrase": "People involved in the AI process often lack complete visibility and control over different parts and their contexts. The connections between various activities and the AI stakeholders can make it hard to predict the effects of AI systems reliably. For instance, early choices about the goals of an AI system can change how it behaves and what it can do, while the environment where it is deployed (like the end users or those affected) can influence the outcomes of the AI's decisions. Therefore, even well-meaning actions in one part of the AI lifecycle can be negatively impacted by decisions and conditions in later stages.\n\nThis complexity and varying levels of visibility can create uncertainty in managing risks. By anticipating and addressing potential negative risks, organizations can reduce this uncertainty and improve their decision-making process. The information collected during the MAP function helps prevent negative risks and guides decisions related to model management and whether an AI solution is appropriate. The results from the MAP function are crucial for the MEASURE and MANAGE functions. Without understanding the context and being aware of risks, effective risk management is challenging. The MAP function aims to improve an organization\u2019s ability to identify risks and other contributing factors.\n\nImplementing this function is more effective when it includes insights from a diverse internal team and involves collaboration with people outside the team that created or deployed the AI system. The level of engagement with external partners, end users, and affected communities may vary depending on the risk level of the AI system, the composition of the internal team, and the organization's policies.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document related to AI risk management frameworks published by NIST."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0, which provides guidelines for managing risks associated with AI systems."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function that enables negative risk prevention and informs decisions for processes such as model management."
        },
        {
          "name": "MEASURE function",
          "type": "function",
          "description": "A function that is based on the outcomes of the MAP function and is involved in assessing AI risks."
        },
        {
          "name": "MANAGE function",
          "type": "function",
          "description": "A function that follows the MAP function and is focused on managing identified risks in AI systems."
        },
        {
          "name": "internal team",
          "type": "organization",
          "description": "A diverse team within an organization that contributes to the development and deployment of AI systems."
        },
        {
          "name": "external collaborators",
          "type": "organization",
          "description": "Individuals or groups outside the internal team that engage with the team to provide perspectives on AI systems."
        },
        {
          "name": "end users",
          "type": "person",
          "description": "Individuals who use the AI systems and may be impacted by their decisions."
        },
        {
          "name": "potentially impacted communities",
          "type": "category",
          "description": "Groups of people who may be affected by the deployment and decisions of AI systems."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document published by NIST that outlines the AI RMF 1.0 framework.",
          "strength": 9
        },
        {
          "source": "MAP function",
          "target": "MEASURE function",
          "type": "supports",
          "description": "The outcomes of the MAP function support the MEASURE function in assessing AI risks.",
          "strength": 8
        },
        {
          "source": "MAP function",
          "target": "MANAGE function",
          "type": "supports",
          "description": "The MAP function provides foundational outcomes that inform the MANAGE function.",
          "strength": 8
        },
        {
          "source": "internal team",
          "target": "external collaborators",
          "type": "engages_with",
          "description": "The internal team engages with external collaborators to incorporate diverse perspectives in AI development.",
          "strength": 7
        },
        {
          "source": "internal team",
          "target": "end users",
          "type": "engages_with",
          "description": "The internal team interacts with end users to understand their needs and impacts of AI systems.",
          "strength": 7
        },
        {
          "source": "internal team",
          "target": "potentially impacted communities",
          "type": "engages_with",
          "description": "The internal team may engage with potentially impacted communities based on the risk level of AI systems.",
          "strength": 6
        }
      ],
      "chunk_time": 19.097327666000638,
      "chunk_tokens": 378,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 40,
      "text": " developed or deployed\nthe AI system. Engagement with external collaborators, end users, potentially impacted\ncommunities, and others may vary based on the risk level of a particular AI system, the\nmakeup of the internal team, and organizational policies. Gathering such broad perspec-\ntives can help organizations proactively prevent negative risks and develop more trustwor-\nthy AI systems by:\n\u2022 improving their capacity for understanding contexts;\n\u2022 checking their assumptions about context of use;\n\u2022 enabling recognition of when systems are not functional within or out of their in-\ntended context;\n\u2022 identifying positive and beneficial uses of their existing AI systems;\n\u2022 improving understanding of limitations in AI and ML processes;\n\u2022 identifying constraints in real-world applications that may lead to negative impacts;\n\u2022 identifying known and foreseeable negative impacts related to intended use of AI\nsystems; and\n\u2022 anticipating risks of the use of AI systems beyond intended use.\nAfter completing the MAP function, Framework users should have sufficient contextual\nknowledge about AI system impacts to inform an initial go/no-go decision about whether\nto design, develop, or deploy an AI system. If a decision is made to proceed, organizations\nshould utilize the MEASURE and MANAGE functions along with policies and procedures\nput into place in the GOVERN function to assist in AI risk management efforts. It is incum-\nbent on Framework users to continue applying the MAP function to AI systems as context,\ncapabilities, risks, benefits, and potential impacts evolve over time.\nPractices related to mapping AI risks are described in the NIST AI RMF Playbook. Table\n2 lists the MAP function\u2019s categories and subcategories.\nPage 25\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 2: Categories and subcategories for the MAP function.\nMAP 1: Context is\nestablished and\nunderstood.\nMAP 1.",
      "rephrase": "The development and deployment of an AI system involves engaging with external collaborators, end users, and affected communities. The level of engagement depends on the risk associated with the AI system, the internal team composition, and the organization's policies. Gathering diverse perspectives helps organizations prevent negative risks and build more trustworthy AI systems by: \n- Enhancing understanding of different contexts; \n- Validating assumptions about how the AI will be used; \n- Recognizing when the AI system is not functioning as intended; \n- Identifying beneficial uses of existing AI systems; \n- Understanding the limitations of AI and machine learning processes; \n- Recognizing real-world constraints that could lead to negative outcomes; \n- Identifying known and potential negative impacts of the AI's intended use; \n- Anticipating risks associated with unintended uses of AI systems. \nAfter completing the MAP function, users of the Framework should have enough knowledge about the impacts of the AI system to make an initial decision on whether to proceed with its design, development, or deployment. If they choose to move forward, organizations should use the MEASURE and MANAGE functions, along with the policies established in the GOVERN function, to manage AI risks. It is important for users to continually apply the MAP function as contexts, capabilities, risks, benefits, and impacts change over time. Practices for mapping AI risks are outlined in the NIST AI RMF Playbook. Table 2 provides the categories and subcategories for the MAP function.",
      "entities": [
        {
          "name": "AI system",
          "type": "system",
          "description": "A technology designed to perform tasks that typically require human intelligence, such as understanding context and recognizing limitations."
        },
        {
          "name": "NIST AI RMF Playbook",
          "type": "document",
          "description": "A guideline that describes practices related to mapping AI risks and provides a framework for managing AI system impacts."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function within the NIST AI RMF that helps users establish and understand the context of AI systems to inform decision-making."
        },
        {
          "name": "MEASURE function",
          "type": "function",
          "description": "A function that assists organizations in evaluating AI systems as part of risk management efforts."
        },
        {
          "name": "MANAGE function",
          "type": "function",
          "description": "A function that supports organizations in managing risks associated with AI systems."
        },
        {
          "name": "GOVERN function",
          "type": "function",
          "description": "A function that involves implementing policies and procedures for AI risk management."
        },
        {
          "name": "Framework users",
          "type": "person",
          "description": "Individuals or organizations that utilize the NIST AI RMF to assess and manage AI system risks."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "Potential negative impacts associated with the use of AI systems, including limitations and unforeseen consequences."
        }
      ],
      "relations": [
        {
          "source": "NIST AI RMF Playbook",
          "target": "MAP function",
          "type": "includes",
          "description": "The NIST AI RMF Playbook includes the MAP function as part of its guidelines for managing AI risks.",
          "strength": 9
        },
        {
          "source": "Framework users",
          "target": "MAP function",
          "type": "utilizes",
          "description": "Framework users utilize the MAP function to inform decisions about AI system design and deployment.",
          "strength": 8
        },
        {
          "source": "MAP function",
          "target": "AI risks",
          "type": "identifies",
          "description": "The MAP function identifies AI risks to help organizations understand potential negative impacts.",
          "strength": 8
        },
        {
          "source": "GOVERN function",
          "target": "AI risks",
          "type": "manages",
          "description": "The GOVERN function manages AI risks through the implementation of policies and procedures.",
          "strength": 7
        },
        {
          "source": "MEASURE function",
          "target": "AI risks",
          "type": "evaluates",
          "description": "The MEASURE function evaluates AI risks as part of the overall risk management strategy.",
          "strength": 7
        },
        {
          "source": "MANAGE function",
          "target": "AI risks",
          "type": "supports",
          "description": "The MANAGE function supports organizations in their efforts to mitigate AI risks.",
          "strength": 7
        }
      ],
      "chunk_time": 17.25107625000237,
      "chunk_tokens": 381,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 41,
      "text": " 25\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 2: Categories and subcategories for the MAP function.\nMAP 1: Context is\nestablished and\nunderstood.\nMAP 1.1: Intended purposes, potentially beneficial uses, context-\nspecific laws, norms and expectations, and prospective settings in\nwhich the AI system will be deployed are understood and docu-\nmented. Considerations include: the specific set or types of users\nalong with their expectations; potential positive and negative im-\npacts of system uses to individuals, communities, organizations,\nsociety, and the planet; assumptions and related limitations about\nAI system purposes, uses, and risks across the development or\nproduct AI lifecycle; and related TEVV and system metrics.\nMAP 1.2: Interdisciplinary AI actors, competencies, skills, and\ncapacities for establishing context reflect demographic diversity\nand broad domain and user experience expertise, and their par-\nticipation is documented. Opportunities for interdisciplinary col-\nlaboration are prioritized.\nMAP 1.3: The organization\u2019s mission and relevant goals for AI\ntechnology are understood and documented.\nMAP 1.4: The business value or context of business use has been\nclearly defined or \u2013 in the case of assessing existing AI systems\n\u2013 re-evaluated.\nMAP 1.5: Organizational risk tolerances are determined and\ndocumented.\nMAP 1.6: System requirements (e.g., \u201cthe system shall respect\nthe privacy of its users\u201d) are elicited from and understood by rel-\nevant AI actors. Design decisions take socio-technical implica-\ntions into account to address AI risks.\nMAP 2:\nCategorization of\nthe AI system is\nperformed.\nMAP 2.",
      "rephrase": "NIST AI 100-1 outlines the AI Risk Management Framework (RMF) 1.0, specifically focusing on the MAP function. \n\n**MAP 1: Establishing Context** \n- **MAP 1.1:** Understand and document the intended purposes of the AI system, its potential benefits, relevant laws, norms, and the environments where it will be used. This includes identifying the types of users and their expectations, assessing the positive and negative impacts on individuals, communities, organizations, society, and the environment, and recognizing any assumptions and limitations regarding the AI system's purposes and risks throughout its lifecycle. \n- **MAP 1.2:** Ensure that the team involved in the AI project includes diverse interdisciplinary members with various skills and experiences, and document their participation. Encourage collaboration across different fields. \n- **MAP 1.3:** Clearly understand and document the organization\u2019s mission and goals related to AI technology. \n- **MAP 1.4:** Define or reassess the business value or context for using the AI system. \n- **MAP 1.5:** Identify and document the organization\u2019s risk tolerances. \n- **MAP 1.6:** Gather and understand system requirements from relevant stakeholders, ensuring that design decisions consider the social and technical implications to manage AI risks. \n\n**MAP 2: Categorizing the AI System** \n- **MAP 2:** Perform a categorization of the AI system.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document published by NIST that outlines the AI Risk Management Framework (AI RMF) 1.0."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0, which provides guidelines for managing risks associated with AI systems."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function within the AI RMF that focuses on establishing and understanding the context of AI systems."
        },
        {
          "name": "AI system",
          "type": "system",
          "description": "A system that utilizes artificial intelligence technologies to perform tasks or functions."
        },
        {
          "name": "organizational risk tolerances",
          "type": "risk",
          "description": "The levels of risk that an organization is willing to accept when deploying AI technologies."
        },
        {
          "name": "interdisciplinary AI actors",
          "type": "person",
          "description": "Individuals with diverse competencies and skills involved in the development and deployment of AI systems."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document that outlines the AI RMF 1.0 framework.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "MAP function",
          "type": "includes",
          "description": "The AI RMF 1.0 includes the MAP function, which establishes and understands the context of AI systems.",
          "strength": 8
        },
        {
          "source": "MAP function",
          "target": "AI system",
          "type": "applicable_to",
          "description": "The MAP function is applicable to AI systems to ensure their context is understood.",
          "strength": 8
        },
        {
          "source": "interdisciplinary AI actors",
          "target": "organizational risk tolerances",
          "type": "evaluates",
          "description": "Interdisciplinary AI actors evaluate and document the organizational risk tolerances related to AI technologies.",
          "strength": 7
        }
      ],
      "chunk_time": 13.685015584000212,
      "chunk_tokens": 373,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 42,
      "text": " from and understood by rel-\nevant AI actors. Design decisions take socio-technical implica-\ntions into account to address AI risks.\nMAP 2:\nCategorization of\nthe AI system is\nperformed.\nMAP 2.1: The specific tasks and methods used to implement the\ntasks that the AI system will support are defined (e.g., classifiers,\ngenerative models, recommenders).\nMAP 2.2: Information about the AI system\u2019s knowledge limits\nand how system output may be utilized and overseen by humans\nis documented. Documentation provides sufficient information\nto assist relevant AI actors when making decisions and taking\nsubsequent actions.\nCategories\nSubcategories\nContinued on next page\nPage 26\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 2: Categories and subcategories for the MAP function. (Continued)\nMAP 2.3: Scientific integrity and TEVV considerations are iden-\ntified and documented, including those related to experimental\ndesign, data collection and selection (e.g., availability, repre-\nsentativeness, suitability), system trustworthiness, and construct\nvalidation.\nMAP 3: AI\ncapabilities, targeted\nusage, goals, and\nexpected benefits\nand costs compared\nwith appropriate\nbenchmarks are\nunderstood.\nMAP 3.1: Potential benefits of intended AI system functionality\nand performance are examined and documented.\nMAP 3.2: Potential costs, including non-monetary costs, which\nresult from expected or realized AI errors or system functionality\nand trustworthiness \u2013 as connected to organizational risk toler-\nance \u2013 are examined and documented.\nMAP 3.3: Targeted application scope is specified and docu-\nmented based on the system\u2019s capability, established context, and\nAI system categorization.\nMAP 3.",
      "rephrase": "The design of AI systems considers social and technical factors to manage AI risks. \n\n**MAP 2: Categorization of the AI System** \n- **MAP 2.1:** Define the specific tasks and methods the AI system will support, such as classifiers, generative models, or recommenders. \n- **MAP 2.2:** Document the AI system's knowledge limits and how its outputs can be used and monitored by humans. This documentation helps relevant AI stakeholders make informed decisions and take appropriate actions. \n- **MAP 2.3:** Identify and document considerations related to scientific integrity and TEVV (Trustworthiness, Effectiveness, Validity, and Value), including aspects of experimental design, data collection, and system trustworthiness. \n\n**MAP 3: Understanding AI Capabilities and Impacts** \n- **MAP 3.1:** Examine and document the potential benefits of the AI system's functionality and performance. \n- **MAP 3.2:** Assess and document potential costs, including non-monetary costs, that may arise from AI errors or issues with system functionality and trustworthiness, in relation to the organization's risk tolerance. \n- **MAP 3.3:** Specify and document the targeted application scope based on the AI system's capabilities, context, and categorization.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) 1.0, providing guidelines for managing risks associated with AI systems."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework 1.0 developed by NIST, which categorizes and provides guidelines for assessing and managing risks in AI systems."
        },
        {
          "name": "AI system",
          "type": "system",
          "description": "A technological system that utilizes artificial intelligence to perform specific tasks and functions."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function within the AI RMF that focuses on the categorization and assessment of AI systems."
        },
        {
          "name": "scientific integrity",
          "type": "category",
          "description": "A category that encompasses the principles of maintaining accuracy and reliability in scientific research related to AI."
        },
        {
          "name": "TEVV considerations",
          "type": "category",
          "description": "Considerations related to Trustworthiness, Explainability, Verifiability, and Validity in the context of AI systems."
        },
        {
          "name": "organizational risk tolerance",
          "type": "risk",
          "description": "The level of risk that an organization is willing to accept when implementing AI systems."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is the document that publishes the AI RMF 1.0 framework.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "MAP function",
          "type": "includes",
          "description": "The AI RMF 1.0 includes the MAP function for categorizing AI systems.",
          "strength": 8
        },
        {
          "source": "MAP function",
          "target": "AI system",
          "type": "applicable_to",
          "description": "The MAP function is applicable to AI systems for categorization and assessment.",
          "strength": 8
        },
        {
          "source": "MAP function",
          "target": "scientific integrity",
          "type": "supports",
          "description": "The MAP function supports the identification and documentation of scientific integrity considerations.",
          "strength": 7
        },
        {
          "source": "MAP function",
          "target": "TEVV considerations",
          "type": "supports",
          "description": "The MAP function supports the identification and documentation of TEVV considerations.",
          "strength": 7
        },
        {
          "source": "MAP function",
          "target": "organizational risk tolerance",
          "type": "evaluates",
          "description": "The MAP function evaluates potential costs related to organizational risk tolerance in AI systems.",
          "strength": 8
        }
      ],
      "chunk_time": 15.244883500003198,
      "chunk_tokens": 380,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 43,
      "text": " to organizational risk toler-\nance \u2013 are examined and documented.\nMAP 3.3: Targeted application scope is specified and docu-\nmented based on the system\u2019s capability, established context, and\nAI system categorization.\nMAP 3.4: Processes for operator and practitioner proficiency\nwith AI system performance and trustworthiness \u2013 and relevant\ntechnical standards and certifications \u2013 are defined, assessed, and\ndocumented.\nMAP 3.5: Processes for human oversight are defined, assessed,\nand documented in accordance with organizational policies from\nthe GOVERN function.\nMAP 4: Risks and\nbenefits are mapped\nfor all components\nof the AI system\nincluding third-party\nsoftware and data.\nMAP 4.1: Approaches for mapping AI technology and legal risks\nof its components \u2013 including the use of third-party data or soft-\nware \u2013 are in place, followed, and documented, as are risks of in-\nfringement of a third party\u2019s intellectual property or other rights.\nMAP 4.2: Internal risk controls for components of the AI sys-\ntem, including third-party AI technologies, are identified and\ndocumented.\nMAP 5: Impacts to\nindividuals, groups,\ncommunities,\norganizations, and\nsociety are\ncharacterized.\nMAP 5.1: Likelihood and magnitude of each identified impact\n(both potentially beneficial and harmful) based on expected use,\npast uses of AI systems in similar contexts, public incident re-\nports, feedback from those external to the team that developed\nor deployed the AI system, or other data are identified and\ndocumented.\nCategories\nSubcategories\nContinued on next page\nPage 27\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 2: Categories and subcategories for the MAP function. (Continued)\nMAP 5.",
      "rephrase": "The organizational risk tolerance is examined and documented. The targeted application scope is specified and documented based on the system's capabilities, context, and AI system categorization. Processes for ensuring operator and practitioner proficiency with AI system performance and trustworthiness, along with relevant technical standards and certifications, are defined, assessed, and documented. Human oversight processes are also defined, assessed, and documented according to the organization's policies from the GOVERN function. Risks and benefits of all components of the AI system, including third-party software and data, are mapped. Approaches for identifying and documenting AI technology and legal risks, including those related to third-party data or software and potential infringement of intellectual property rights, are established and followed. Internal risk controls for the AI system's components, including third-party technologies, are identified and documented. The impacts on individuals, groups, communities, organizations, and society are characterized. The likelihood and magnitude of each identified impact, both beneficial and harmful, based on expected use, past uses of similar AI systems, public incident reports, and external feedback, are identified and documented.",
      "entities": [
        {
          "name": "organizational risk tolerance",
          "type": "category",
          "description": "The level of risk that an organization is willing to accept in pursuit of its objectives."
        },
        {
          "name": "AI system categorization",
          "type": "function",
          "description": "The classification of AI systems based on their capabilities and contexts."
        },
        {
          "name": "operator and practitioner proficiency",
          "type": "function",
          "description": "The skills and knowledge required for effective operation and trustworthiness of AI systems."
        },
        {
          "name": "GOVERN function",
          "type": "function",
          "description": "A function related to governance and oversight within the context of AI systems."
        },
        {
          "name": "AI technology",
          "type": "technology",
          "description": "Technological systems and tools that utilize artificial intelligence."
        },
        {
          "name": "third-party software and data",
          "type": "resource",
          "description": "External software and data sources that may be integrated into AI systems."
        },
        {
          "name": "intellectual property rights",
          "type": "risk",
          "description": "Legal rights that grant creators control over the use of their inventions and creations."
        },
        {
          "name": "internal risk controls",
          "type": "system",
          "description": "Measures and protocols established to manage and mitigate risks within AI systems."
        },
        {
          "name": "impacts to individuals, groups, communities, organizations, and society",
          "type": "outcome",
          "description": "The effects that AI systems may have on various stakeholders and societal structures."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document published by NIST outlining guidelines and frameworks for AI risk management."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0 developed by NIST."
        }
      ],
      "relations": [
        {
          "source": "AI system categorization",
          "target": "organizational risk tolerance",
          "type": "aligned_with",
          "description": "The categorization of AI systems is aligned with the organization's risk tolerance.",
          "strength": 8
        },
        {
          "source": "operator and practitioner proficiency",
          "target": "AI system performance and trustworthiness",
          "type": "supports",
          "description": "Proficiency in operation supports the performance and trustworthiness of AI systems.",
          "strength": 9
        },
        {
          "source": "GOVERN function",
          "target": "human oversight processes",
          "type": "operates_under",
          "description": "Human oversight processes operate under the governance established by the GOVERN function.",
          "strength": 7
        },
        {
          "source": "AI technology",
          "target": "third-party software and data",
          "type": "includes",
          "description": "AI technology includes the use of third-party software and data.",
          "strength": 8
        },
        {
          "source": "internal risk controls",
          "target": "AI system components",
          "type": "manages",
          "description": "Internal risk controls manage risks associated with components of AI systems.",
          "strength": 9
        },
        {
          "source": "impacts to individuals, groups, communities, organizations, and society",
          "target": "AI systems",
          "type": "evaluates",
          "description": "The impacts of AI systems are evaluated to understand their effects on various stakeholders.",
          "strength": 8
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "The document NIST AI 100-1 is published by NIST and outlines the AI RMF 1.0.",
          "strength": 10
        }
      ],
      "chunk_time": 22.18874295799469,
      "chunk_tokens": 384,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 44,
      "text": "Subcategories\nContinued on next page\nPage 27\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 2: Categories and subcategories for the MAP function. (Continued)\nMAP 5.2: Practices and personnel for supporting regular en-\ngagement with relevant AI actors and integrating feedback about\npositive, negative, and unanticipated impacts are in place and\ndocumented.\nCategories\nSubcategories\n5.3\nMeasure\nThe MEASURE function employs quantitative, qualitative, or mixed-method tools, tech-\nniques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related\nimpacts. It uses knowledge relevant to AI risks identified in the MAP function and informs\nthe MANAGE function. AI systems should be tested before their deployment and regu-\nlarly while in operation. AI risk measurements include documenting aspects of systems\u2019\nfunctionality and trustworthiness.\nMeasuring AI risks includes tracking metrics for trustworthy characteristics, social impact,\nand human-AI configurations. Processes developed or adopted in the MEASURE function\nshould include rigorous software testing and performance assessment methodologies with\nassociated measures of uncertainty, comparisons to performance benchmarks, and formal-\nized reporting and documentation of results. Processes for independent review can improve\nthe effectiveness of testing and can mitigate internal biases and potential conflicts of inter-\nest.\nWhere tradeoffs among the trustworthy characteristics arise, measurement provides a trace-\nable basis to inform management decisions. Options may include recalibration, impact\nmitigation, or removal of the system from design, development, production, or use, as well\nas a range of compensating, detective, deterrent, directive, and recovery controls.\nAfter completing the MEASURE function, objective, repeatable, or scalable test, evaluation,\nverification, and validation (TEVV) processes including metrics, methods, and methodolo-\ngies are in place, followed, and documented.",
      "rephrase": "Subcategories\nContinued on next page\nPage 27\n\nNIST AI 100-1\nAI RMF 1.0\nTable 2: Categories and subcategories for the MAP function. (Continued)\n\nMAP 5.2: There are documented practices and personnel in place to regularly engage with relevant AI stakeholders and gather feedback on the positive, negative, and unexpected impacts of AI systems.\n\nCategories\nSubcategories\n5.3\nMeasure\nThe MEASURE function uses various tools and methods\u2014quantitative, qualitative, or a mix of both\u2014to analyze and monitor AI risks and their impacts. It builds on the AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before they are deployed and regularly while they are in use. Measuring AI risks involves documenting how well the systems function and their trustworthiness.\n\nTracking AI risks includes monitoring metrics related to trustworthy characteristics, social impact, and human-AI interactions. The MEASURE function should involve thorough software testing and performance assessments, including measures of uncertainty, comparisons to performance benchmarks, and formal reporting of results. Independent reviews can enhance testing effectiveness and reduce internal biases and conflicts of interest.\n\nWhen there are trade-offs among trustworthy characteristics, measurement provides a clear basis for management decisions. Possible actions include recalibrating the system, mitigating impacts, or even removing the system from design, development, production, or use. This may involve various controls such as compensating, detective, deterrent, directive, and recovery measures.\n\nAfter completing the MEASURE function, objective, repeatable, and scalable testing, evaluation, verification, and validation (TEVV) processes\u2014including metrics, methods, and methodologies\u2014should be established, followed, and documented.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) and its categories and subcategories."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The first version of the AI Risk Management Framework developed by NIST, focusing on managing risks associated with AI systems."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function within the AI RMF that focuses on practices and personnel for engaging with AI actors and integrating feedback."
        },
        {
          "name": "MEASURE function",
          "type": "function",
          "description": "A function that employs tools and methodologies to analyze and monitor AI risks and impacts."
        },
        {
          "name": "MANAGE function",
          "type": "function",
          "description": "A function that is informed by the MEASURE function and focuses on managing AI risks."
        },
        {
          "name": "TEVV processes",
          "type": "methodology",
          "description": "Test, evaluation, verification, and validation processes that ensure the effectiveness of AI systems."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "Potential negative impacts associated with the deployment and operation of AI systems."
        },
        {
          "name": "trustworthy characteristics",
          "type": "category",
          "description": "Attributes of AI systems that contribute to their reliability and ethical use."
        }
      ],
      "relations": [
        {
          "source": "MEASURE function",
          "target": "AI risks",
          "type": "supports",
          "description": "The MEASURE function supports the identification and analysis of AI risks.",
          "strength": 9
        },
        {
          "source": "MAP function",
          "target": "AI actors",
          "type": "includes",
          "description": "The MAP function includes practices for engaging with relevant AI actors.",
          "strength": 8
        },
        {
          "source": "MEASURE function",
          "target": "MANAGE function",
          "type": "informs",
          "description": "The MEASURE function informs the MANAGE function regarding AI risks.",
          "strength": 9
        },
        {
          "source": "TEVV processes",
          "target": "MEASURE function",
          "type": "composed_of",
          "description": "TEVV processes are composed of methodologies that are part of the MEASURE function.",
          "strength": 7
        },
        {
          "source": "AI RMF 1.0",
          "target": "NIST AI 100-1",
          "type": "published_by",
          "description": "The AI RMF 1.0 framework is published as part of the NIST AI 100-1 document.",
          "strength": 10
        }
      ],
      "chunk_time": 14.794582916998479,
      "chunk_tokens": 389,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 45,
      "text": " and recovery controls.\nAfter completing the MEASURE function, objective, repeatable, or scalable test, evaluation,\nverification, and validation (TEVV) processes including metrics, methods, and methodolo-\ngies are in place, followed, and documented. Metrics and measurement methodologies\nshould adhere to scientific, legal, and ethical norms and be carried out in an open and trans-\nparent process. New types of measurement, qualitative and quantitative, may need to be\ndeveloped. The degree to which each measurement type provides unique and meaningful\ninformation to the assessment of AI risks should be considered. Framework users will en-\nhance their capacity to comprehensively evaluate system trustworthiness, identify and track\nexisting and emergent risks, and verify efficacy of the metrics. Measurement outcomes will\nbe utilized in the MANAGE function to assist risk monitoring and response efforts. It is in-\ncumbent on Framework users to continue applying the MEASURE function to AI systems\nas knowledge, methodologies, risks, and impacts evolve over time.\nPage 28\n\n\nNIST AI 100-1\nAI RMF 1.0\nPractices related to measuring AI risks are described in the NIST AI RMF Playbook. Table\n3 lists the MEASURE function\u2019s categories and subcategories.\nTable 3: Categories and subcategories for the MEASURE function.\nMEASURE 1:\nAppropriate\nmethods and metrics\nare identified and\napplied.\nMEASURE 1.1: Approaches and metrics for measurement of AI\nrisks enumerated during the MAP function are selected for imple-\nmentation starting with the most significant AI risks. The risks\nor trustworthiness characteristics that will not \u2013 or cannot \u2013 be\nmeasured are properly documented.\nMEASURE 1.2: Appropriateness of AI metrics and effectiveness\nof existing controls are regularly assessed and updated, including\nreports of errors and potential impacts on affected communities.\nMEASURE 1.",
      "rephrase": "After completing the MEASURE function, clear and repeatable processes for testing, evaluating, verifying, and validating (TEVV) AI systems are established, documented, and followed. These processes include specific metrics and methods that comply with scientific, legal, and ethical standards and are conducted transparently. New qualitative and quantitative measurement methods may need to be created. It's important to evaluate how each type of measurement contributes unique and valuable insights into assessing AI risks. Users of the framework will improve their ability to thoroughly assess system reliability, identify and monitor existing and emerging risks, and confirm the effectiveness of the metrics. The results from these measurements will be used in the MANAGE function to support risk monitoring and response efforts. Framework users must continue to apply the MEASURE function to AI systems as knowledge, methods, risks, and impacts change over time.\n\nThe NIST AI RMF Playbook outlines practices for measuring AI risks. Table 3 details the categories and subcategories of the MEASURE function:\n\n- MEASURE 1: Appropriate methods and metrics are identified and applied.\n  - MEASURE 1.1: Metrics for measuring AI risks identified during the MAP function are selected for implementation, starting with the most significant risks. Any risks or trustworthiness characteristics that cannot be measured are documented.\n  - MEASURE 1.2: The suitability of AI metrics and the effectiveness of existing controls are regularly reviewed and updated, including reports on errors and their potential impacts on affected communities.",
      "entities": [
        {
          "name": "MEASURE function",
          "type": "function",
          "description": "A function that involves objective, repeatable, or scalable test, evaluation, verification, and validation processes for assessing AI risks."
        },
        {
          "name": "NIST AI RMF 1.0",
          "type": "framework",
          "description": "A framework developed by NIST that provides guidelines for managing risks associated with AI systems."
        },
        {
          "name": "NIST AI RMF Playbook",
          "type": "document",
          "description": "A document that describes practices related to measuring AI risks as part of the NIST AI RMF."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "Potential threats and vulnerabilities associated with the deployment and operation of AI systems."
        },
        {
          "name": "metrics and measurement methodologies",
          "type": "methodology",
          "description": "Approaches and techniques used to quantify and assess AI risks, ensuring adherence to scientific, legal, and ethical norms."
        },
        {
          "name": "system trustworthiness",
          "type": "outcome",
          "description": "The degree to which an AI system can be trusted based on its performance and risk assessment."
        },
        {
          "name": "risk monitoring and response efforts",
          "type": "action",
          "description": "Activities aimed at overseeing and addressing identified risks in AI systems."
        }
      ],
      "relations": [
        {
          "source": "NIST AI RMF 1.0",
          "target": "NIST AI RMF Playbook",
          "type": "published_by",
          "description": "The NIST AI RMF Playbook is published as part of the NIST AI RMF framework.",
          "strength": 9
        },
        {
          "source": "MEASURE function",
          "target": "AI risks",
          "type": "evaluates",
          "description": "The MEASURE function evaluates AI risks through various metrics and methodologies.",
          "strength": 8
        },
        {
          "source": "metrics and measurement methodologies",
          "target": "AI risks",
          "type": "supports",
          "description": "Metrics and measurement methodologies support the identification and assessment of AI risks.",
          "strength": 8
        },
        {
          "source": "system trustworthiness",
          "target": "MEASURE function",
          "type": "reflects",
          "description": "The effectiveness of the MEASURE function reflects the system trustworthiness of AI systems.",
          "strength": 7
        },
        {
          "source": "risk monitoring and response efforts",
          "target": "MEASURE function",
          "type": "contributes_to",
          "description": "The MEASURE function contributes to risk monitoring and response efforts by providing metrics and outcomes.",
          "strength": 8
        }
      ],
      "chunk_time": 14.390903208004602,
      "chunk_tokens": 395,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 46,
      "text": " be\nmeasured are properly documented.\nMEASURE 1.2: Appropriateness of AI metrics and effectiveness\nof existing controls are regularly assessed and updated, including\nreports of errors and potential impacts on affected communities.\nMEASURE 1.3: Internal experts who did not serve as front-line\ndevelopers for the system and/or independent assessors are in-\nvolved in regular assessments and updates.\nDomain experts,\nusers, AI actors external to the team that developed or deployed\nthe AI system, and affected communities are consulted in support\nof assessments as necessary per organizational risk tolerance.\nMEASURE 2: AI\nsystems are\nevaluated for\ntrustworthy\ncharacteristics.\nMEASURE 2.1: Test sets, metrics, and details about the tools used\nduring TEVV are documented.\nMEASURE 2.2: Evaluations involving human subjects meet ap-\nplicable requirements (including human subject protection) and\nare representative of the relevant population.\nMEASURE 2.3: AI system performance or assurance criteria\nare measured qualitatively or quantitatively and demonstrated\nfor conditions similar to deployment setting(s). Measures are\ndocumented.\nMEASURE 2.4: The functionality and behavior of the AI sys-\ntem and its components \u2013 as identified in the MAP function \u2013 are\nmonitored when in production.\nMEASURE 2.5: The AI system to be deployed is demonstrated\nto be valid and reliable. Limitations of the generalizability be-\nyond the conditions under which the technology was developed\nare documented.\nCategories\nSubcategories\nContinued on next page\nPage 29\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 3: Categories and subcategories for the MEASURE function. (Continued)\nMEASURE 2.6: The AI system is evaluated regularly for safety\nrisks \u2013 as identified in the MAP function.",
      "rephrase": "MEASURE 1.2: The suitability of AI metrics and the effectiveness of current controls are regularly reviewed and updated. This includes documenting errors and their potential effects on impacted communities. MEASURE 1.3: Internal experts who were not directly involved in developing the system, as well as independent assessors, participate in regular evaluations and updates. Domain experts, users, external AI professionals, and affected communities are consulted as needed based on the organization's risk tolerance. MEASURE 2: AI systems are assessed for trustworthy characteristics. MEASURE 2.1: Documentation includes test sets, metrics, and details about the tools used during testing and evaluation. MEASURE 2.2: Evaluations involving human subjects comply with relevant requirements, including protections for those subjects, and are representative of the relevant population. MEASURE 2.3: The performance of the AI system is measured both qualitatively and quantitatively under conditions similar to where it will be deployed, and these measures are documented. MEASURE 2.4: The functionality and behavior of the AI system and its components, as identified in the MAP function, are monitored during production. MEASURE 2.5: The AI system is proven to be valid and reliable, with documented limitations on its applicability beyond the conditions under which it was developed. MEASURE 2.6: The AI system is regularly assessed for safety risks, as identified in the MAP function.",
      "entities": [
        {
          "name": "AI RMF 1.0",
          "type": "document",
          "description": "A framework that outlines measures for assessing the appropriateness and effectiveness of AI metrics and controls."
        },
        {
          "name": "NIST",
          "type": "organization",
          "description": "The National Institute of Standards and Technology, responsible for developing standards and guidelines for various technologies, including AI."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function within the AI RMF that identifies safety risks and monitors the functionality and behavior of AI systems."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Systems that utilize artificial intelligence to perform tasks that typically require human intelligence."
        },
        {
          "name": "trustworthy characteristics",
          "type": "category",
          "description": "Attributes that ensure AI systems operate reliably and ethically."
        },
        {
          "name": "human subject protection",
          "type": "standard",
          "description": "Requirements that ensure the safety and rights of human subjects involved in evaluations."
        },
        {
          "name": "safety risks",
          "type": "risk",
          "description": "Potential hazards associated with the deployment and operation of AI systems."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "NIST",
          "type": "published_by",
          "description": "AI RMF 1.0 is published by NIST.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "MAP function",
          "type": "includes",
          "description": "The MAP function is included in the AI RMF 1.0 framework.",
          "strength": 8
        },
        {
          "source": "AI systems",
          "target": "trustworthy characteristics",
          "type": "evaluates",
          "description": "AI systems are evaluated for trustworthy characteristics.",
          "strength": 7
        },
        {
          "source": "evaluations involving human subjects",
          "target": "human subject protection",
          "type": "applicable_to",
          "description": "Evaluations involving human subjects must meet human subject protection requirements.",
          "strength": 8
        },
        {
          "source": "AI systems",
          "target": "safety risks",
          "type": "evaluates",
          "description": "AI systems are evaluated regularly for safety risks.",
          "strength": 7
        }
      ],
      "chunk_time": 14.070865500005311,
      "chunk_tokens": 393,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 47,
      "text": "F 1.0\nTable 3: Categories and subcategories for the MEASURE function. (Continued)\nMEASURE 2.6: The AI system is evaluated regularly for safety\nrisks \u2013 as identified in the MAP function. The AI system to be de-\nployed is demonstrated to be safe, its residual negative risk does\nnot exceed the risk tolerance, and it can fail safely, particularly if\nmade to operate beyond its knowledge limits. Safety metrics re-\nflect system reliability and robustness, real-time monitoring, and\nresponse times for AI system failures.\nMEASURE 2.7: AI system security and resilience \u2013 as identified\nin the MAP function \u2013 are evaluated and documented.\nMEASURE 2.8: Risks associated with transparency and account-\nability \u2013 as identified in the MAP function \u2013 are examined and\ndocumented.\nMEASURE 2.9: The AI model is explained, validated, and docu-\nmented, and AI system output is interpreted within its context \u2013\nas identified in the MAP function \u2013 to inform responsible use and\ngovernance.\nMEASURE 2.10: Privacy risk of the AI system \u2013 as identified in\nthe MAP function \u2013 is examined and documented.\nMEASURE 2.11: Fairness and bias \u2013 as identified in the MAP\nfunction \u2013 are evaluated and results are documented.\nMEASURE 2.12: Environmental impact and sustainability of AI\nmodel training and management activities \u2013 as identified in the\nMAP function \u2013 are assessed and documented.\nMEASURE 2.13: Effectiveness of the employed TEVV met-\nrics and processes in the MEASURE function are evaluated and\ndocumented.\nMEASURE 3:\nMechanisms for\ntracking identified\nAI risks over time\nare in place.\nMEASURE 3.",
      "rephrase": "F 1.0 Table 3 outlines the categories and subcategories for the MEASURE function. MEASURE 2.6 states that the AI system is regularly checked for safety risks identified in the MAP function. It must be proven safe, with any remaining risks within acceptable limits, and it should be able to fail safely, especially when pushed beyond its knowledge limits. Safety metrics include system reliability, robustness, real-time monitoring, and response times for failures. MEASURE 2.7 involves evaluating and documenting the security and resilience of the AI system, as noted in the MAP function. MEASURE 2.8 requires examining and documenting risks related to transparency and accountability, also identified in the MAP function. MEASURE 2.9 emphasizes that the AI model must be explained, validated, and documented, with its outputs interpreted in context to support responsible use and governance. MEASURE 2.10 focuses on examining and documenting privacy risks associated with the AI system, as identified in the MAP function. MEASURE 2.11 involves evaluating fairness and bias, with results documented. MEASURE 2.12 assesses and documents the environmental impact and sustainability of AI model training and management activities, as noted in the MAP function. MEASURE 2.13 evaluates and documents the effectiveness of the TEVV metrics and processes used in the MEASURE function. MEASURE 3 establishes mechanisms for tracking identified AI risks over time.",
      "entities": [
        {
          "name": "MEASURE function",
          "type": "function",
          "description": "A function that encompasses various measures for evaluating AI systems, focusing on safety, security, transparency, and other critical aspects."
        },
        {
          "name": "AI system",
          "type": "system",
          "description": "An artificial intelligence system that is evaluated for safety risks, security, transparency, and other factors to ensure responsible use and governance."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function that identifies various risks associated with AI systems, serving as a reference for evaluations in the MEASURE function."
        },
        {
          "name": "TEVV metrics",
          "type": "resource",
          "description": "Metrics and processes employed to evaluate the effectiveness of the MEASURE function in assessing AI systems."
        },
        {
          "name": "safety risks",
          "type": "risk",
          "description": "Potential hazards associated with the operation of AI systems, which are regularly evaluated to ensure they do not exceed risk tolerance."
        },
        {
          "name": "privacy risk",
          "type": "risk",
          "description": "Risks related to the privacy of data handled by AI systems, which are examined and documented."
        },
        {
          "name": "fairness and bias",
          "type": "risk",
          "description": "Concerns regarding the equitable treatment of individuals by AI systems, which are evaluated and documented."
        },
        {
          "name": "environmental impact",
          "type": "risk",
          "description": "The effects of AI model training and management activities on the environment, which are assessed and documented."
        }
      ],
      "relations": [
        {
          "source": "AI system",
          "target": "safety risks",
          "type": "evaluates",
          "description": "The AI system is evaluated for safety risks as identified in the MAP function.",
          "strength": 9
        },
        {
          "source": "AI system",
          "target": "privacy risk",
          "type": "evaluates",
          "description": "The privacy risk of the AI system is examined and documented.",
          "strength": 8
        },
        {
          "source": "AI system",
          "target": "fairness and bias",
          "type": "evaluates",
          "description": "Fairness and bias associated with the AI system are evaluated and results are documented.",
          "strength": 8
        },
        {
          "source": "AI system",
          "target": "environmental impact",
          "type": "assesses",
          "description": "The environmental impact of AI model training and management activities is assessed and documented.",
          "strength": 8
        },
        {
          "source": "MEASURE function",
          "target": "TEVV metrics",
          "type": "evaluates",
          "description": "The effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.",
          "strength": 7
        },
        {
          "source": "MEASURE function",
          "target": "MAP function",
          "type": "aligned_with",
          "description": "The MEASURE function includes evaluations based on risks identified in the MAP function.",
          "strength": 9
        }
      ],
      "chunk_time": 16.162138917003176,
      "chunk_tokens": 370,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 48,
      "text": "iveness of the employed TEVV met-\nrics and processes in the MEASURE function are evaluated and\ndocumented.\nMEASURE 3:\nMechanisms for\ntracking identified\nAI risks over time\nare in place.\nMEASURE 3.1: Approaches, personnel, and documentation are\nin place to regularly identify and track existing, unanticipated,\nand emergent AI risks based on factors such as intended and ac-\ntual performance in deployed contexts.\nMEASURE 3.2: Risk tracking approaches are considered for\nsettings where AI risks are difficult to assess using currently\navailable measurement techniques or where metrics are not yet\navailable.\nCategories\nSubcategories\nContinued on next page\nPage 30\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 3: Categories and subcategories for the MEASURE function. (Continued)\nMEASURE 3.3: Feedback processes for end users and impacted\ncommunities to report problems and appeal system outcomes are\nestablished and integrated into AI system evaluation metrics.\nMEASURE 4:\nFeedback about\nefficacy of\nmeasurement is\ngathered and\nassessed.\nMEASURE 4.1: Measurement approaches for identifying AI risks\nare connected to deployment context(s) and informed through\nconsultation with domain experts and other end users.\nAp-\nproaches are documented.\nMEASURE 4.2: Measurement results regarding AI system trust-\nworthiness in deployment context(s) and across the AI lifecycle\nare informed by input from domain experts and relevant AI ac-\ntors to validate whether the system is performing consistently as\nintended. Results are documented.\nMEASURE 4.3: Measurable performance improvements or de-\nclines based on consultations with relevant AI actors, in-\ncluding affected communities, and field data about context-\nrelevant risks and trustworthiness characteristics are identified\nand documented.\nCategories\nSubcategories\n5.",
      "rephrase": "The effectiveness of the TEVV metrics and processes used in the MEASURE function is assessed and recorded.\n\n**MEASURE 3:** Mechanisms are established to track identified AI risks over time.\n\n- **MEASURE 3.1:** There are methods, personnel, and documentation in place to regularly identify and monitor existing, unexpected, and emerging AI risks based on both intended and actual performance in real-world applications.\n- **MEASURE 3.2:** Risk tracking methods are considered for situations where AI risks are hard to evaluate using current measurement techniques or where metrics are not yet available.\n\n**MEASURE 3.3:** Feedback systems are set up for end users and affected communities to report issues and challenge system outcomes, and these are integrated into the evaluation metrics of the AI system.\n\n**MEASURE 4:** Feedback on the effectiveness of the measurement is collected and evaluated.\n\n- **MEASURE 4.1:** Measurement methods for identifying AI risks are linked to the deployment context and are developed with input from domain experts and other end users. These methods are documented.\n- **MEASURE 4.2:** Results from measuring the trustworthiness of the AI system in its deployment context and throughout its lifecycle are validated with input from domain experts and relevant AI stakeholders to ensure the system performs as intended. These results are documented.\n- **MEASURE 4.3:** Any measurable improvements or declines in performance, based on consultations with relevant AI stakeholders, including affected communities, and field data about context-specific risks and trustworthiness, are identified and documented.",
      "entities": [
        {
          "name": "MEASURE function",
          "type": "function",
          "description": "A function that evaluates the effectiveness of employed TEVV metrics and processes."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "Potential issues associated with the deployment and performance of AI systems."
        },
        {
          "name": "NIST AI RMF 1.0",
          "type": "document",
          "description": "A document that outlines the framework for managing AI risks."
        },
        {
          "name": "domain experts",
          "type": "person",
          "description": "Individuals with specialized knowledge in a particular area relevant to AI risk assessment."
        },
        {
          "name": "end users",
          "type": "person",
          "description": "Individuals who interact with AI systems and can provide feedback on their performance."
        },
        {
          "name": "affected communities",
          "type": "organization",
          "description": "Groups of people who may be impacted by the deployment of AI systems."
        },
        {
          "name": "AI lifecycle",
          "type": "category",
          "description": "The stages through which an AI system progresses from development to deployment and beyond."
        }
      ],
      "relations": [
        {
          "source": "MEASURE function",
          "target": "AI risks",
          "type": "evaluates",
          "description": "The MEASURE function evaluates the effectiveness of metrics and processes in tracking AI risks.",
          "strength": 9
        },
        {
          "source": "MEASURE function",
          "target": "NIST AI RMF 1.0",
          "type": "aligned_with",
          "description": "The MEASURE function is aligned with the guidelines provided in the NIST AI RMF 1.0 document.",
          "strength": 8
        },
        {
          "source": "AI risks",
          "target": "domain experts",
          "type": "informed_by",
          "description": "Domain experts provide insights that inform the identification and tracking of AI risks.",
          "strength": 7
        },
        {
          "source": "AI risks",
          "target": "end users",
          "type": "provides",
          "description": "End users provide feedback on AI risks based on their interactions with the systems.",
          "strength": 8
        },
        {
          "source": "AI risks",
          "target": "affected communities",
          "type": "supports",
          "description": "Affected communities are supported in reporting problems related to AI risks.",
          "strength": 7
        },
        {
          "source": "AI lifecycle",
          "target": "AI risks",
          "type": "includes",
          "description": "The AI lifecycle includes various stages where AI risks need to be assessed.",
          "strength": 8
        }
      ],
      "chunk_time": 16.30310470800032,
      "chunk_tokens": 394,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 49,
      "text": ": Measurable performance improvements or de-\nclines based on consultations with relevant AI actors, in-\ncluding affected communities, and field data about context-\nrelevant risks and trustworthiness characteristics are identified\nand documented.\nCategories\nSubcategories\n5.4\nManage\nThe MANAGE function entails allocating risk resources to mapped and measured risks on\na regular basis and as defined by the GOVERN function. Risk treatment comprises plans to\nrespond to, recover from, and communicate about incidents or events.\nContextual information gleaned from expert consultation and input from relevant AI actors\n\u2013 established in GOVERN and carried out in MAP \u2013 is utilized in this function to decrease\nthe likelihood of system failures and negative impacts. Systematic documentation practices\nestablished in GOVERN and utilized in MAP and MEASURE bolster AI risk management\nefforts and increase transparency and accountability. Processes for assessing emergent risks\nare in place, along with mechanisms for continual improvement.\nAfter completing the MANAGE function, plans for prioritizing risk and regular monitoring\nand improvement will be in place. Framework users will have enhanced capacity to man-\nage the risks of deployed AI systems and to allocate risk management resources based on\nassessed and prioritized risks. It is incumbent on Framework users to continue to apply\nthe MANAGE function to deployed AI systems as methods, contexts, risks, and needs or\nexpectations from relevant AI actors evolve over time.\nPage 31\n\n\nNIST AI 100-1\nAI RMF 1.0\nPractices related to managing AI risks are described in the NIST AI RMF Playbook. Table\n4 lists the MANAGE function\u2019s categories and subcategories.\nTable 4: Categories and subcategories for the MANAGE function.\nMANAGE 1: AI\nrisks based on\nassessments and\nother analytical\noutput from the\nMAP and MEASURE\nfunctions are\nprioritized,\nresponded to, and\nmanaged.\n",
      "rephrase": "Performance improvements or declines are measured by consulting with relevant AI stakeholders, including affected communities, and using field data about risks and trustworthiness. \n\nThe MANAGE function involves regularly allocating resources to identified risks, as determined by the GOVERN function. This includes creating plans to respond to, recover from, and communicate about incidents. Information gathered from expert consultations and AI stakeholders, established in the GOVERN function and used in the MAP function, helps reduce the chances of system failures and negative impacts. Documentation practices from the GOVERN function, along with those in MAP and MEASURE, support AI risk management and enhance transparency and accountability. There are processes in place to assess new risks and mechanisms for ongoing improvement. \n\nAfter completing the MANAGE function, there will be plans for prioritizing risks and for regular monitoring and improvement. Users of the framework will be better equipped to manage the risks associated with deployed AI systems and allocate resources based on assessed risks. It is important for users to continue applying the MANAGE function as methods, contexts, risks, and expectations from AI stakeholders change over time. \n\nThe NIST AI RMF Playbook describes practices for managing AI risks. Table 4 outlines the categories and subcategories of the MANAGE function.",
      "entities": [
        {
          "name": "MANAGE",
          "type": "function",
          "description": "The MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function."
        },
        {
          "name": "GOVERN",
          "type": "function",
          "description": "The GOVERN function is responsible for defining the parameters for risk management and is utilized in the MANAGE function."
        },
        {
          "name": "MAP",
          "type": "function",
          "description": "The MAP function contributes to the assessment and management of AI risks, providing analytical output that informs the MANAGE function."
        },
        {
          "name": "MEASURE",
          "type": "function",
          "description": "The MEASURE function is involved in assessing AI risks and contributes to the prioritization and management of those risks in the MANAGE function."
        },
        {
          "name": "NIST AI RMF 1.0",
          "type": "document",
          "description": "A framework that describes practices related to managing AI risks, including the MANAGE function's categories and subcategories."
        },
        {
          "name": "AI risks",
          "type": "category",
          "description": "Risks associated with the deployment and operation of AI systems, which are prioritized and managed through the MANAGE function."
        },
        {
          "name": "Framework users",
          "type": "person",
          "description": "Individuals or organizations that utilize the framework to manage AI risks and allocate resources based on assessed risks."
        }
      ],
      "relations": [
        {
          "source": "MANAGE",
          "target": "GOVERN",
          "type": "aligned_with",
          "description": "The MANAGE function is defined by the parameters set in the GOVERN function.",
          "strength": 9
        },
        {
          "source": "MANAGE",
          "target": "MAP",
          "type": "supports",
          "description": "The MAP function provides analytical output that supports the prioritization and management of AI risks in the MANAGE function.",
          "strength": 8
        },
        {
          "source": "MANAGE",
          "target": "MEASURE",
          "type": "supports",
          "description": "The MEASURE function contributes to the assessment of AI risks, which is essential for the MANAGE function.",
          "strength": 8
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "AI risks",
          "type": "describes",
          "description": "The NIST AI RMF 1.0 document describes practices related to managing AI risks.",
          "strength": 7
        },
        {
          "source": "Framework users",
          "target": "MANAGE",
          "type": "operates_under",
          "description": "Framework users apply the MANAGE function to effectively manage AI risks.",
          "strength": 8
        }
      ],
      "chunk_time": 18.847131167000043,
      "chunk_tokens": 396,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 50,
      "text": "categories for the MANAGE function.\nMANAGE 1: AI\nrisks based on\nassessments and\nother analytical\noutput from the\nMAP and MEASURE\nfunctions are\nprioritized,\nresponded to, and\nmanaged.\nMANAGE 1.1: A determination is made as to whether the AI\nsystem achieves its intended purposes and stated objectives and\nwhether its development or deployment should proceed.\nMANAGE 1.2: Treatment of documented AI risks is prioritized\nbased on impact, likelihood, and available resources or methods.\nMANAGE 1.3: Responses to the AI risks deemed high priority, as\nidentified by the MAP function, are developed, planned, and doc-\numented. Risk response options can include mitigating, transfer-\nring, avoiding, or accepting.\nMANAGE 1.4: Negative residual risks (defined as the sum of all\nunmitigated risks) to both downstream acquirers of AI systems\nand end users are documented.\nMANAGE 2:\nStrategies to\nmaximize AI\nbenefits and\nminimize negative\nimpacts are planned,\nprepared,\nimplemented,\ndocumented, and\ninformed by input\nfrom relevant AI\nactors.\nMANAGE 2.1: Resources required to manage AI risks are taken\ninto account \u2013 along with viable non-AI alternative systems, ap-\nproaches, or methods \u2013 to reduce the magnitude or likelihood of\npotential impacts.\nMANAGE 2.2: Mechanisms are in place and applied to sustain\nthe value of deployed AI systems.\nMANAGE 2.3: Procedures are followed to respond to and recover\nfrom a previously unknown risk when it is identified.\nMANAGE 2.4: Mechanisms are in place and applied, and respon-\nsibilities are assigned and understood, to supersede, disengage, or\ndeactivate AI systems that demonstrate performance or outcomes\ninconsistent with intended use.\n",
      "rephrase": "The MANAGE function has two main categories for handling AI risks and benefits. \n\n**MANAGE 1: AI Risks**  \n1. **Assessing AI Risks**: Risks identified through assessments from the MAP and MEASURE functions are prioritized and managed.  \n   - **1.1**: Determine if the AI system meets its goals and if it should continue development or deployment.  \n   - **1.2**: Prioritize treatment of documented AI risks based on their impact, likelihood, and available resources.  \n   - **1.3**: Develop, plan, and document responses for high-priority AI risks identified by the MAP function. Options include mitigating, transferring, avoiding, or accepting the risks.  \n   - **1.4**: Document any negative residual risks that remain for both downstream users of AI systems and end users.  \n\n**MANAGE 2: Maximizing AI Benefits**  \n2. **Planning and Implementing Strategies**: Strategies are created to maximize the benefits of AI while minimizing negative impacts, informed by input from relevant stakeholders.  \n   - **2.1**: Consider the resources needed to manage AI risks, along with alternative non-AI methods, to reduce potential impacts.  \n   - **2.2**: Implement mechanisms to maintain the value of deployed AI systems.  \n   - **2.3**: Follow procedures to respond to and recover from any previously unknown risks once they are identified.  \n   - **2.4**: Establish mechanisms and assign responsibilities to deactivate AI systems that perform poorly or yield unintended results.",
      "entities": [
        {
          "name": "MANAGE function",
          "type": "function",
          "description": "A function that encompasses the management of AI risks based on assessments and analytical outputs from other functions."
        },
        {
          "name": "AI risks",
          "type": "risk",
          "description": "Risks associated with the development and deployment of AI systems, which are prioritized and managed."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function that identifies high-priority AI risks that need to be addressed."
        },
        {
          "name": "MEASURE function",
          "type": "function",
          "description": "A function that provides analytical outputs used in the assessment of AI risks."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Systems that utilize artificial intelligence to perform tasks and achieve objectives."
        },
        {
          "name": "AI actors",
          "type": "organization",
          "description": "Relevant stakeholders involved in the development and deployment of AI systems."
        },
        {
          "name": "negative residual risks",
          "type": "risk",
          "description": "The sum of all unmitigated risks that affect downstream acquirers of AI systems and end users."
        },
        {
          "name": "non-AI alternative systems",
          "type": "technology",
          "description": "Systems or approaches that do not utilize artificial intelligence but can serve as alternatives."
        }
      ],
      "relations": [
        {
          "source": "MANAGE function",
          "target": "AI risks",
          "type": "manages",
          "description": "The MANAGE function is responsible for managing AI risks based on assessments.",
          "strength": 9
        },
        {
          "source": "AI risks",
          "target": "MAP function",
          "type": "identified_by",
          "description": "The MAP function identifies high-priority AI risks that need to be addressed.",
          "strength": 8
        },
        {
          "source": "MEASURE function",
          "target": "AI risks",
          "type": "provides",
          "description": "The MEASURE function provides analytical outputs that inform the assessment of AI risks.",
          "strength": 7
        },
        {
          "source": "AI systems",
          "target": "negative residual risks",
          "type": "affects",
          "description": "AI systems can contribute to negative residual risks for downstream acquirers and end users.",
          "strength": 8
        },
        {
          "source": "AI actors",
          "target": "MANAGE function",
          "type": "informed_by",
          "description": "The planning and implementation of the MANAGE function are informed by input from relevant AI actors.",
          "strength": 8
        },
        {
          "source": "AI risks",
          "target": "non-AI alternative systems",
          "type": "considered_with",
          "description": "Resources required to manage AI risks are considered alongside viable non-AI alternative systems.",
          "strength": 7
        }
      ],
      "chunk_time": 18.358565000002272,
      "chunk_tokens": 398,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 51,
      "text": "AGE 2.4: Mechanisms are in place and applied, and respon-\nsibilities are assigned and understood, to supersede, disengage, or\ndeactivate AI systems that demonstrate performance or outcomes\ninconsistent with intended use.\nMANAGE 3: AI\nrisks and benefits\nfrom third-party\nentities are\nmanaged.\nMANAGE 3.1: AI risks and benefits from third-party resources\nare regularly monitored, and risk controls are applied and\ndocumented.\nMANAGE 3.2: Pre-trained models which are used for develop-\nment are monitored as part of AI system regular monitoring and\nmaintenance.\nCategories\nSubcategories\nContinued on next page\nPage 32\n\n\nNIST AI 100-1\nAI RMF 1.0\nTable 4: Categories and subcategories for the MANAGE function. (Continued)\nMANAGE 4: Risk\ntreatments,\nincluding response\nand recovery, and\ncommunication\nplans for the\nidentified and\nmeasured AI risks\nare documented and\nmonitored regularly.\nMANAGE 4.1: Post-deployment AI system monitoring plans\nare implemented, including mechanisms for capturing and eval-\nuating input from users and other relevant AI actors, appeal\nand override, decommissioning, incident response, recovery, and\nchange management.\nMANAGE 4.2: Measurable activities for continual improvements\nare integrated into AI system updates and include regular engage-\nment with interested parties, including relevant AI actors.\nMANAGE 4.3: Incidents and errors are communicated to relevant\nAI actors, including affected communities. Processes for track-\ning, responding to, and recovering from incidents and errors are\nfollowed and documented.\nCategories\nSubcategories\n6.\n",
      "rephrase": "AGE 2.4: There are established procedures to deactivate or disengage AI systems that do not perform as intended, and everyone understands their responsibilities in this process. \n\nMANAGE 3: The risks and benefits of AI from third-party sources are actively managed. \n\nMANAGE 3.1: The risks and benefits from third-party AI resources are regularly monitored, and appropriate risk controls are documented. \n\nMANAGE 3.2: Pre-trained models used in development are included in the regular monitoring and maintenance of the AI system. \n\nMANAGE 4: Plans for addressing risks, including response, recovery, and communication strategies for identified AI risks, are documented and regularly reviewed. \n\nMANAGE 4.1: Monitoring plans for AI systems after deployment are in place, which include ways to gather and assess feedback from users and other stakeholders, as well as procedures for appeals, overrides, decommissioning, incident response, recovery, and change management. \n\nMANAGE 4.2: Activities aimed at continuous improvement are incorporated into AI system updates and involve regular communication with stakeholders. \n\nMANAGE 4.3: Incidents and errors are reported to relevant stakeholders, including affected communities. There are established processes for tracking, responding to, and recovering from these incidents, which are documented.",
      "entities": [
        {
          "name": "AGE 2.4",
          "type": "framework",
          "description": "A framework that outlines mechanisms for managing AI systems that demonstrate inconsistent performance or outcomes."
        },
        {
          "name": "MANAGE 3",
          "type": "function",
          "description": "A function focused on managing AI risks and benefits from third-party entities."
        },
        {
          "name": "MANAGE 3.1",
          "type": "subdivision",
          "description": "A subcategory that involves regular monitoring of AI risks and benefits from third-party resources."
        },
        {
          "name": "MANAGE 3.2",
          "type": "subdivision",
          "description": "A subcategory that focuses on monitoring pre-trained models used in AI system development."
        },
        {
          "name": "MANAGE 4",
          "type": "function",
          "description": "A function that documents and monitors risk treatments, including response and recovery plans for identified AI risks."
        },
        {
          "name": "MANAGE 4.1",
          "type": "subdivision",
          "description": "A subcategory that implements post-deployment monitoring plans for AI systems."
        },
        {
          "name": "MANAGE 4.2",
          "type": "subdivision",
          "description": "A subcategory that integrates measurable activities for continual improvements into AI system updates."
        },
        {
          "name": "MANAGE 4.3",
          "type": "subdivision",
          "description": "A subcategory that focuses on communication processes regarding incidents and errors to relevant AI actors."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that includes categories and subcategories for managing AI risks and benefits."
        },
        {
          "name": "AI RMF 1.0",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "MANAGE 3",
          "type": "includes",
          "description": "NIST AI 100-1 includes the MANAGE 3 function for managing AI risks and benefits.",
          "strength": 8
        },
        {
          "source": "NIST AI 100-1",
          "target": "MANAGE 4",
          "type": "includes",
          "description": "NIST AI 100-1 includes the MANAGE 4 function for documenting and monitoring risk treatments.",
          "strength": 8
        },
        {
          "source": "MANAGE 3",
          "target": "MANAGE 3.1",
          "type": "subdivided_into",
          "description": "MANAGE 3 is subdivided into MANAGE 3.1 for monitoring AI risks and benefits.",
          "strength": 7
        },
        {
          "source": "MANAGE 3",
          "target": "MANAGE 3.2",
          "type": "subdivided_into",
          "description": "MANAGE 3 is subdivided into MANAGE 3.2 for monitoring pre-trained models.",
          "strength": 7
        },
        {
          "source": "MANAGE 4",
          "target": "MANAGE 4.1",
          "type": "subdivided_into",
          "description": "MANAGE 4 is subdivided into MANAGE 4.1 for post-deployment monitoring plans.",
          "strength": 7
        },
        {
          "source": "MANAGE 4",
          "target": "MANAGE 4.2",
          "type": "subdivided_into",
          "description": "MANAGE 4 is subdivided into MANAGE 4.2 for continual improvements in AI system updates.",
          "strength": 7
        },
        {
          "source": "MANAGE 4",
          "target": "MANAGE 4.3",
          "type": "subdivided_into",
          "description": "MANAGE 4 is subdivided into MANAGE 4.3 for communication regarding incidents and errors.",
          "strength": 7
        }
      ],
      "chunk_time": 22.520811957998376,
      "chunk_tokens": 362,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 52,
      "text": " 4.3: Incidents and errors are communicated to relevant\nAI actors, including affected communities. Processes for track-\ning, responding to, and recovering from incidents and errors are\nfollowed and documented.\nCategories\nSubcategories\n6.\nAI RMF Profiles\nAI RMF use-case profiles are implementations of the AI RMF functions, categories, and\nsubcategories for a specific setting or application based on the requirements, risk tolerance,\nand resources of the Framework user: for example, an AI RMF hiring profile or an AI\nRMF fair housing profile. Profiles may illustrate and offer insights into how risk can be\nmanaged at various stages of the AI lifecycle or in specific sector, technology, or end-use\napplications. AI RMF profiles assist organizations in deciding how they might best manage\nAI risk that is well-aligned with their goals, considers legal/regulatory requirements and\nbest practices, and reflects risk management priorities.\nAI RMF temporal profiles are descriptions of either the current state or the desired, target\nstate of specific AI risk management activities within a given sector, industry, organization,\nor application context. An AI RMF Current Profile indicates how AI is currently being\nmanaged and the related risks in terms of current outcomes. A Target Profile indicates the\noutcomes needed to achieve the desired or target AI risk management goals.\nComparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk\nmanagement objectives. Action plans can be developed to address these gaps to fulfill\noutcomes in a given category or subcategory. Prioritization of gap mitigation is driven by\nthe user\u2019s needs and risk management processes. This risk-based approach also enables\nFramework users to compare their approaches with other approaches and to gauge the\nresources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-\neffective, prioritized manner.\nPage 33\n\n\nNIST AI 100-1\nAI RMF 1.",
      "rephrase": "4.3: Incidents and errors are reported to the relevant AI stakeholders, including the communities affected. There are established processes for tracking, responding to, and recovering from these incidents and errors, and these processes are documented.\n\nCategories and Subcategories:\n\n6. AI RMF Profiles\nAI RMF profiles are specific applications of the AI Risk Management Framework (RMF) functions, categories, and subcategories tailored to particular settings or applications. These profiles are based on the user's requirements, risk tolerance, and available resources. For example, there could be an AI RMF profile for hiring or for fair housing. These profiles help organizations understand how to manage AI risks at different stages of the AI lifecycle or within specific sectors, technologies, or applications. They guide organizations in aligning their AI risk management with their goals, legal requirements, best practices, and risk management priorities.\n\nAI RMF temporal profiles describe either the current state or the desired state of AI risk management activities in a specific sector, industry, organization, or application. A Current Profile shows how AI is currently managed and the associated risks based on current outcomes. A Target Profile outlines the outcomes needed to achieve the desired AI risk management goals. By comparing the Current and Target Profiles, organizations can identify gaps that need to be addressed to meet their AI risk management objectives. Action plans can then be created to close these gaps, with prioritization based on the user's needs and risk management processes. This risk-based approach allows users to compare their methods with others and assess the resources required (like staffing and funding) to achieve their AI risk management goals effectively and efficiently.",
      "entities": [
        {
          "name": "AI RMF Profiles",
          "type": "framework",
          "description": "AI RMF use-case profiles are implementations of the AI RMF functions, categories, and subcategories for specific settings or applications based on requirements, risk tolerance, and resources."
        },
        {
          "name": "AI RMF temporal profiles",
          "type": "framework",
          "description": "Descriptions of either the current state or the desired, target state of specific AI risk management activities within a given sector, industry, organization, or application context."
        },
        {
          "name": "Current Profile",
          "type": "document",
          "description": "Indicates how AI is currently being managed and the related risks in terms of current outcomes."
        },
        {
          "name": "Target Profile",
          "type": "document",
          "description": "Indicates the outcomes needed to achieve the desired or target AI risk management goals."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document related to the AI Risk Management Framework."
        }
      ],
      "relations": [
        {
          "source": "AI RMF Profiles",
          "target": "AI RMF temporal profiles",
          "type": "composed_of",
          "description": "AI RMF Profiles include AI RMF temporal profiles as part of their framework.",
          "strength": 7
        },
        {
          "source": "Current Profile",
          "target": "Target Profile",
          "type": "compares",
          "description": "Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives.",
          "strength": 8
        },
        {
          "source": "AI RMF Profiles",
          "target": "NIST AI 100-1",
          "type": "published_by",
          "description": "AI RMF Profiles are associated with the NIST AI 100-1 document.",
          "strength": 6
        }
      ],
      "chunk_time": 10.928388083004393,
      "chunk_tokens": 397,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 53,
      "text": " approaches and to gauge the\nresources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-\neffective, prioritized manner.\nPage 33\n\n\nNIST AI 100-1\nAI RMF 1.0\nAI RMF cross-sectoral profiles cover risks of models or applications that can be used across\nuse cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure,\nand manage risks for activities or business processes common across sectors such as the\nuse of large language models, cloud-based services or acquisition.\nThis Framework does not prescribe profile templates, allowing for flexibility in implemen-\ntation.\nPage 34\n\n\nNIST AI 100-1\nAI RMF 1.0\nAppendix A:\nDescriptions of AI Actor Tasks from Figures 2 and 3\nAI Design tasks are performed during the Application Context and Data and Input phases\nof the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI\nsystems and are responsible for the planning, design, and data collection and processing\ntasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include ar-\nticulating and documenting the system\u2019s concept and objectives, underlying assumptions,\ncontext, and requirements; gathering and cleaning data; and documenting the metadata\nand characteristics of the dataset. AI actors in this category include data scientists, do-\nmain experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion,\nand accessibility, members of impacted communities, human factors experts (e.g., UX/UI\ndesign), governance experts, data engineers, data providers, system funders, product man-\nagers, third-party entities, evaluators, and legal and privacy governance.\nAI Development tasks are performed during the AI Model phase of the lifecycle in Figure\n2.",
      "rephrase": "This document outlines approaches and resources needed, such as staffing and funding, to effectively manage AI risks in a prioritized way.\n\nThe NIST AI 100-1 AI Risk Management Framework (AI RMF) 1.0 includes cross-sectoral profiles that address risks associated with AI models or applications used in various sectors. These profiles also provide guidance on how to govern, map, measure, and manage risks for common activities across sectors, such as using large language models, cloud services, or acquisitions. The framework allows flexibility in implementation by not prescribing specific profile templates.\n\nAppendix A describes the tasks of AI actors involved in the AI lifecycle. During the Application Context and Data and Input phases, AI Design tasks are carried out. AI Design actors are responsible for creating the concept and objectives of AI systems, planning, designing, and collecting and processing data to ensure the AI system is lawful and suitable for its purpose. Their tasks include defining and documenting the system's concept, assumptions, context, and requirements, as well as gathering and cleaning data and documenting the dataset's metadata and characteristics. AI actors in this category include data scientists, domain experts, socio-cultural analysts, diversity and inclusion experts, community members, human factors experts, governance experts, data engineers, data providers, system funders, product managers, third-party entities, evaluators, and legal and privacy governance experts. AI Development tasks occur during the AI Model phase of the lifecycle.",
      "entities": [],
      "relations": [],
      "chunk_time": 21.06186725000589,
      "chunk_tokens": 379,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 54,
      "text": "), governance experts, data engineers, data providers, system funders, product man-\nagers, third-party entities, evaluators, and legal and privacy governance.\nAI Development tasks are performed during the AI Model phase of the lifecycle in Figure\n2. AI Development actors provide the initial infrastructure of AI systems and are responsi-\nble for model building and interpretation tasks, which involve the creation, selection, cali-\nbration, training, and/or testing of models or algorithms. AI actors in this category include\nmachine learning experts, data scientists, developers, third-party entities, legal and privacy\ngovernance experts, and experts in the socio-cultural and contextual factors associated with\nthe deployment setting.\nAI Deployment tasks are performed during the Task and Output phase of the lifecycle in\nFigure 2. AI Deployment actors are responsible for contextual decisions relating to how\nthe AI system is used to assure deployment of the system into production. Related tasks\ninclude piloting the system, checking compatibility with legacy systems, ensuring regu-\nlatory compliance, managing organizational change, and evaluating user experience. AI\nactors in this category include system integrators, software developers, end users, oper-\nators and practitioners, evaluators, and domain experts with expertise in human factors,\nsocio-cultural analysis, and governance.\nOperation and Monitoring tasks are performed in the Application Context/Operate and\nMonitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are\nresponsible for operating the AI system and working with others to regularly assess system\noutput and impacts. AI actors in this category include system operators, domain experts, AI\ndesigners, users who interpret or incorporate the output of AI systems, product developers,\nevaluators and auditors, compliance experts, organizational management, and members of\nthe research community.\nTest, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout\nthe AI lifecycle.",
      "rephrase": "The AI lifecycle involves several key roles and tasks. Governance experts, data engineers, data providers, system funders, product managers, third-party entities, evaluators, and legal and privacy governance professionals all play important roles. \n\nDuring the AI Model phase, AI Development tasks are carried out. This phase includes building and interpreting AI models, which involves creating, selecting, calibrating, training, and testing algorithms. Key players in this phase are machine learning experts, data scientists, developers, third-party entities, legal and privacy experts, and those knowledgeable about socio-cultural factors related to the deployment setting.\n\nIn the Task and Output phase, AI Deployment tasks take place. Here, actors make decisions about how the AI system will be used to ensure it is effectively deployed. Tasks include piloting the system, checking compatibility with existing systems, ensuring compliance with regulations, managing organizational changes, and evaluating user experiences. Participants in this phase include system integrators, software developers, end users, operators, evaluators, and domain experts focused on human factors and governance.\n\nThe Application Context/Operate and Monitor phase involves Operation and Monitoring tasks. In this phase, actors operate the AI system and regularly assess its output and impacts. This group includes system operators, domain experts, AI designers, users who interpret AI outputs, product developers, evaluators, auditors, compliance experts, organizational management, and researchers.\n\nFinally, Test, Evaluation, Verification, and Validation (TEVV) tasks are conducted throughout the entire AI lifecycle.",
      "entities": [
        {
          "name": "AI Development",
          "type": "function",
          "description": "AI Development encompasses tasks performed during the AI Model phase of the lifecycle, focusing on model building and interpretation."
        },
        {
          "name": "AI Deployment",
          "type": "function",
          "description": "AI Deployment involves tasks performed during the Task and Output phase, ensuring the AI system is effectively used in production."
        },
        {
          "name": "Operation and Monitoring",
          "type": "function",
          "description": "Operation and Monitoring tasks are conducted in the Application Context/Operate and Monitor phase, focusing on assessing system output and impacts."
        },
        {
          "name": "Test, Evaluation, Verification, and Validation (TEVV)",
          "type": "function",
          "description": "TEVV tasks are performed throughout the AI lifecycle to ensure the reliability and effectiveness of AI systems."
        },
        {
          "name": "AI actors",
          "type": "category",
          "description": "AI actors include various roles such as data engineers, product managers, and evaluators involved in different phases of the AI lifecycle."
        },
        {
          "name": "machine learning experts",
          "type": "person",
          "description": "Professionals specializing in machine learning who contribute to the AI Development phase."
        },
        {
          "name": "data scientists",
          "type": "person",
          "description": "Experts who analyze and interpret complex data, playing a key role in AI Development."
        },
        {
          "name": "system integrators",
          "type": "person",
          "description": "Individuals responsible for integrating AI systems into existing infrastructures during the AI Deployment phase."
        },
        {
          "name": "end users",
          "type": "person",
          "description": "Individuals who interact with the AI system, providing feedback and insights during the AI Deployment phase."
        },
        {
          "name": "system operators",
          "type": "person",
          "description": "Professionals who operate AI systems and monitor their performance in the Operation and Monitoring phase."
        },
        {
          "name": "compliance experts",
          "type": "person",
          "description": "Specialists who ensure that AI systems adhere to regulatory standards during the Operation and Monitoring phase."
        }
      ],
      "relations": [
        {
          "source": "AI Development",
          "target": "AI actors",
          "type": "includes",
          "description": "AI Development includes various AI actors such as machine learning experts and data scientists.",
          "strength": 9
        },
        {
          "source": "AI Deployment",
          "target": "AI actors",
          "type": "includes",
          "description": "AI Deployment includes AI actors like system integrators and end users who are crucial for effective deployment.",
          "strength": 9
        },
        {
          "source": "Operation and Monitoring",
          "target": "AI actors",
          "type": "includes",
          "description": "Operation and Monitoring includes AI actors such as system operators and compliance experts responsible for system assessment.",
          "strength": 9
        },
        {
          "source": "TEVV",
          "target": "AI actors",
          "type": "includes",
          "description": "TEVV tasks involve AI actors who ensure the reliability of AI systems throughout their lifecycle.",
          "strength": 8
        }
      ],
      "chunk_time": 16.69542304099741,
      "chunk_tokens": 391,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 55,
      "text": " output of AI systems, product developers,\nevaluators and auditors, compliance experts, organizational management, and members of\nthe research community.\nTest, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout\nthe AI lifecycle. They are carried out by AI actors who examine the AI system or its\ncomponents, or detect and remediate problems. Ideally, AI actors carrying out verification\nPage 35\n\n\nNIST AI 100-1\nAI RMF 1.0\nand validation tasks are distinct from those who perform test and evaluation actions. Tasks\ncan be incorporated into a phase as early as design, where tests are planned in accordance\nwith the design requirement.\n\u2022 TEVV tasks for design, planning, and data may center on internal and external vali-\ndation of assumptions for system design, data collection, and measurements relative\nto the intended context of deployment or application.\n\u2022 TEVV tasks for development (i.e., model building) include model validation and\nassessment.\n\u2022 TEVV tasks for deployment include system validation and integration in production,\nwith testing, and recalibration for systems and process integration, user experience,\nand compliance with existing legal, regulatory, and ethical specifications.\n\u2022 TEVV tasks for operations involve ongoing monitoring for periodic updates, testing,\nand subject matter expert (SME) recalibration of models, the tracking of incidents\nor errors reported and their management, the detection of emergent properties and\nrelated impacts, and processes for redress and response.\nHuman Factors tasks and activities are found throughout the dimensions of the AI life-\ncycle. They include human-centered design practices and methodologies, promoting the\nactive involvement of end users and other interested parties and relevant AI actors, incor-\nporating context-specific norms and values in system design, evaluating and adapting end\nuser experiences, and broad integration of humans and human dynamics in all phases of the\nAI lifecycle.",
      "rephrase": "AI systems involve various roles, including product developers, evaluators, auditors, compliance experts, organizational managers, and researchers. Throughout the AI lifecycle, tasks related to Testing, Evaluation, Verification, and Validation (TEVV) are conducted by AI professionals. These tasks help assess the AI system or its components and address any issues that arise. Ideally, those who verify and validate the AI system should be separate from those who conduct tests and evaluations. \n\nTEVV tasks can begin as early as the design phase, where tests are planned based on design requirements. \n- In the design phase, TEVV tasks focus on validating assumptions about system design, data collection, and measurements relevant to the intended use. \n- During development, tasks include validating and assessing the AI model. \n- In the deployment phase, tasks involve validating the system, integrating it into production, testing, and ensuring compliance with legal and ethical standards. \n- For operations, ongoing monitoring is essential, which includes regular updates, testing, managing reported errors, detecting new issues, and establishing processes for addressing problems. \n\nHuman Factors tasks are integrated throughout the AI lifecycle. These tasks emphasize human-centered design, ensuring that end users and stakeholders are actively involved, incorporating relevant norms and values into system design, and continuously evaluating and improving user experiences.",
      "entities": [
        {
          "name": "AI systems",
          "type": "system",
          "description": "Artificial Intelligence systems that undergo various tasks throughout their lifecycle, including testing, evaluation, verification, and validation."
        },
        {
          "name": "Test, Evaluation, Verification, and Validation (TEVV)",
          "type": "framework",
          "description": "A framework encompassing tasks performed throughout the AI lifecycle to ensure the reliability and compliance of AI systems."
        },
        {
          "name": "AI actors",
          "type": "person",
          "description": "Individuals or groups involved in the AI lifecycle, responsible for carrying out TEVV tasks and ensuring the integrity of AI systems."
        },
        {
          "name": "Human Factors",
          "type": "category",
          "description": "Tasks and activities that focus on human-centered design and the integration of human dynamics in the AI lifecycle."
        },
        {
          "name": "organizational management",
          "type": "organization",
          "description": "Entities responsible for overseeing and managing the implementation and operation of AI systems within organizations."
        },
        {
          "name": "compliance experts",
          "type": "person",
          "description": "Professionals who ensure that AI systems adhere to legal, regulatory, and ethical standards."
        },
        {
          "name": "research community",
          "type": "organization",
          "description": "A collective of researchers and scholars engaged in the study and development of AI technologies."
        }
      ],
      "relations": [
        {
          "source": "AI actors",
          "target": "Test, Evaluation, Verification, and Validation (TEVV)",
          "type": "supports",
          "description": "AI actors support the TEVV framework by performing necessary tasks throughout the AI lifecycle.",
          "strength": 9
        },
        {
          "source": "Test, Evaluation, Verification, and Validation (TEVV)",
          "target": "AI systems",
          "type": "applies_to",
          "description": "The TEVV framework applies to AI systems to ensure their reliability and compliance.",
          "strength": 10
        },
        {
          "source": "Human Factors",
          "target": "AI lifecycle",
          "type": "infused_throughout",
          "description": "Human Factors are integrated throughout the AI lifecycle to enhance user experience and system design.",
          "strength": 8
        },
        {
          "source": "compliance experts",
          "target": "AI systems",
          "type": "evaluates",
          "description": "Compliance experts evaluate AI systems to ensure they meet legal and ethical standards.",
          "strength": 9
        },
        {
          "source": "organizational management",
          "target": "AI systems",
          "type": "manages",
          "description": "Organizational management oversees the implementation and operation of AI systems within organizations.",
          "strength": 8
        }
      ],
      "chunk_time": 13.149261334001494,
      "chunk_tokens": 386,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 56,
      "text": " of end users and other interested parties and relevant AI actors, incor-\nporating context-specific norms and values in system design, evaluating and adapting end\nuser experiences, and broad integration of humans and human dynamics in all phases of the\nAI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives\nto understand context of use, inform interdisciplinary and demographic diversity, engage\nin consultative processes, design and evaluate user experience, perform human-centered\nevaluation and testing, and inform impact assessments.\nDomain Expert tasks involve input from multidisciplinary practitioners or scholars who\nprovide knowledge or expertise in \u2013 and about \u2013 an industry sector, economic sector, con-\ntext, or application area where an AI system is being used. AI actors who are domain\nexperts can provide essential guidance for AI system design and development, and inter-\npret outputs in support of work performed by TEVV and AI impact assessment teams.\nAI Impact Assessment tasks include assessing and evaluating requirements for AI system\naccountability, combating harmful bias, examining impacts of AI systems, product safety,\nliability, and security, among others. AI actors such as impact assessors and evaluators\nprovide technical, human factor, socio-cultural, and legal expertise.\nProcurement tasks are conducted by AI actors with financial, legal, or policy management\nauthority for acquisition of AI models, products, or services from a third-party developer,\nvendor, or contractor.\nGovernance and Oversight tasks are assumed by AI actors with management, fiduciary,\nand legal authority and responsibility for the organization in which an AI system is de-\nPage 36\n\n\nNIST AI 100-1\nAI RMF 1.0\nsigned, developed, and/or deployed. Key AI actors responsible for AI governance include\norganizational management, senior leadership, and the Board of Directors. These actors\nare parties that are concerned with the impact and sustainability of the organization as a\nwhole.\n",
      "rephrase": "End users, interested parties, and relevant AI professionals should incorporate specific norms and values into the design of AI systems. This includes evaluating and adapting user experiences and ensuring that human dynamics are considered throughout the AI lifecycle. Human factors experts bring diverse skills to understand how AI will be used, promote diversity, engage in consultations, design and assess user experiences, conduct human-centered evaluations, and inform impact assessments.\n\nDomain Experts are professionals or scholars from various fields who provide knowledge about the specific industry or context where an AI system is applied. Their insights are crucial for guiding the design and development of AI systems and interpreting results for teams involved in testing and assessing AI impacts.\n\nAI Impact Assessment involves evaluating the accountability of AI systems, addressing harmful biases, and examining their effects on safety, liability, and security. Professionals in this area offer expertise in technical, human factors, socio-cultural, and legal aspects.\n\nProcurement tasks are handled by AI professionals with the authority to acquire AI models, products, or services from external developers or vendors.\n\nGovernance and Oversight responsibilities fall to AI leaders with management and legal authority within the organization. Key figures in AI governance include organizational management, senior leadership, and the Board of Directors, who focus on the overall impact and sustainability of the organization.",
      "entities": [
        {
          "name": "Human factors professionals",
          "type": "person",
          "description": "Professionals who provide multidisciplinary skills and perspectives to understand the context of use and engage in consultative processes related to AI system design."
        },
        {
          "name": "Domain Expert",
          "type": "person",
          "description": "Practitioners or scholars who provide knowledge or expertise in a specific industry or application area where an AI system is used."
        },
        {
          "name": "AI Impact Assessment",
          "type": "function",
          "description": "Tasks that involve assessing and evaluating requirements for AI system accountability and examining the impacts of AI systems."
        },
        {
          "name": "Procurement tasks",
          "type": "function",
          "description": "Tasks conducted by AI actors with authority for the acquisition of AI models, products, or services from third-party developers."
        },
        {
          "name": "Governance and Oversight tasks",
          "type": "function",
          "description": "Tasks assumed by AI actors with management and legal authority responsible for the organization in which an AI system is designed and deployed."
        },
        {
          "name": "NIST AI RMF 1.0",
          "type": "document",
          "description": "A framework document published by NIST that outlines risk management practices for AI systems."
        },
        {
          "name": "AI actors",
          "type": "organization",
          "description": "Individuals or groups involved in the development, assessment, and governance of AI systems."
        }
      ],
      "relations": [
        {
          "source": "Human factors professionals",
          "target": "AI actors",
          "type": "supports",
          "description": "Human factors professionals provide support to AI actors by informing user experience design and evaluation.",
          "strength": 8
        },
        {
          "source": "Domain Expert",
          "target": "AI Impact Assessment",
          "type": "contributes_to",
          "description": "Domain experts contribute essential guidance for AI impact assessment tasks.",
          "strength": 7
        },
        {
          "source": "AI Impact Assessment",
          "target": "AI actors",
          "type": "includes",
          "description": "AI actors such as impact assessors and evaluators are included in AI impact assessment tasks.",
          "strength": 9
        },
        {
          "source": "Procurement tasks",
          "target": "AI actors",
          "type": "managed_by",
          "description": "Procurement tasks are managed by AI actors with financial, legal, or policy authority.",
          "strength": 8
        },
        {
          "source": "Governance and Oversight tasks",
          "target": "AI actors",
          "type": "operates_under",
          "description": "Governance and oversight tasks operate under the authority of AI actors responsible for organizational management.",
          "strength": 9
        },
        {
          "source": "NIST AI RMF 1.0",
          "target": "AI actors",
          "type": "published_by",
          "description": "NIST AI RMF 1.0 is a document published by NIST that serves as a guideline for AI actors.",
          "strength": 10
        }
      ],
      "chunk_time": 15.717356959001336,
      "chunk_tokens": 387,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 57,
      "text": ", developed, and/or deployed. Key AI actors responsible for AI governance include\norganizational management, senior leadership, and the Board of Directors. These actors\nare parties that are concerned with the impact and sustainability of the organization as a\nwhole.\nAdditional AI Actors\nThird-party entities include providers, developers, vendors, and evaluators of data, al-\ngorithms, models, and/or systems and related services for another organization or the or-\nganization\u2019s customers or clients. Third-party entities are responsible for AI design and\ndevelopment tasks, in whole or in part. By definition, they are external to the design, devel-\nopment, or deployment team of the organization that acquires its technologies or services.\nThe technologies acquired from third-party entities may be complex or opaque, and risk\ntolerances may not align with the deploying or operating organization.\nEnd users of an AI system are the individuals or groups that use the system for specific\npurposes. These individuals or groups interact with an AI system in a specific context. End\nusers can range in competency from AI experts to first-time technology end users.\nAffected individuals/communities encompass all individuals, groups, communities, or\norganizations directly or indirectly affected by AI systems or decisions based on the output\nof AI systems. These individuals do not necessarily interact with the deployed system or\napplication.\nOther AI actors may provide formal or quasi-formal norms or guidance for specifying\nand managing AI risks. They can include trade associations, standards developing or-\nganizations, advocacy groups, researchers, environmental groups, and civil society\norganizations.\nThe general public is most likely to directly experience positive and negative impacts of\nAI technologies. They may provide the motivation for actions taken by the AI actors. This\ngroup can include individuals, communities, and consumers associated with the context in\nwhich an AI system is developed or deployed.\nPage 37\n\n\nNIST AI 100-1\nAI RMF 1.",
      "rephrase": "Key players in AI governance include organizational management, senior leadership, and the Board of Directors. They focus on the overall impact and sustainability of the organization. \n\nAdditional AI actors include third-party entities such as providers, developers, vendors, and evaluators of data, algorithms, models, and systems. These external parties are involved in the design and development of AI technologies for other organizations or their clients. The technologies they provide can be complex, and their risk tolerances may differ from those of the organizations using them. \n\nEnd users are individuals or groups who interact with AI systems for specific purposes, ranging from AI experts to first-time users. \n\nAffected individuals and communities are those who are directly or indirectly impacted by AI systems or decisions made based on AI outputs, even if they do not use the systems themselves. \n\nOther AI actors may offer guidance on managing AI risks, including trade associations, standards organizations, advocacy groups, researchers, environmental organizations, and civil society groups. \n\nThe general public is likely to experience both positive and negative effects of AI technologies and can influence the actions of AI actors. This group includes individuals, communities, and consumers related to the context in which AI systems are developed or used.",
      "entities": [
        {
          "name": "AI Actors",
          "type": "category",
          "description": "Key stakeholders involved in AI governance, including organizational management, senior leadership, and the Board of Directors, who are concerned with the impact and sustainability of the organization."
        },
        {
          "name": "Third-party entities",
          "type": "organization",
          "description": "Providers, developers, vendors, and evaluators of data, algorithms, models, and systems that are external to the organization acquiring their technologies or services."
        },
        {
          "name": "End users",
          "type": "person",
          "description": "Individuals or groups that use an AI system for specific purposes, ranging from AI experts to first-time technology users."
        },
        {
          "name": "Affected individuals/communities",
          "type": "person",
          "description": "Individuals, groups, communities, or organizations directly or indirectly affected by AI systems or decisions based on their outputs."
        },
        {
          "name": "Other AI actors",
          "type": "organization",
          "description": "Entities that provide norms or guidance for specifying and managing AI risks, including trade associations, standards organizations, advocacy groups, and civil society organizations."
        },
        {
          "name": "General public",
          "type": "person",
          "description": "Individuals, communities, and consumers who directly experience the impacts of AI technologies and may motivate actions taken by AI actors."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document related to AI Risk Management Framework (AI RMF) published by NIST."
        },
        {
          "name": "AI RMF 1",
          "type": "framework",
          "description": "The first version of the AI Risk Management Framework developed by NIST."
        }
      ],
      "relations": [
        {
          "source": "AI Actors",
          "target": "Third-party entities",
          "type": "supports",
          "description": "AI actors support the involvement of third-party entities in AI design and development tasks.",
          "strength": 7
        },
        {
          "source": "Third-party entities",
          "target": "End users",
          "type": "provides",
          "description": "Third-party entities provide technologies and services that end users utilize in AI systems.",
          "strength": 8
        },
        {
          "source": "Affected individuals/communities",
          "target": "AI systems",
          "type": "impacts",
          "description": "Affected individuals and communities are impacted by the decisions made based on AI systems.",
          "strength": 9
        },
        {
          "source": "Other AI actors",
          "target": "AI risks",
          "type": "provides",
          "description": "Other AI actors provide guidance for managing AI risks.",
          "strength": 8
        },
        {
          "source": "General public",
          "target": "AI technologies",
          "type": "experiences",
          "description": "The general public experiences both positive and negative impacts of AI technologies.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document published by NIST related to the AI RMF.",
          "strength": 10
        }
      ],
      "chunk_time": 16.092731542004913,
      "chunk_tokens": 396,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 58,
      "text": " taken by the AI actors. This\ngroup can include individuals, communities, and consumers associated with the context in\nwhich an AI system is developed or deployed.\nPage 37\n\n\nNIST AI 100-1\nAI RMF 1.0\nAppendix B:\nHow AI Risks Differ from Traditional Software Risks\nAs with traditional software, risks from AI-based technology can be bigger than an en-\nterprise, span organizations, and lead to societal impacts. AI systems also bring a set of\nrisks that are not comprehensively addressed by current risk frameworks and approaches.\nSome AI system features that present risks also can be beneficial. For example, pre-trained\nmodels and transfer learning can advance research and increase accuracy and resilience\nwhen compared to other models and approaches. Identifying contextual factors in the MAP\nfunction will assist AI actors in determining the level of risk and potential management\nefforts.\nCompared to traditional software, AI-specific risks that are new or increased include the\nfollowing:\n\u2022 The data used for building an AI system may not be a true or appropriate representa-\ntion of the context or intended use of the AI system, and the ground truth may either\nnot exist or not be available. Additionally, harmful bias and other data quality issues\ncan affect AI system trustworthiness, which could lead to negative impacts.\n\u2022 AI system dependency and reliance on data for training tasks, combined with in-\ncreased volume and complexity typically associated with such data.\n\u2022 Intentional or unintentional changes during training may fundamentally alter AI sys-\ntem performance.\n\u2022 Datasets used to train AI systems may become detached from their original and in-\ntended context or may become stale or outdated relative to deployment context.\n\u2022 AI system scale and complexity (many systems contain billions or even trillions of\ndecision points) housed within more traditional software applications.\n",
      "rephrase": "AI actors include individuals, communities, and consumers involved in the development or use of AI systems. According to the NIST AI RMF 1.0 document, AI risks differ from traditional software risks in several ways. While both can have significant impacts on organizations and society, AI systems introduce unique risks that current risk management frameworks do not fully address. Some features of AI, like pre-trained models and transfer learning, can improve research and accuracy, but they also come with risks. Understanding the context in which AI operates helps identify these risks and manage them effectively.\n\nNew or heightened risks specific to AI compared to traditional software include:\n- The data used to create AI systems may not accurately represent the intended context, and there may be issues with bias and data quality that affect trustworthiness.\n- AI systems rely heavily on data for training, which can be complex and voluminous.\n- Changes made during the training process can significantly impact how the AI system performs.\n- Training datasets may lose their relevance or become outdated compared to the context in which the AI is deployed.\n- The scale and complexity of AI systems can be much greater, with some containing billions or trillions of decision points, compared to traditional software.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document that outlines the AI Risk Management Framework (AI RMF) and discusses how AI risks differ from traditional software risks."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The first version of the AI Risk Management Framework developed by NIST, focusing on identifying and managing risks associated with AI systems."
        },
        {
          "name": "AI actors",
          "type": "category",
          "description": "Individuals, communities, and consumers involved in the development or deployment of AI systems."
        },
        {
          "name": "AI-based technology",
          "type": "technology",
          "description": "Technologies that utilize artificial intelligence to perform tasks that typically require human intelligence."
        },
        {
          "name": "traditional software",
          "type": "category",
          "description": "Software systems that do not incorporate artificial intelligence and follow conventional programming paradigms."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "A function that assists AI actors in determining the level of risk and potential management efforts related to AI systems."
        },
        {
          "name": "pre-trained models",
          "type": "resource",
          "description": "AI models that have been previously trained on a dataset and can be fine-tuned for specific tasks."
        },
        {
          "name": "transfer learning",
          "type": "methodology",
          "description": "A machine learning technique where a model developed for a particular task is reused as the starting point for a model on a second task."
        },
        {
          "name": "data quality issues",
          "type": "risk",
          "description": "Problems related to the accuracy, completeness, and reliability of data used in AI systems, which can affect their trustworthiness."
        },
        {
          "name": "datasets",
          "type": "resource",
          "description": "Collections of data used to train AI systems, which may become outdated or detached from their intended context."
        }
      ],
      "relations": [
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document that publishes the AI RMF 1.0 framework.",
          "strength": 9
        },
        {
          "source": "AI RMF 1.0",
          "target": "AI actors",
          "type": "applicable_to",
          "description": "The AI RMF 1.0 framework is applicable to AI actors involved in the development and deployment of AI systems.",
          "strength": 8
        },
        {
          "source": "AI-based technology",
          "target": "traditional software",
          "type": "compared_to",
          "description": "AI-based technology presents new risks compared to traditional software.",
          "strength": 7
        },
        {
          "source": "MAP function",
          "target": "AI actors",
          "type": "supports",
          "description": "The MAP function supports AI actors in assessing risks associated with AI systems.",
          "strength": 8
        },
        {
          "source": "pre-trained models",
          "target": "transfer learning",
          "type": "includes",
          "description": "Pre-trained models are often utilized in transfer learning to enhance model performance.",
          "strength": 7
        },
        {
          "source": "data quality issues",
          "target": "AI-based technology",
          "type": "affects",
          "description": "Data quality issues can negatively impact the trustworthiness of AI-based technology.",
          "strength": 9
        },
        {
          "source": "datasets",
          "target": "AI systems",
          "type": "used_to_train",
          "description": "Datasets are used to train AI systems, but may become outdated or detached from their original context.",
          "strength": 8
        }
      ],
      "chunk_time": 21.961350750003476,
      "chunk_tokens": 377,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 59,
      "text": " may become detached from their original and in-\ntended context or may become stale or outdated relative to deployment context.\n\u2022 AI system scale and complexity (many systems contain billions or even trillions of\ndecision points) housed within more traditional software applications.\n\u2022 Use of pre-trained models that can advance research and improve performance can\nalso increase levels of statistical uncertainty and cause issues with bias management,\nscientific validity, and reproducibility.\n\u2022 Higher degree of difficulty in predicting failure modes for emergent properties of\nlarge-scale pre-trained models.\n\u2022 Privacy risk due to enhanced data aggregation capability for AI systems.\n\u2022 AI systems may require more frequent maintenance and triggers for conducting cor-\nrective maintenance due to data, model, or concept drift.\n\u2022 Increased opacity and concerns about reproducibility.\n\u2022 Underdeveloped software testing standards and inability to document AI-based prac-\ntices to the standard expected of traditionally engineered software for all but the\nsimplest of cases.\n\u2022 Difficulty in performing regular AI-based software testing, or determining what to\ntest, since AI systems are not subject to the same controls as traditional code devel-\nopment.\nPage 38\n\n\nNIST AI 100-1\nAI RMF 1.0\n\u2022 Computational costs for developing AI systems and their impact on the environment\nand planet.\n\u2022 Inability to predict or detect the side effects of AI-based systems beyond statistical\nmeasures.\nPrivacy and cybersecurity risk management considerations and approaches are applicable\nin the design, development, deployment, evaluation, and use of AI systems. Privacy and\ncybersecurity risks are also considered as part of broader enterprise risk management con-\nsiderations, which may incorporate AI risks.",
      "rephrase": "AI systems can lose their original purpose or become outdated when used in different contexts. These systems can be very complex, often containing billions or trillions of decision points within traditional software. While pre-trained models can enhance research and performance, they may also introduce statistical uncertainty and issues with bias, scientific validity, and reproducibility. It is challenging to predict failures in large-scale pre-trained models due to their emergent properties. Additionally, AI systems pose privacy risks because they can aggregate data more effectively. They may also need more frequent maintenance due to changes in data, models, or concepts. There are concerns about the lack of transparency and reproducibility in AI systems, as well as underdeveloped software testing standards. Regular testing of AI software is difficult because these systems do not follow the same controls as traditional software development. The costs of developing AI systems can also have environmental impacts, and it is hard to predict or identify side effects beyond statistical measures. Privacy and cybersecurity risks must be managed throughout the design, development, deployment, evaluation, and use of AI systems, and these risks should be integrated into broader enterprise risk management strategies.",
      "entities": [
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document published by NIST that outlines guidelines and frameworks for AI risk management."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework version 1.0 developed by NIST to guide organizations in managing AI-related risks."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Systems that utilize artificial intelligence to perform tasks that typically require human intelligence, such as decision-making and problem-solving."
        },
        {
          "name": "Privacy and cybersecurity risk management",
          "type": "practice",
          "description": "Approaches and considerations for managing privacy and cybersecurity risks in the context of AI system design and deployment."
        },
        {
          "name": "enterprise risk management",
          "type": "framework",
          "description": "A comprehensive approach to identifying, assessing, and managing risks across an organization, including those related to AI."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "AI systems",
          "type": "supports",
          "description": "The AI RMF 1.0 provides guidelines that support the design and deployment of AI systems.",
          "strength": 9
        },
        {
          "source": "Privacy and cybersecurity risk management",
          "target": "AI systems",
          "type": "applicable_to",
          "description": "Privacy and cybersecurity risk management considerations are applicable to the design and use of AI systems.",
          "strength": 8
        },
        {
          "source": "enterprise risk management",
          "target": "AI systems",
          "type": "includes",
          "description": "Enterprise risk management includes considerations for managing risks associated with AI systems.",
          "strength": 7
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "The NIST AI 100-1 document is published by NIST and outlines the AI RMF 1.0 framework.",
          "strength": 10
        }
      ],
      "chunk_time": 11.701461917000415,
      "chunk_tokens": 340,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 60,
      "text": " and approaches are applicable\nin the design, development, deployment, evaluation, and use of AI systems. Privacy and\ncybersecurity risks are also considered as part of broader enterprise risk management con-\nsiderations, which may incorporate AI risks. As part of the effort to address AI trustworthi-\nness characteristics such as \u201cSecure and Resilient\u201d and \u201cPrivacy-Enhanced,\u201d organizations\nmay consider leveraging available standards and guidance that provide broad guidance to\norganizations to reduce security and privacy risks, such as, but not limited to, the NIST Cy-\nbersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Frame-\nwork, and the Secure Software Development Framework. These frameworks have some\nfeatures in common with the AI RMF. Like most risk management approaches, they are\noutcome-based rather than prescriptive and are often structured around a Core set of func-\ntions, categories, and subcategories. While there are significant differences between these\nframeworks based on the domain addressed \u2013 and because AI risk management calls for\naddressing many other types of risks \u2013 frameworks like those mentioned above may inform\nsecurity and privacy considerations in the MAP, MEASURE, and MANAGE functions of the\nAI RMF.\nAt the same time, guidance available before publication of this AI RMF does not compre-\nhensively address many AI system risks. For example, existing frameworks and guidance\nare unable to:\n\u2022 adequately manage the problem of harmful bias in AI systems;\n\u2022 confront the challenging risks related to generative AI;\n\u2022 comprehensively address security concerns related to evasion, model extraction, mem-\nbership inference, availability, or other machine learning attacks;\n\u2022 account for the complex attack surface of AI systems or other security abuses enabled\nby AI systems; and\n\u2022 consider risks associated with third-party AI technologies, transfer learning, and off-\nlabel use where AI systems may be trained for decision-making outside an organiza-\ntion\u2019s security controls",
      "rephrase": "This text discusses the design, development, deployment, evaluation, and use of AI systems, focusing on privacy and cybersecurity risks as part of overall enterprise risk management. Organizations can enhance the trustworthiness of AI by using existing standards and guidelines to minimize security and privacy risks. Key frameworks include the NIST Cybersecurity Framework, NIST Privacy Framework, NIST Risk Management Framework, and Secure Software Development Framework. These frameworks share similarities with the AI Risk Management Framework (AI RMF) and are outcome-based, focusing on core functions and categories. However, they differ in their specific areas of focus, and AI risk management must address additional risks. Current guidance does not fully cover many AI system risks, such as: managing harmful bias in AI, addressing risks from generative AI, tackling security issues like evasion and model extraction, understanding the complex vulnerabilities of AI systems, and considering risks from third-party AI technologies and off-label uses where AI systems are used outside an organization\u2019s security controls.",
      "entities": [
        {
          "name": "NIST Cybersecurity Framework",
          "type": "framework",
          "description": "A framework that provides guidelines for managing cybersecurity risks, focusing on securing and resilient practices."
        },
        {
          "name": "NIST Privacy Framework",
          "type": "framework",
          "description": "A framework designed to help organizations manage privacy risks and enhance privacy protections."
        },
        {
          "name": "NIST Risk Management Framework",
          "type": "framework",
          "description": "A framework that provides a structured process for managing risks associated with information systems."
        },
        {
          "name": "Secure Software Development Framework",
          "type": "framework",
          "description": "A framework that outlines best practices for developing secure software to mitigate security risks."
        },
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "The AI Risk Management Framework that addresses various risks associated with AI systems."
        },
        {
          "name": "MAP, MEASURE, and MANAGE functions",
          "type": "function",
          "description": "Functions within the AI RMF that focus on managing and measuring risks associated with AI systems."
        },
        {
          "name": "AI systems",
          "type": "system",
          "description": "Systems that utilize artificial intelligence technologies for various applications."
        },
        {
          "name": "harmful bias",
          "type": "risk",
          "description": "A risk associated with AI systems that leads to unfair or discriminatory outcomes."
        },
        {
          "name": "generative AI",
          "type": "technology",
          "description": "A type of AI that can generate new content or data based on learned patterns."
        },
        {
          "name": "machine learning attacks",
          "type": "risk",
          "description": "Security threats that exploit vulnerabilities in machine learning models."
        },
        {
          "name": "third-party AI technologies",
          "type": "technology",
          "description": "AI technologies developed by external organizations that may pose additional risks."
        }
      ],
      "relations": [
        {
          "source": "NIST Cybersecurity Framework",
          "target": "AI RMF",
          "type": "aligned_with",
          "description": "The NIST Cybersecurity Framework aligns with the AI RMF in addressing security and privacy considerations.",
          "strength": 8
        },
        {
          "source": "NIST Privacy Framework",
          "target": "AI RMF",
          "type": "aligned_with",
          "description": "The NIST Privacy Framework aligns with the AI RMF to enhance privacy protections in AI systems.",
          "strength": 8
        },
        {
          "source": "NIST Risk Management Framework",
          "target": "AI RMF",
          "type": "aligned_with",
          "description": "The NIST Risk Management Framework provides a structured approach that complements the AI RMF.",
          "strength": 8
        },
        {
          "source": "Secure Software Development Framework",
          "target": "AI RMF",
          "type": "aligned_with",
          "description": "The Secure Software Development Framework shares principles that can inform the AI RMF.",
          "strength": 8
        },
        {
          "source": "AI RMF",
          "target": "MAP, MEASURE, and MANAGE functions",
          "type": "includes",
          "description": "The AI RMF includes the MAP, MEASURE, and MANAGE functions to address AI system risks.",
          "strength": 9
        },
        {
          "source": "AI systems",
          "target": "harmful bias",
          "type": "identifies",
          "description": "AI systems can exhibit harmful bias, which is a significant risk that needs to be managed.",
          "strength": 9
        },
        {
          "source": "AI systems",
          "target": "generative AI",
          "type": "includes",
          "description": "Generative AI is a subset of AI systems that poses unique risks.",
          "strength": 7
        },
        {
          "source": "AI systems",
          "target": "machine learning attacks",
          "type": "identifies",
          "description": "AI systems are vulnerable to various machine learning attacks that can compromise security.",
          "strength": 9
        },
        {
          "source": "AI systems",
          "target": "third-party AI technologies",
          "type": "identifies",
          "description": "AI systems may face risks from third-party AI technologies that are outside an organization's control.",
          "strength": 8
        }
      ],
      "chunk_time": 25.562375042005442,
      "chunk_tokens": 400,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 61,
      "text": " AI systems or other security abuses enabled\nby AI systems; and\n\u2022 consider risks associated with third-party AI technologies, transfer learning, and off-\nlabel use where AI systems may be trained for decision-making outside an organiza-\ntion\u2019s security controls or trained in one domain and then \u201cfine-tuned\u201d for another.\nBoth AI and traditional software technologies and systems are subject to rapid innovation.\nTechnology advances should be monitored and deployed to take advantage of those devel-\nopments and work towards a future of AI that is both trustworthy and responsible.\nPage 39\n\n\nNIST AI 100-1\nAI RMF 1.0\nAppendix C:\nAI Risk Management and Human-AI Interaction\nOrganizations that design, develop, or deploy AI systems for use in operational settings\nmay enhance their AI risk management by understanding current limitations of human-\nAI interaction. The AI RMF provides opportunities to clearly define and differentiate the\nvarious human roles and responsibilities when using, interacting with, or managing AI\nsystems.\nMany of the data-driven approaches that AI systems rely on attempt to convert or represent\nindividual and social observational and decision-making practices into measurable quanti-\nties. Representing complex human phenomena with mathematical models can come at the\ncost of removing necessary context. This loss of context may in turn make it difficult to\nunderstand individual and societal impacts that are key to AI risk management efforts.\nIssues that merit further consideration and research include:\n1. Human roles and responsibilities in decision making and overseeing AI systems\nneed to be clearly defined and differentiated. Human-AI configurations can span\nfrom fully autonomous to fully manual. AI systems can autonomously make deci-\nsions, defer decision making to a human expert, or be used by a human decision\nmaker as an additional opinion. Some AI systems may not require human oversight,\nsuch as models used to improve video compression. Other systems may specifically\nrequire human oversight.\n2.",
      "rephrase": "AI systems can pose security risks, especially when using third-party technologies or when they are trained for purposes outside an organization's security measures. It's important to monitor advancements in both AI and traditional software to ensure they are used responsibly and effectively. \n\nOrganizations that create or use AI systems should improve their risk management by understanding how humans interact with these systems. The AI Risk Management Framework (AI RMF) helps clarify the roles and responsibilities of people involved in using or managing AI. \n\nAI systems often convert complex human behaviors and decisions into measurable data, but this can lead to a loss of important context, making it harder to assess their impact on individuals and society. Key areas for further research include: \n1. Clearly defining human roles in decision-making and oversight of AI systems, which can range from fully automated to fully manual processes. Some AI systems can make decisions on their own, while others may need human input or oversight. For example, AI used for video compression may not need human supervision, while other systems do.",
      "entities": [
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "The AI Risk Management Framework (AI RMF) provides guidelines for organizations to enhance their AI risk management by understanding human-AI interaction."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "A document published by NIST that outlines standards and guidelines for AI risk management."
        },
        {
          "name": "AI systems",
          "type": "technology",
          "description": "Technological systems that utilize artificial intelligence to perform tasks, often involving decision-making processes."
        },
        {
          "name": "human roles and responsibilities",
          "type": "category",
          "description": "The defined roles and responsibilities of humans in the context of decision-making and oversight of AI systems."
        },
        {
          "name": "third-party AI technologies",
          "type": "resource",
          "description": "AI technologies developed by external organizations that may pose risks when integrated into an organization's systems."
        },
        {
          "name": "transfer learning",
          "type": "methodology",
          "description": "A machine learning technique where a model developed for one task is reused as the starting point for a model on a second task."
        },
        {
          "name": "off-label use",
          "type": "practice",
          "description": "The use of AI systems for purposes outside their intended or approved applications."
        },
        {
          "name": "video compression models",
          "type": "technology",
          "description": "AI models specifically designed to improve the efficiency of video data compression."
        }
      ],
      "relations": [
        {
          "source": "AI RMF 1.0",
          "target": "AI systems",
          "type": "supports",
          "description": "The AI RMF 1.0 framework supports organizations in managing risks associated with AI systems.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "published_by",
          "description": "NIST AI 100-1 is a document that outlines the AI RMF 1.0 framework.",
          "strength": 10
        },
        {
          "source": "third-party AI technologies",
          "target": "AI systems",
          "type": "risk",
          "description": "Third-party AI technologies may introduce risks when integrated into an organization's AI systems.",
          "strength": 8
        },
        {
          "source": "transfer learning",
          "target": "AI systems",
          "type": "methodology",
          "description": "Transfer learning is a methodology that can be applied to enhance the capabilities of AI systems.",
          "strength": 7
        },
        {
          "source": "human roles and responsibilities",
          "target": "AI systems",
          "type": "evaluates",
          "description": "Human roles and responsibilities are evaluated to ensure effective oversight of AI systems.",
          "strength": 8
        },
        {
          "source": "video compression models",
          "target": "AI systems",
          "type": "function",
          "description": "Video compression models function as a specific application of AI systems that may not require human oversight.",
          "strength": 6
        }
      ],
      "chunk_time": 14.331804041998112,
      "chunk_tokens": 392,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 62,
      "text": " decision making to a human expert, or be used by a human decision\nmaker as an additional opinion. Some AI systems may not require human oversight,\nsuch as models used to improve video compression. Other systems may specifically\nrequire human oversight.\n2. Decisions that go into the design, development, deployment, evaluation, and use\nof AI systems reflect systemic and human cognitive biases. AI actors bring their\ncognitive biases, both individual and group, into the process. Biases can stem from\nend-user decision-making tasks and be introduced across the AI lifecycle via human\nassumptions, expectations, and decisions during design and modeling tasks. These\nbiases, which are not necessarily always harmful, may be exacerbated by AI system\nopacity and the resulting lack of transparency. Systemic biases at the organizational\nlevel can influence how teams are structured and who controls the decision-making\nprocesses throughout the AI lifecycle. These biases can also influence downstream\ndecisions by end users, decision makers, and policy makers and may lead to negative\nimpacts.\n3. Human-AI interaction results vary. Under certain conditions \u2013 for example, in\nperceptual-based judgment tasks \u2013 the AI part of the human-AI interaction can am-\nplify human biases, leading to more biased decisions than the AI or human alone.\nWhen these variations are judiciously taken into account in organizing human-AI\nteams, however, they can result in complementarity and improved overall perfor-\nmance.\nPage 40\n\n\nNIST AI 100-1\nAI RMF 1.0\n4. Presenting AI system information to humans is complex. Humans perceive and\nderive meaning from AI system output and explanations in different ways, reflecting\ndifferent individual preferences, traits, and skills.\nThe GOVERN function provides organizations with the opportunity to clarify and define\nthe roles and responsibilities for the humans in the Human-AI team configurations and\nthose who are overseeing the AI system performance.",
      "rephrase": "AI systems can either make decisions on their own or provide additional insights for human decision makers. Some AI, like those used for video compression, may not need human oversight, while others do. The design and use of AI systems can be influenced by human biases, which can come from individual or group perspectives. These biases can be introduced at various stages of the AI lifecycle, including design and modeling, and may not always be harmful. However, they can be worsened by a lack of transparency in AI systems. Organizational biases can affect team structures and decision-making processes, potentially leading to negative outcomes for users and policymakers. The results of human-AI interactions can vary; in some cases, AI can amplify human biases, resulting in more biased decisions than either the AI or human would make alone. However, if these variations are managed well, they can lead to better teamwork and improved performance. Understanding how humans interpret AI outputs is complex, as individuals have different preferences and skills. The GOVERN function helps organizations define the roles and responsibilities of humans in AI teams and those overseeing AI performance.",
      "entities": [
        {
          "name": "AI systems",
          "type": "system",
          "description": "AI systems are technologies that can operate independently or with human oversight, used for various applications including decision-making and video compression."
        },
        {
          "name": "human decision maker",
          "type": "person",
          "description": "A human decision maker is an individual who makes decisions, potentially using AI systems as a supplementary opinion."
        },
        {
          "name": "cognitive biases",
          "type": "risk",
          "description": "Cognitive biases are systematic patterns of deviation from norm or rationality in judgment, which can affect decision-making processes in AI systems."
        },
        {
          "name": "GOVERN function",
          "type": "function",
          "description": "The GOVERN function provides organizations with a framework to clarify roles and responsibilities within Human-AI team configurations."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "NIST AI 100-1 is a document that outlines standards and guidelines related to AI systems and their management."
        },
        {
          "name": "AI RMF 1.0",
          "type": "framework",
          "description": "AI RMF 1.0 is a framework designed to guide organizations in managing risks associated with AI systems."
        }
      ],
      "relations": [
        {
          "source": "AI systems",
          "target": "human decision maker",
          "type": "supports",
          "description": "AI systems can support human decision makers by providing additional opinions or insights.",
          "strength": 7
        },
        {
          "source": "cognitive biases",
          "target": "AI systems",
          "type": "influences",
          "description": "Cognitive biases can influence the design and decision-making processes within AI systems.",
          "strength": 8
        },
        {
          "source": "GOVERN function",
          "target": "AI systems",
          "type": "provides",
          "description": "The GOVERN function provides clarity on roles and responsibilities for humans overseeing AI systems.",
          "strength": 9
        },
        {
          "source": "NIST AI 100-1",
          "target": "AI RMF 1.0",
          "type": "aligned_with",
          "description": "NIST AI 100-1 is aligned with the principles outlined in the AI RMF 1.0 framework.",
          "strength": 8
        }
      ],
      "chunk_time": 12.555558375002875,
      "chunk_tokens": 399,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 63,
      "text": " reflecting\ndifferent individual preferences, traits, and skills.\nThe GOVERN function provides organizations with the opportunity to clarify and define\nthe roles and responsibilities for the humans in the Human-AI team configurations and\nthose who are overseeing the AI system performance. The GOVERN function also creates\nmechanisms for organizations to make their decision-making processes more explicit, to\nhelp counter systemic biases.\nThe MAP function suggests opportunities to define and document processes for operator\nand practitioner proficiency with AI system performance and trustworthiness concepts, and\nto define relevant technical standards and certifications. Implementing MAP function cat-\negories and subcategories may help organizations improve their internal competency for\nanalyzing context, identifying procedural and system limitations, exploring and examining\nimpacts of AI-based systems in the real world, and evaluating decision-making processes\nthroughout the AI lifecycle.\nThe GOVERN and MAP functions describe the importance of interdisciplinarity and demo-\ngraphically diverse teams and utilizing feedback from potentially impacted individuals and\ncommunities. AI actors called out in the AI RMF who perform human factors tasks and\nactivities can assist technical teams by anchoring in design and development practices to\nuser intentions and representatives of the broader AI community, and societal values. These\nactors further help to incorporate context-specific norms and values in system design and\nevaluate end user experiences \u2013 in conjunction with AI systems.\nAI risk management approaches for human-AI configurations will be augmented by on-\ngoing research and evaluation. For example, the degree to which humans are empowered\nand incentivized to challenge AI system output requires further studies. Data about the fre-\nquency and rationale with which humans overrule AI system output in deployed systems\nmay be useful to collect and analyze.\nPage 41\n\n\nNIST AI 100-1\nAI RMF 1.",
      "rephrase": "The text discusses two important functions, GOVERN and MAP, that help organizations manage Human-AI teams effectively. The GOVERN function helps clarify roles and responsibilities for people involved in overseeing AI systems, making decision-making processes clearer to reduce biases. The MAP function focuses on defining and documenting how operators can effectively use AI systems and ensure their reliability. By implementing the MAP function, organizations can enhance their ability to analyze situations, identify limitations, and assess the real-world impacts of AI systems throughout their lifecycle. Both functions emphasize the need for diverse teams and feedback from affected communities to ensure AI systems align with user needs and societal values. Additionally, ongoing research is necessary to understand how humans can effectively challenge AI outputs and to gather data on how often and why humans override AI decisions.",
      "entities": [
        {
          "name": "GOVERN function",
          "type": "function",
          "description": "The GOVERN function provides organizations with the opportunity to clarify and define the roles and responsibilities for humans in Human-AI team configurations and those overseeing AI system performance."
        },
        {
          "name": "MAP function",
          "type": "function",
          "description": "The MAP function suggests opportunities to define and document processes for operator and practitioner proficiency with AI system performance and trustworthiness concepts."
        },
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "AI RMF refers to the AI Risk Management Framework that outlines approaches for managing risks associated with AI systems."
        },
        {
          "name": "NIST AI 100-1",
          "type": "document",
          "description": "NIST AI 100-1 is a document that likely contains guidelines or standards related to AI risk management."
        }
      ],
      "relations": [
        {
          "source": "GOVERN function",
          "target": "AI RMF",
          "type": "supports",
          "description": "The GOVERN function supports the AI RMF by clarifying roles and responsibilities in AI team configurations.",
          "strength": 8
        },
        {
          "source": "MAP function",
          "target": "AI RMF",
          "type": "supports",
          "description": "The MAP function supports the AI RMF by suggesting documentation processes for AI system performance and trustworthiness.",
          "strength": 8
        },
        {
          "source": "AI RMF",
          "target": "NIST AI 100-1",
          "type": "published_by",
          "description": "The AI RMF is published by NIST as part of their guidelines on AI risk management.",
          "strength": 9
        }
      ],
      "chunk_time": 10.519149457999447,
      "chunk_tokens": 368,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 64,
      "text": " further studies. Data about the fre-\nquency and rationale with which humans overrule AI system output in deployed systems\nmay be useful to collect and analyze.\nPage 41\n\n\nNIST AI 100-1\nAI RMF 1.0\nAppendix D:\nAttributes of the AI RMF\nNIST described several key attributes of the AI RMF when work on the Framework first\nbegan. These attributes have remained intact and were used to guide the AI RMF\u2019s devel-\nopment. They are provided here as a reference.\nThe AI RMF strives to:\n1. Be risk-based, resource-efficient, pro-innovation, and voluntary.\n2. Be consensus-driven and developed and regularly updated through an open, trans-\nparent process. All stakeholders should have the opportunity to contribute to the AI\nRMF\u2019s development.\n3. Use clear and plain language that is understandable by a broad audience, including\nsenior executives, government officials, non-governmental organization leadership,\nand those who are not AI professionals \u2013 while still of sufficient technical depth to\nbe useful to practitioners. The AI RMF should allow for communication of AI risks\nacross an organization, between organizations, with customers, and to the public at\nlarge.\n4. Provide common language and understanding to manage AI risks. The AI RMF\nshould offer taxonomy, terminology, definitions, metrics, and characterizations for\nAI risk.\n5. Be easily usable and fit well with other aspects of risk management. Use of the\nFramework should be intuitive and readily adaptable as part of an organization\u2019s\nbroader risk management strategy and processes. It should be consistent or aligned\nwith other approaches to managing AI risks.\n6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI\nRMF should be universally applicable to any AI technology and to context-specific\nuse cases.\n7. Be outcome-focused and non-prescriptive.",
      "rephrase": "Further research could be beneficial in understanding how often and why people override the outputs of AI systems in real-world applications. \n\nNIST AI 100-1\nAI RMF 1.0\nAppendix D:\nAttributes of the AI RMF\nWhen the development of the AI RMF began, NIST identified several important characteristics that have guided its creation and remain unchanged. Here are those key attributes:\n\n1. The AI RMF aims to be risk-based, efficient in resource use, supportive of innovation, and voluntary.\n2. It is developed through a consensus-driven process that is open and transparent, allowing all stakeholders to contribute.\n3. The language used in the AI RMF is clear and simple, making it accessible to a wide audience, including senior executives, government officials, and non-experts, while still being detailed enough for professionals in the field. This ensures effective communication of AI risks within organizations and to the public.\n4. The AI RMF provides a common framework for understanding and managing AI risks, including terminology, definitions, metrics, and classifications.\n5. It is designed to be user-friendly and compatible with other risk management practices, making it easy to integrate into an organization\u2019s overall risk strategy.\n6. The AI RMF is applicable across various sectors and technology areas, making it relevant for any AI technology and specific use cases.\n7. It focuses on outcomes and does not prescribe specific methods.",
      "entities": [],
      "relations": [],
      "chunk_time": 24.113276750002115,
      "chunk_tokens": 392,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    },
    {
      "chunk_idx": 65,
      "text": " risks.\n6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI\nRMF should be universally applicable to any AI technology and to context-specific\nuse cases.\n7. Be outcome-focused and non-prescriptive. The Framework should provide a catalog\nof outcomes and approaches rather than prescribe one-size-fits-all requirements.\n8. Take advantage of and foster greater awareness of existing standards, guidelines, best\npractices, methodologies, and tools for managing AI risks \u2013 as well as illustrate the\nneed for additional, improved resources.\n9. Be law- and regulation-agnostic.\nThe Framework should support organizations\u2019\nabilities to operate under applicable domestic and international legal or regulatory\nregimes.\n10. Be a living document. The AI RMF should be readily updated as technology, under-\nstanding, and approaches to AI trustworthiness and uses of AI change and as stake-\nholders learn from implementing AI risk management generally and this framework\nin particular.\nPage 42\n\n\nThis publication is available free of charge from:\nhttps://doi.org/10.6028/NIST.AI.100-1\n",
      "rephrase": "6. The AI Risk Management Framework (AI RMF) should be applicable to various perspectives, sectors, and types of technology, making it suitable for any AI technology and specific use cases. 7. It should focus on outcomes rather than strict rules, offering a variety of outcomes and methods instead of a one-size-fits-all approach. 8. The framework should utilize and promote awareness of existing standards, guidelines, best practices, and tools for managing AI risks, while also highlighting the need for better resources. 9. It should be neutral regarding laws and regulations, allowing organizations to comply with relevant domestic and international legal requirements. 10. The AI RMF should be a dynamic document that can be updated as technology evolves, our understanding of AI trustworthiness improves, and as stakeholders gain insights from implementing AI risk management and this framework.",
      "entities": [
        {
          "name": "AI RMF",
          "type": "framework",
          "description": "The AI RMF (Artificial Intelligence Risk Management Framework) is designed to be universally applicable to any AI technology and context-specific use cases, focusing on outcomes rather than prescriptive requirements."
        },
        {
          "name": "standards, guidelines, best practices, methodologies, and tools",
          "type": "category",
          "description": "A collection of existing resources that help manage AI risks and foster awareness in the field."
        },
        {
          "name": "organizations",
          "type": "organization",
          "description": "Entities that are supported by the AI RMF to operate under applicable domestic and international legal or regulatory regimes."
        },
        {
          "name": "stakeholders",
          "type": "person",
          "description": "Individuals or groups involved in implementing AI risk management and learning from the framework."
        },
        {
          "name": "AI technology",
          "type": "technology",
          "description": "Technological systems and applications that utilize artificial intelligence, which the AI RMF aims to support."
        },
        {
          "name": "legal or regulatory regimes",
          "type": "domain",
          "description": "The legal frameworks and regulations that organizations must navigate when implementing AI technologies."
        },
        {
          "name": "NIST.AI.100-1",
          "type": "document",
          "description": "A publication related to the AI RMF, available for free online."
        }
      ],
      "relations": [
        {
          "source": "AI RMF",
          "target": "organizations",
          "type": "supports",
          "description": "The AI RMF supports organizations' abilities to operate under applicable legal or regulatory regimes.",
          "strength": 8
        },
        {
          "source": "AI RMF",
          "target": "standards, guidelines, best practices, methodologies, and tools",
          "type": "fosters",
          "description": "The AI RMF fosters greater awareness of existing resources for managing AI risks.",
          "strength": 7
        },
        {
          "source": "AI RMF",
          "target": "stakeholders",
          "type": "provides",
          "description": "The AI RMF provides a framework for stakeholders to learn from implementing AI risk management.",
          "strength": 9
        },
        {
          "source": "AI RMF",
          "target": "AI technology",
          "type": "applicable_to",
          "description": "The AI RMF is applicable to any AI technology and context-specific use cases.",
          "strength": 10
        },
        {
          "source": "NIST.AI.100-1",
          "target": "AI RMF",
          "type": "published_by",
          "description": "The document NIST.AI.100-1 is a publication related to the AI RMF.",
          "strength": 8
        }
      ],
      "chunk_time": 14.406484332997934,
      "chunk_tokens": 228,
      "source_file": "/Users/swa/Desktop/AI/fast-graph-extraction/samples/Nist.pdf"
    }
  ]
};
      function toggleChunk(id) {
        const reph = document.getElementById(id + "-rephrase");
        const orig = document.getElementById(id + "-original");
        const toggle = reph.previousElementSibling;
        if (orig.style.display === "block") {
          orig.style.display = "none";
          reph.style.display = "block";
          toggle.textContent = labels.showOriginal;
        } else {
          orig.style.display = "block";
          reph.style.display = "none";
          toggle.textContent = labels.showRephrased;
        }
      }

      function transformRawData(rawData, minDegree = 3) {
        // Count degree for each entity based on relations
        const degree = new Map();
        rawData.consolidated.entities.forEach((entity) => {
          degree.set(entity.name, 0);
        });
        rawData.consolidated.relations.forEach((relation) => {
          degree.set(relation.source, (degree.get(relation.source) || 0) + 1);
          degree.set(relation.target, (degree.get(relation.target) || 0) + 1);
        });

        // Keep only nodes that meet the minimum degree threshold
        const kept = new Set();
        for (const [name, deg] of degree) {
          if (deg >= minDegree) kept.add(name);
        }

        const transformed = {
          nodes: [],
          edges: [],
        };

        rawData.consolidated.entities.forEach((entity) => {
          if (kept.has(entity.name)) {
            transformed.nodes.push({
              id: entity.name,
              label: entity.name,
              description: entity.description || "",
            });
          }
        });

        rawData.consolidated.relations.forEach((relation) => {
          if (kept.has(relation.source) && kept.has(relation.target)) {
            transformed.edges.push({
              source: relation.source,
              target: relation.target,
            });
          }
        });

        return transformed;
      }

      // ─── Graph Parsing ──────────────────────────────────────────────
      function parseGraph(data) {
        const nodes = new Map();
        const adj = new Map();

        for (const n of data.nodes) {
          nodes.set(n.id, {
            id: n.id,
            label: n.label || n.id,
            description: n.description || "",
            layer: -1,
            angle: 0,
            x: 0,
            y: 0,
          });
          adj.set(n.id, new Set());
        }

        const edges = [];
        for (const e of data.edges) {
          if (!nodes.has(e.source) || !nodes.has(e.target)) continue;
          if (e.source === e.target) continue;
          adj.get(e.source).add(e.target);
          adj.get(e.target).add(e.source);
          edges.push({ source: e.source, target: e.target });
        }

        return { nodes, adj, edges };
      }

      // ─── Layer Assignment (BFS from a given or highest-degree node) ──
      function assignLayers(nodes, adj, rootId) {
        // Use specified root, or fall back to highest-degree node
        let root = rootId || null;
        if (!root) {
          let maxDeg = -1;
          for (const [id, neighbors] of adj) {
            if (neighbors.size > maxDeg) {
              maxDeg = neighbors.size;
              root = id;
            }
          }
        }

        // Reset all layers
        for (const [, node] of nodes) node.layer = -1;

        // BFS
        const visited = new Set();
        const queue = [root];
        visited.add(root);
        nodes.get(root).layer = 0;

        while (queue.length > 0) {
          const current = queue.shift();
          const currentLayer = nodes.get(current).layer;
          for (const neighbor of adj.get(current)) {
            if (!visited.has(neighbor)) {
              visited.add(neighbor);
              nodes.get(neighbor).layer = currentLayer + 1;
              queue.push(neighbor);
            }
          }
        }

        // Handle disconnected components
        for (const [id, node] of nodes) {
          if (node.layer === -1) {
            // Find a new root for this component
            node.layer = 0;
            const compQueue = [id];
            visited.add(id);
            while (compQueue.length > 0) {
              const cur = compQueue.shift();
              const curLayer = nodes.get(cur).layer;
              for (const nb of adj.get(cur)) {
                if (!visited.has(nb)) {
                  visited.add(nb);
                  nodes.get(nb).layer = curLayer + 1;
                  compQueue.push(nb);
                }
              }
            }
          }
        }

        // Build layers map
        const layers = new Map();
        for (const [id, node] of nodes) {
          if (!layers.has(node.layer)) layers.set(node.layer, []);
          layers.get(node.layer).push(id);
        }

        return layers;
      }

      // ─── Classify Edges ─────────────────────────────────────────────
      function classifyEdges(edges, nodes) {
        const crossLayer = [];
        const sameLayer = [];

        for (const e of edges) {
          const sLayer = nodes.get(e.source).layer;
          const tLayer = nodes.get(e.target).layer;
          if (sLayer === tLayer) {
            sameLayer.push(e);
          } else {
            crossLayer.push(e);
          }
        }

        return { crossLayer, sameLayer };
      }

      // ─── Crossing Minimization (Barycenter Heuristic) ───────────────
      function minimizeCrossings(layers, nodes, adj, edges) {
        const layerIndices = Array.from(layers.keys()).sort((a, b) => a - b);
        if (layerIndices.length <= 1) return;

        // Helper: get index of a node in its layer
        function positionOf(nodeId) {
          const layer = nodes.get(nodeId).layer;
          const arr = layers.get(layer);
          return arr.indexOf(nodeId);
        }

        // Helper: count crossings between two adjacent layers
        function countCrossings(layerA, layerB) {
          const nodesA = layers.get(layerA);
          const nodesB = layers.get(layerB);
          if (!nodesA || !nodesB) return 0;

          // Collect edges between these two layers
          const edgePairs = [];
          for (const e of edges) {
            const sLayer = nodes.get(e.source).layer;
            const tLayer = nodes.get(e.target).layer;
            let posA, posB;
            if (sLayer === layerA && tLayer === layerB) {
              posA = nodesA.indexOf(e.source);
              posB = nodesB.indexOf(e.target);
            } else if (sLayer === layerB && tLayer === layerA) {
              posA = nodesA.indexOf(e.target);
              posB = nodesB.indexOf(e.source);
            } else {
              continue;
            }
            if (posA >= 0 && posB >= 0) edgePairs.push([posA, posB]);
          }

          // Count inversions
          let crossings = 0;
          for (let i = 0; i < edgePairs.length; i++) {
            for (let j = i + 1; j < edgePairs.length; j++) {
              const [a1, b1] = edgePairs[i];
              const [a2, b2] = edgePairs[j];
              if ((a1 - a2) * (b1 - b2) < 0) crossings++;
            }
          }
          return crossings;
        }

        // Total crossings
        function totalCrossings() {
          let total = 0;
          for (let i = 0; i < layerIndices.length - 1; i++) {
            total += countCrossings(layerIndices[i], layerIndices[i + 1]);
          }
          return total;
        }

        // Barycenter sweep
        function barycenterSweep(fixedLayer, freeLayer) {
          const fixedNodes = layers.get(fixedLayer);
          const freeNodes = layers.get(freeLayer);
          if (!fixedNodes || !freeNodes || freeNodes.length <= 1) return;

          const barycenters = new Map();
          for (const nodeId of freeNodes) {
            const neighbors = adj.get(nodeId);
            let sum = 0;
            let count = 0;
            for (const nb of neighbors) {
              if (nodes.get(nb).layer === fixedLayer) {
                sum += fixedNodes.indexOf(nb);
                count++;
              }
            }
            barycenters.set(nodeId, count > 0 ? sum / count : Infinity);
          }

          // Sort free layer by barycenter
          freeNodes.sort((a, b) => {
            const ba = barycenters.get(a);
            const bb = barycenters.get(b);
            if (ba === bb) return 0;
            if (ba === Infinity) return 1;
            if (bb === Infinity) return -1;
            return ba - bb;
          });
          layers.set(freeLayer, freeNodes);
        }

        // Adjacent swap pass for same-layer edges
        function adjacentSwapPass(layerIdx) {
          const layerNodes = layers.get(layerIdx);
          if (!layerNodes || layerNodes.length <= 1) return;

          let improved = true;
          while (improved) {
            improved = false;
            for (let i = 0; i < layerNodes.length - 1; i++) {
              // Count crossings before swap
              const before = totalCrossings();
              // Swap
              [layerNodes[i], layerNodes[i + 1]] = [
                layerNodes[i + 1],
                layerNodes[i],
              ];
              const after = totalCrossings();
              if (after < before) {
                improved = true;
              } else {
                // Swap back
                [layerNodes[i], layerNodes[i + 1]] = [
                  layerNodes[i + 1],
                  layerNodes[i],
                ];
              }
            }
          }
        }

        // Run sweeps
        const maxIterations = 15;
        let bestCrossings = totalCrossings();
        let bestArrangement = new Map();
        for (const [k, v] of layers) bestArrangement.set(k, [...v]);

        for (let iter = 0; iter < maxIterations; iter++) {
          // Forward sweep (inner to outer)
          for (let i = 0; i < layerIndices.length - 1; i++) {
            barycenterSweep(layerIndices[i], layerIndices[i + 1]);
          }
          // Backward sweep (outer to inner)
          for (let i = layerIndices.length - 1; i > 0; i--) {
            barycenterSweep(layerIndices[i], layerIndices[i - 1]);
          }

          const c = totalCrossings();
          if (c < bestCrossings) {
            bestCrossings = c;
            for (const [k, v] of layers) bestArrangement.set(k, [...v]);
          }

          if (bestCrossings === 0) break;
        }

        // Restore best
        for (const [k, v] of bestArrangement) layers.set(k, v);

        // Adjacent swap refinement
        for (const li of layerIndices) {
          adjacentSwapPass(li);
        }
      }

      // ─── Coordinate Computation ─────────────────────────────────────
      const BASE_RADIUS = 80;
      const RING_SPACING = 110;
      const NODE_RADIUS = 10;

      function computePositions(layers, nodes) {
        for (const [layerIdx, layerNodes] of layers) {
          const radius = BASE_RADIUS + layerIdx * RING_SPACING;
          const n = layerNodes.length;

          // Special case: single node at center (layer 0 with 1 node)
          if (layerIdx === 0 && n === 1) {
            const node = nodes.get(layerNodes[0]);
            node.angle = 0;
            node.x = 0;
            node.y = 0;
            node.radius = 0;
            continue;
          }

          for (let i = 0; i < n; i++) {
            const angle = i * ((2 * Math.PI) / n); // starts at 0, counter-clockwise
            const node = nodes.get(layerNodes[i]);
            node.angle = angle;
            node.x = radius * Math.cos(angle);
            node.y = -radius * Math.sin(angle); // negate y for SVG (y-down) to get counter-clockwise
            node.radius = radius;
          }
        }
      }

      // ─── Layer Colors ───────────────────────────────────────────────
      const LAYER_COLORS = [
        "#4e79a7", // blue
        "#f28e2b", // orange
        "#e15759", // red
        "#76b7b2", // teal
        "#59a14f", // green
        "#edc948", // yellow
        "#b07aa1", // purple
        "#ff9da7", // pink
      ];

      function layerColor(layerIdx) {
        return LAYER_COLORS[layerIdx % LAYER_COLORS.length];
      }

      // ─── Edge Opacity by Distance ───────────────────────────────────
      function setEdgeOpacities(edgesContainer, getPos) {
        const edgeEls = edgesContainer.querySelectorAll(".edge");
        let maxDist = 0;
        const dists = [];

        edgeEls.forEach((el) => {
          const src = getPos(el.dataset.source);
          const tgt = getPos(el.dataset.target);
          const dx = tgt.x - src.x;
          const dy = tgt.y - src.y;
          const dist = Math.sqrt(dx * dx + dy * dy);
          dists.push(dist);
          if (dist > maxDist) maxDist = dist;
        });

        edgeEls.forEach((el, i) => {
          const opacity =
            maxDist > 0 ? Math.max(0.1, 1 - 0.85 * (dists[i] / maxDist)) : 1;
          el.style.opacity = opacity;
          el.dataset.baseOpacity = opacity;
        });
      }

      // ─── SVG Rendering ──────────────────────────────────────────────
      const svgNS = "http://www.w3.org/2000/svg";

      function renderRings(container, layers) {
        container.innerHTML = "";
        const maxLayer = Math.max(...layers.keys());

        for (let i = 0; i <= maxLayer; i++) {
          const layerNodes = layers.get(i);
          // Skip ring for layer 0 if single node at center
          if (i === 0 && layerNodes && layerNodes.length === 1) continue;

          const radius = BASE_RADIUS + i * RING_SPACING;
          const circle = document.createElementNS(svgNS, "circle");
          circle.setAttribute("cx", 0);
          circle.setAttribute("cy", 0);
          circle.setAttribute("r", radius);
          circle.classList.add("ring");
          container.appendChild(circle);
        }
      }

      function renderEdges(container, edges, nodes, sameLayerSet) {
        container.innerHTML = "";

        for (const e of edges) {
          const src = nodes.get(e.source);
          const tgt = nodes.get(e.target);
          const isSameLayer = sameLayerSet.has(e);

          if (isSameLayer) {
            // Quadratic Bezier arc bulging outward
            const path = document.createElementNS(svgNS, "path");
            const r = src.radius;

            let theta1 = src.angle;
            let theta2 = tgt.angle;

            // Compute angular difference (shortest arc)
            let dTheta = theta2 - theta1;
            // Normalize to [-PI, PI]
            while (dTheta > Math.PI) dTheta -= 2 * Math.PI;
            while (dTheta < -Math.PI) dTheta += 2 * Math.PI;

            const thetaMid = theta1 + dTheta / 2;
            const bulge = 0.35 * r * Math.abs(dTheta);

            // Control point outward along the midpoint radius
            const cx = (r + bulge) * Math.cos(thetaMid);
            const cy = -(r + bulge) * Math.sin(thetaMid); // negate for SVG

            const d = `M ${src.x},${src.y} Q ${cx},${cy} ${tgt.x},${tgt.y}`;
            path.setAttribute("d", d);
            path.classList.add("edge", "same-layer");
            path.dataset.source = e.source;
            path.dataset.target = e.target;
            container.appendChild(path);
          } else {
            // Quadratic Bezier with slight outward bulge
            const path = document.createElementNS(svgNS, "path");

            // Midpoint of the chord
            const mx = (src.x + tgt.x) / 2;
            const my = (src.y + tgt.y) / 2;

            // Chord direction and perpendicular
            const dx = tgt.x - src.x;
            const dy = tgt.y - src.y;
            const chordLen = Math.sqrt(dx * dx + dy * dy);

            // Perpendicular (rotated 90°): (-dy, dx)
            // Pick the direction pointing away from center (outward)
            let px = -dy;
            let py = dx;
            // Dot with midpoint vector — if negative, flip to point outward
            if (px * mx + py * my < 0) {
              px = -px;
              py = -py;
            }

            // Normalize and scale: subtle bulge proportional to chord length
            const bulge = 0.12 * chordLen;
            const pLen = Math.sqrt(px * px + py * py);
            const cx = mx + (px / pLen) * bulge;
            const cy = my + (py / pLen) * bulge;

            const d = `M ${src.x},${src.y} Q ${cx},${cy} ${tgt.x},${tgt.y}`;
            path.setAttribute("d", d);
            path.classList.add("edge");
            path.dataset.source = e.source;
            path.dataset.target = e.target;
            container.appendChild(path);
          }
        }
      }

      function renderNodes(container, nodes, adj) {
        container.innerHTML = "";

        for (const [id, node] of nodes) {
          const g = document.createElementNS(svgNS, "g");
          g.classList.add("node-group");
          g.dataset.id = id;

          const circle = document.createElementNS(svgNS, "circle");
          circle.setAttribute("cx", node.x);
          circle.setAttribute("cy", node.y);
          circle.setAttribute("r", NODE_RADIUS);
          circle.setAttribute("fill", layerColor(node.layer));
          circle.classList.add("node-circle");
          circle.dataset.id = id;
          g.appendChild(circle);

          // Label offset: push label outward from center
          const dist = Math.sqrt(node.x * node.x + node.y * node.y);
          let lx, ly;
          if (dist < 1) {
            lx = node.x;
            ly = node.y - NODE_RADIUS - 8;
          } else {
            const offset = NODE_RADIUS + 10;
            lx = node.x + (node.x / dist) * offset;
            ly = node.y + (node.y / dist) * offset;
          }

          const label = document.createElementNS(svgNS, "text");
          label.setAttribute("x", lx);
          label.setAttribute("y", ly);
          label.classList.add("node-label");
          label.textContent = node.label;
          label.dataset.id = id;
          g.appendChild(label);

          container.appendChild(g);
        }
      }

      // ─── Interaction ────────────────────────────────────────────────
      function setupInteraction(svg, viewport, nodes, adj) {
        let scale = 1;
        let translateX = 0;
        let translateY = 0;
        let isPanning = false;
        let startX, startY, startTX, startTY;

        function updateTransform() {
          viewport.setAttribute(
            "transform",
            `translate(${translateX}, ${translateY}) scale(${scale})`,
          );
        }

        // Center initially
        function centerView() {
          const rect = svg.getBoundingClientRect();
          translateX = rect.width / 2;
          translateY = rect.height / 2;
          updateTransform();
        }

        centerView();
        window.addEventListener("resize", centerView);

        // Zoom
        svg.addEventListener(
          "wheel",
          (e) => {
            e.preventDefault();
            const factor = e.deltaY < 0 ? 1.08 : 1 / 1.08;
            const newScale = Math.max(0.15, Math.min(5, scale * factor));

            // Zoom toward mouse position
            const rect = svg.getBoundingClientRect();
            const mx = e.clientX - rect.left;
            const my = e.clientY - rect.top;

            translateX = mx - (mx - translateX) * (newScale / scale);
            translateY = my - (my - translateY) * (newScale / scale);
            scale = newScale;
            updateTransform();
          },
          { passive: false },
        );

        // Pan
        svg.addEventListener("mousedown", (e) => {
          if (e.target.classList.contains("node-circle")) return;
          isPanning = true;
          startX = e.clientX;
          startY = e.clientY;
          startTX = translateX;
          startTY = translateY;
          svg.classList.add("panning");
        });

        window.addEventListener("mousemove", (e) => {
          if (!isPanning) return;
          translateX = startTX + (e.clientX - startX);
          translateY = startTY + (e.clientY - startY);
          updateTransform();
        });

        window.addEventListener("mouseup", () => {
          isPanning = false;
          svg.classList.remove("panning");
        });

        // Hover tooltips & highlighting
        const tooltip = document.getElementById("tooltip");

        svg.addEventListener("mouseover", (e) => {
          const circle = e.target.closest(".node-circle");
          if (!circle) return;

          const id = circle.dataset.id;
          const node = nodes.get(id);
          const neighbors = adj.get(id);
          const degree = neighbors.size;

          // Show tooltip
          tooltip.innerHTML =
            `<div class="tt-label">${node.label}</div>` +
            `<div class="tt-detail">${node.description}</div>`;
          tooltip.style.display = "block";

          // Highlight connected edges, fade others
          const connectedNodes = new Set([id, ...neighbors]);

          document.querySelectorAll(".edge").forEach((el) => {
            if (el.dataset.source === id || el.dataset.target === id) {
              el.classList.add("highlighted");
              el.classList.remove("faded");
              el.style.opacity = 1;
            } else {
              el.classList.add("faded");
              el.classList.remove("highlighted");
              el.style.opacity = 0.12;
            }
          });

          document.querySelectorAll(".node-circle").forEach((el) => {
            if (!connectedNodes.has(el.dataset.id)) {
              el.classList.add("faded");
            }
          });

          document.querySelectorAll(".node-label").forEach((el) => {
            if (!connectedNodes.has(el.dataset.id)) {
              el.classList.add("faded");
            }
          });
        });

        svg.addEventListener("mousemove", (e) => {
          if (tooltip.style.display === "block") {
            tooltip.style.left = e.clientX + 14 + "px";
            tooltip.style.top = e.clientY - 10 + "px";
          }
        });

        svg.addEventListener("mouseout", (e) => {
          const circle = e.target.closest(".node-circle");
          if (!circle) return;

          tooltip.style.display = "none";

          document.querySelectorAll(".edge").forEach((el) => {
            el.classList.remove("highlighted", "faded");
            el.style.opacity = el.dataset.baseOpacity || 0.6;
          });
          document.querySelectorAll(".node-circle").forEach((el) => {
            el.classList.remove("faded");
          });
          document.querySelectorAll(".node-label").forEach((el) => {
            el.classList.remove("faded");
          });
        });
      }

      // ─── Easing ─────────────────────────────────────────────────────
      function easeInOutCubic(t) {
        return t < 0.5 ? 4 * t * t * t : 1 - Math.pow(-2 * t + 2, 3) / 2;
      }

      // ─── Animation helpers ──────────────────────────────────────────
      function snapshotPositions(nodes) {
        const snap = new Map();
        for (const [id, n] of nodes) {
          snap.set(id, {
            x: n.x,
            y: n.y,
            radius: n.radius,
            angle: n.angle,
            layer: n.layer,
          });
        }
        return snap;
      }

      function lerpNode(old, cur, t) {
        return {
          x: old.x + (cur.x - old.x) * t,
          y: old.y + (cur.y - old.y) * t,
          radius: old.radius + (cur.radius - old.radius) * t,
          angle: old.angle + (cur.angle - old.angle) * t,
          layer: cur.layer,
        };
      }

      function renderEdgesFromPositions(
        container,
        edges,
        posMap,
        nodes,
        sameLayerSet,
      ) {
        container.innerHTML = "";

        for (const e of edges) {
          const src = posMap.get(e.source);
          const tgt = posMap.get(e.target);
          const isSameLayer = sameLayerSet.has(e);

          if (isSameLayer) {
            const path = document.createElementNS(svgNS, "path");
            const r = src.radius;
            let theta1 = src.angle;
            let theta2 = tgt.angle;
            let dTheta = theta2 - theta1;
            while (dTheta > Math.PI) dTheta -= 2 * Math.PI;
            while (dTheta < -Math.PI) dTheta += 2 * Math.PI;
            const thetaMid = theta1 + dTheta / 2;
            const bulge = 0.35 * r * Math.abs(dTheta);
            const cx = (r + bulge) * Math.cos(thetaMid);
            const cy = -(r + bulge) * Math.sin(thetaMid);
            path.setAttribute(
              "d",
              `M ${src.x},${src.y} Q ${cx},${cy} ${tgt.x},${tgt.y}`,
            );
            path.classList.add("edge", "same-layer");
            path.dataset.source = e.source;
            path.dataset.target = e.target;
            container.appendChild(path);
          } else {
            const path = document.createElementNS(svgNS, "path");
            const mx = (src.x + tgt.x) / 2;
            const my = (src.y + tgt.y) / 2;
            const dx = tgt.x - src.x;
            const dy = tgt.y - src.y;
            const chordLen = Math.sqrt(dx * dx + dy * dy);
            let px = -dy,
              py = dx;
            if (px * mx + py * my < 0) {
              px = -px;
              py = -py;
            }
            const bulge = 0.12 * chordLen;
            const pLen = Math.sqrt(px * px + py * py) || 1;
            const cx = mx + (px / pLen) * bulge;
            const cy = my + (py / pLen) * bulge;
            path.setAttribute(
              "d",
              `M ${src.x},${src.y} Q ${cx},${cy} ${tgt.x},${tgt.y}`,
            );
            path.classList.add("edge");
            path.dataset.source = e.source;
            path.dataset.target = e.target;
            container.appendChild(path);
          }
        }
      }

      function renderNodesFromPositions(container, posMap, nodes, adj) {
        container.innerHTML = "";

        for (const [id, node] of nodes) {
          const p = posMap.get(id);
          const g = document.createElementNS(svgNS, "g");
          g.classList.add("node-group");
          g.dataset.id = id;

          const circle = document.createElementNS(svgNS, "circle");
          circle.setAttribute("cx", p.x);
          circle.setAttribute("cy", p.y);
          circle.setAttribute("r", NODE_RADIUS);
          circle.setAttribute("fill", layerColor(p.layer));
          circle.classList.add("node-circle");
          circle.dataset.id = id;
          g.appendChild(circle);

          const dist = Math.sqrt(p.x * p.x + p.y * p.y);
          let lx, ly;
          if (dist < 1) {
            lx = p.x;
            ly = p.y - NODE_RADIUS - 8;
          } else {
            const offset = NODE_RADIUS + 10;
            lx = p.x + (p.x / dist) * offset;
            ly = p.y + (p.y / dist) * offset;
          }

          const label = document.createElementNS(svgNS, "text");
          label.setAttribute("x", lx);
          label.setAttribute("y", ly);
          label.classList.add("node-label");
          label.textContent = node.label;
          label.dataset.id = id;
          g.appendChild(label);
          container.appendChild(g);
        }
      }

      function renderRingsAnimated(container, oldLayers, newLayers, t) {
        container.innerHTML = "";
        // Collect all unique layer indices from both old and new
        const allLayers = new Set([...oldLayers.keys(), ...newLayers.keys()]);

        for (const i of allLayers) {
          const oldNodes = oldLayers.get(i);
          const newNodes = newLayers.get(i);
          const hadRing = oldNodes && !(i === 0 && oldNodes.length === 1);
          const hasRing = newNodes && !(i === 0 && newNodes.length === 1);

          if (!hadRing && !hasRing) continue;

          const oldR = hadRing ? BASE_RADIUS + i * RING_SPACING : 0;
          const newR = hasRing ? BASE_RADIUS + i * RING_SPACING : 0;
          const r = oldR + (newR - oldR) * t;

          // Fade in / out
          let opacity = 1;
          if (!hadRing) opacity = t;
          else if (!hasRing) opacity = 1 - t;

          if (r > 0 && opacity > 0.01) {
            const circle = document.createElementNS(svgNS, "circle");
            circle.setAttribute("cx", 0);
            circle.setAttribute("cy", 0);
            circle.setAttribute("r", r);
            circle.classList.add("ring");
            circle.style.opacity = opacity;
            container.appendChild(circle);
          }
        }
      }

      // ─── Main ───────────────────────────────────────────────────────
      function main() {
        const graphData = transformRawData(rawData);
        const { nodes, adj, edges } = parseGraph(graphData);
        let currentLayers = assignLayers(nodes, adj);
        minimizeCrossings(currentLayers, nodes, adj, edges);
        computePositions(currentLayers, nodes);

        const ringsLayer = document.getElementById("rings-layer");
        const edgesLayer = document.getElementById("edges-layer");
        const nodesLayer = document.getElementById("nodes-layer");
        const svg = document.getElementById("graph-svg");
        const viewport = document.getElementById("viewport");

        const { sameLayer } = classifyEdges(edges, nodes);
        let sameLayerSet = new Set(sameLayer);

        renderRings(ringsLayer, currentLayers);
        renderEdges(edgesLayer, edges, nodes, sameLayerSet);
        setEdgeOpacities(edgesLayer, (id) => nodes.get(id));
        renderNodes(nodesLayer, nodes, adj);
        setupInteraction(svg, viewport, nodes, adj);

        // ─── Click-to-recenter ──────────────────────────────────────
        let animating = false;

        svg.addEventListener("click", (e) => {
          const circle = e.target.closest(".node-circle");
          if (!circle || animating) return;

          const clickedId = circle.dataset.id;

          // Snapshot old positions
          const oldSnap = snapshotPositions(nodes);
          const oldLayers = new Map();
          for (const [k, v] of currentLayers) oldLayers.set(k, [...v]);

          // Recompute layout from clicked node
          currentLayers = assignLayers(nodes, adj, clickedId);
          minimizeCrossings(currentLayers, nodes, adj, edges);
          computePositions(currentLayers, nodes);

          // Reclassify same-layer edges for new layout
          const newClassified = classifyEdges(edges, nodes);
          sameLayerSet = new Set(newClassified.sameLayer);

          const newSnap = snapshotPositions(nodes);
          const newLayers = new Map();
          for (const [k, v] of currentLayers) newLayers.set(k, [...v]);

          // Animate
          animating = true;
          const duration = 700; // ms
          const startTime = performance.now();

          function frame(now) {
            const elapsed = now - startTime;
            const rawT = Math.min(elapsed / duration, 1);
            const t = easeInOutCubic(rawT);

            // Interpolate positions
            const interpPos = new Map();
            for (const [id] of nodes) {
              interpPos.set(id, lerpNode(oldSnap.get(id), newSnap.get(id), t));
            }

            // Render interpolated state
            renderRingsAnimated(ringsLayer, oldLayers, newLayers, t);
            renderEdgesFromPositions(
              edgesLayer,
              edges,
              interpPos,
              nodes,
              sameLayerSet,
            );
            setEdgeOpacities(edgesLayer, (id) => interpPos.get(id));
            renderNodesFromPositions(nodesLayer, interpPos, nodes, adj);

            if (rawT < 1) {
              requestAnimationFrame(frame);
            } else {
              animating = false;
              // Final clean render
              renderRings(ringsLayer, currentLayers);
              renderEdges(edgesLayer, edges, nodes, sameLayerSet);
              setEdgeOpacities(edgesLayer, (id) => nodes.get(id));
              renderNodes(nodesLayer, nodes, adj);
            }
          }

          requestAnimationFrame(frame);
        });
      }

      // ─── Fullscreen Toggle ─────────────────────────────────────
      (function () {
        const container = document.getElementById("graph-container");
        const btn = document.getElementById("fullscreen-toggle");
        const iconExpand = document.getElementById("fs-icon-expand");
        const iconCollapse = document.getElementById("fs-icon-collapse");

        btn.addEventListener("click", () => {
          container.classList.toggle("fullscreen");
          const isFs = container.classList.contains("fullscreen");
          iconExpand.style.display = isFs ? "none" : "block";
          iconCollapse.style.display = isFs ? "block" : "none";
          // Re-center the graph after toggling
          window.dispatchEvent(new Event("resize"));
        });

        document.addEventListener("keydown", (e) => {
          if (
            e.key === "Escape" &&
            container.classList.contains("fullscreen")
          ) {
            container.classList.remove("fullscreen");
            iconExpand.style.display = "block";
            iconCollapse.style.display = "none";
            window.dispatchEvent(new Event("resize"));
          }
        });
      })();

      main();
    </script>
  </body>
</html>